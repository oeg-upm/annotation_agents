{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06be5f92-0900-475f-bf8d-38fa1645a557",
   "metadata": {},
   "source": [
    "## **Notebook 02-Splits (train/val/test + prompt_regression):**\n",
    "\n",
    "**Objetivo**\n",
    "- Crear un identificador estable doc_uid para unannotated (no tiene id).\n",
    "- Generar particiones reproducibles: train/val/test y un conjunto pequeño prompt_regression.\n",
    "- (En este NB) preparar la infraestructura para excluir documentos de guías cuando tengamos la lista (la generaremos en el NB03, pero aquí dejamos el hook).\n",
    "\n",
    "**Por qué doc_uid**\n",
    "- Porque si el unannotated no tiene id, necesitas una clave estable para:\n",
    "- excluir documentos usados en guías,\n",
    "- referenciar predicciones,\n",
    "- auditar errores,\n",
    "- reproducir splits.\n",
    "\n",
    "La forma estándar: hash del texto (SHA1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3612050-5436-4ec0-a261-41d3e7344723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "assert ROOT.exists()\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130748aa-d53d-441c-af32-5b0a137d9db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rutas y carga\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_UNANNOT = ROOT / \"data\" / \"corpus_unannotated.jsonl\"\n",
    "\n",
    "OUT_SPLITS = ROOT / \"outputs\" / \"splits\"\n",
    "OUT_SPLITS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "unannot = load_jsonl(PATH_UNANNOT)\n",
    "len(unannot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ad5187-b9e2-4cc7-b2bc-c815c4e1da41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9fd56032e0b2b4d832dc8b8f8c35ec4b9fad3074',\n",
       " 'FORMALIZACIÓN DEL CONTRATO DENOMINADO “APROVECHAMIENTO DE LOS PASTOS DEL TINAJER')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos doc_uid determinista para los documentos que no están etiquetados (cada doc es una línea de json)\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def add_uids_unannot(rows):\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        text = r[\"text\"]\n",
    "        uid = stable_uid(text)\n",
    "        out.append({\"doc_uid\": uid, \"id\": uid, \"text\": text})\n",
    "    return out\n",
    "\n",
    "unannot_uid = add_uids_unannot(unannot)\n",
    "unannot_uid[0][\"doc_uid\"], unannot_uid[0][\"text\"][:80]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f507386-33ed-4112-9d5d-7a3dadd84eee",
   "metadata": {},
   "source": [
    "Como hemos identificado, en los experimentos 2 y 3 se usan las guías como parte del prompt, con el principal **riesgo de que luego evaluemos sobre textos que el modelo ha visto literalmente en el prompt (leakage)** (nuestras guías incluyen 1 ejemplo). Por tanto, debemo garantizar que los textos usados en **memoria/few-shot no estén en prompt_regression/val/test**. Así que, para nuestro MVP, apartamos los documentos completos donde aparezcan los ejemplos de las guías. Haremos:\n",
    "- Construir memoria/few-shot a partir de gold (con offsets), usando los ejemplos de guías solo como “plantilla de criterio”.\n",
    "- Para evitar leakage de manera sencilla: excluir de prompt_regression/val/test los documentos que contengan cualquier cita literal usada en memoria/few-shot.\n",
    "\n",
    "Esto es una versión híbrida:\n",
    "- no excluimos “porque aparecen en la guía” en abstracto,\n",
    "- excluimos solo los docs que contienen exactamente los extractos que tú vas a meter en el prompt.\n",
    "\n",
    "Habría otra forma de hacerlo, que es, de acuerdo con el formato utilizado en las guías, los ejemplos de anotación van entre corchetes ([...]), pudiendo extraer concretamente esos trozos, pero caeríamos en el riesgo de desconfigurar los offsets. \n",
    "\n",
    "**_Opción A (más segura, más simple): excluir documentos que contienen citas de guía_**\n",
    "\n",
    "- Pros: muy robusta, fácil, cero riesgo de que el modelo “vea” el mismo texto en evaluación.\n",
    "- Contras: perdemos algunos documentos.\n",
    "\n",
    "Importante: excluir documentos completos no toca offsets, porque no modificamos el texto; solo cambiamos qué docs entran en splits.\n",
    "\n",
    "**_Opción B (más fina, conserva docs): NO excluir docs, pero excluir esos spans de cualquier “memoria/few-shot” y de tu set de evaluación_**\n",
    "\n",
    "- Pros: no pierdes documentos.\n",
    "- Contras: más compleja; si el documento completo aparece en train, y el mismo fragmento está en la guía, sigue existiendo potencial leakage “por contenido”, aunque no usemos el fragmento como ejemplo.\n",
    "\n",
    "**Nos quedamos con la opción A.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04764ab5-1337-47b2-9a42-7204a708cc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 373, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en NB03 generaremos outputs/memory/excluded_doc_uids.json. Con estas líneas dejamos el mecanismo preparado\n",
    "\n",
    "EXCLUDED_PATH = ROOT / \"outputs\" / \"memory\" / \"excluded_doc_uids.json\"\n",
    "\n",
    "excluded = set()\n",
    "if EXCLUDED_PATH.exists():\n",
    "    excluded = set(json.loads(EXCLUDED_PATH.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "unannot_filtered = [d for d in unannot_uid if d[\"doc_uid\"] not in excluded]\n",
    "len(unannot_filtered), len(unannot_uid), len(excluded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e6158-32df-42f5-a675-22b8f09f23a8",
   "metadata": {},
   "source": [
    "**Split reproducible (train/val/test + prompt_regression)**\n",
    "\n",
    "Recomendación para un MVP:\n",
    "- test: 10%\n",
    "- val: 10%\n",
    "- prompt_regression: 20-40 docs (o 2-3% si el corpus es pequeño)\n",
    "- train: resto\n",
    "\n",
    "Aplicaremos:\n",
    "- prompt_regression_n = 30 (ajustable)\n",
    "- val_pct=0.10, test_pct=0.10\n",
    "\n",
    "**A continuación, las celdas de splits que en [11] reescribiremos para crear el filtro anti-leakage >>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63016bd1-7e50-490f-b938-7c00bb862a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 36, 36, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "docs = unannot_filtered[:]  # copia\n",
    "random.shuffle(docs)\n",
    "\n",
    "prompt_regression_n = min(30, max(10, len(docs)//50))  # regla simple adaptable\n",
    "prompt_regression = docs[:prompt_regression_n]\n",
    "rest = docs[prompt_regression_n:]\n",
    "\n",
    "test_n = int(0.10 * len(rest))\n",
    "val_n  = int(0.10 * len(rest))\n",
    "\n",
    "test = rest[:test_n]\n",
    "val  = rest[test_n:test_n+val_n]\n",
    "train = rest[test_n+val_n:]\n",
    "\n",
    "len(train), len(val), len(test), len(prompt_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1db3859-06dc-4967-ba4c-8badf7680c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos JSONL. Con ensure_ascii=False nos aseguramos que el JSONL guardado sea legible (tildes visibles). No afecta a la carga posterior\n",
    "\n",
    "def save_jsonl(rows, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(train, OUT_SPLITS / \"train.jsonl\")\n",
    "save_jsonl(val, OUT_SPLITS / \"val.jsonl\")\n",
    "save_jsonl(test, OUT_SPLITS / \"test.jsonl\")\n",
    "save_jsonl(prompt_regression, OUT_SPLITS / \"prompt_regression.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb35dc-8cbf-4c6d-b148-05f95da4dc60",
   "metadata": {},
   "source": [
    "## Retomamos NB02 después de crear doc bloqueo en NB03\n",
    "Reharemos NB02 usando el fichero de bloqueo.\n",
    "\n",
    "Objetivo de NB02 ahora:\n",
    "- Generar train/val/test/prompt_regression\n",
    "- asegurando que NINGÚN doc_uid en blocked_doc_uids_by_memory.json aparezca en:\n",
    "    - val\n",
    "    - test\n",
    "    - prompt_regression\n",
    "\n",
    "(En train podemos decidir incluirlos o no; para MVP, lo habitual es permitirlos en train pero nunca en evaluación.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0472b65a-2d9c-4775-8df1-721f7baff2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "# Preferimos el formato nuevo si existe\n",
    "PATH_BLOCKED_KEYS = ROOT / \"outputs\" / \"memory\" / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCKED_LEGACY = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "blocked = set()\n",
    "if PATH_BLOCKED_KEYS.exists():\n",
    "    obj = json.loads(PATH_BLOCKED_KEYS.read_text(encoding=\"utf-8\"))\n",
    "    blocked = set([str(x) for x in obj.get(\"blocked_ids\", [])] + [str(x) for x in obj.get(\"blocked_uids\", [])])\n",
    "elif PATH_BLOCKED_LEGACY.exists():\n",
    "    blocked = set(str(x) for x in json.loads(PATH_BLOCKED_LEGACY.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "def doc_key(d: dict) -> str:\n",
    "    return str(d.get(\"id\") or d.get(\"doc_uid\"))\n",
    "\n",
    "# cuando tengas tu lista de docs con key estable:\n",
    "docs = [d for d in docs if doc_key(d) not in blocked]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f43b8-9517-440e-8d5a-161e9a802ab3",
   "metadata": {},
   "source": [
    "Después de esto, hacemos los splits con docs ya filtrados (solo para val/test/prompt_regression [no mantenerlos en train]\n",
    "Decisión rápida (para que no haya ambigüedad)\n",
    "- Para el MVP:\n",
    "    - val/test/prompt_regression: excluir bloqueados\n",
    "    - train: incluir bloqueados (no afecta a leakage de evaluación en prompting; pero si luego hacemos fine-tuning, lo revisamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f75a2-1c0b-4991-b9ef-2533999fc650",
   "metadata": {},
   "source": [
    "- La lógica base es correcta siempre que **unannot_filtered** ya sea la lista de documentos “candidatos a split” con **doc_uid**.\n",
    "- Con el output que tuvimos (291, 36, 36, 10) cuadra perfectamente con:\n",
    "    - primero separas prompt_regression (10),\n",
    "    - luego haces 10% test y 10% val del resto.\n",
    "\n",
    "Dicho eso, ahora que ya tenemos **blocked_doc_uids_by_memory.json**, hay dos ajustes imprescindibles para que esté “bien” metodológicamente:\n",
    "\n",
    "1) Asegurar anti-leakage en val/test/prompt_regression\n",
    "- Necesitamos filtrar (excluir) los docs cuyo doc_uid esté en blocked antes de crear splits (o, alternativamente, permitirlos en train pero nunca en val/test/prompt_regression).\n",
    "\n",
    "**Reemplazo recomendado (mínimo cambio)**\n",
    "- Justo antes de docs = unannot_filtered añadimos la sustitución de uno por otro\n",
    "- A partir de ahí, en tus splits, usa **docs_for_eval** en vez de **unannot_filtered**:\n",
    "- Con esto garantizamos que **prompt_regression/val/test** no contengan documentos usados en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725cfa21-a76d-4e05-ad7e-68d986cd03b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 34, 34, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "# Preferimos el formato nuevo si existe\n",
    "PATH_BLOCKED_KEYS = ROOT / \"outputs\" / \"memory\" / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCKED_LEGACY = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "blocked = set()\n",
    "if PATH_BLOCKED_KEYS.exists():\n",
    "    obj = json.loads(PATH_BLOCKED_KEYS.read_text(encoding=\"utf-8\"))\n",
    "    blocked = set([str(x) for x in obj.get(\"blocked_ids\", [])] + [str(x) for x in obj.get(\"blocked_uids\", [])])\n",
    "elif PATH_BLOCKED_LEGACY.exists():\n",
    "    blocked = set(str(x) for x in json.loads(PATH_BLOCKED_LEGACY.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "def doc_key(d: dict) -> str:\n",
    "    return str(d.get(\"id\") or d.get(\"doc_uid\"))\n",
    "\n",
    "# Docs disponibles para evaluación (anti-leakage)\n",
    "docs_for_eval = [d for d in unannot_filtered if doc_key(d) not in blocked]\n",
    "\n",
    "#reescribimos: usaremos docs_for_eval en vez de unnannot_filtered\n",
    "docs = docs_for_eval[:]  # copia\n",
    "random.shuffle(docs)\n",
    "\n",
    "prompt_regression_n = min(30, max(10, len(docs)//50))  # regla simple adaptable\n",
    "prompt_regression = docs[:prompt_regression_n]\n",
    "rest = docs[prompt_regression_n:]\n",
    "\n",
    "test_n = int(0.10 * len(rest))\n",
    "val_n  = int(0.10 * len(rest))\n",
    "\n",
    "test = rest[:test_n]\n",
    "val  = rest[test_n:test_n+val_n]\n",
    "train = rest[test_n+val_n:]\n",
    "\n",
    "len(train), len(val), len(test), len(prompt_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438bfd5-7f84-44dd-bef8-f4007cac4dc1",
   "metadata": {},
   "source": [
    "**En el anterior output tuvimos:**\n",
    "(291, 36, 36, 10), **ahora tenemos** (279, 34, 34, 10). Con esto garantizamos que prompt_regression/val/test no contiene documentos usados en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a3ad9b-0f40-451f-8544-4fdd6b7f55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y el guardado igual (nos aseguramos de tener import json arriba)\n",
    "\n",
    "def save_jsonl(rows, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(train, OUT_SPLITS / \"train.jsonl\")\n",
    "save_jsonl(val, OUT_SPLITS / \"val.jsonl\")\n",
    "save_jsonl(test, OUT_SPLITS / \"test.jsonl\")\n",
    "save_jsonl(prompt_regression, OUT_SPLITS / \"prompt_regression.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4273d7-1566-42dc-b1e8-13f0130fcc4f",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "Después de generar splits, debe dar 0, 0, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58229ea-daa5-48e4-bd9e-843f281666f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocked en prompt_regression: 0\n",
      "blocked en val: 0\n",
      "blocked en test: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "# Preferimos el formato nuevo si existe\n",
    "PATH_BLOCKED_KEYS = ROOT / \"outputs\" / \"memory\" / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCKED_LEGACY = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "blocked = set()\n",
    "if PATH_BLOCKED_KEYS.exists():\n",
    "    obj = json.loads(PATH_BLOCKED_KEYS.read_text(encoding=\"utf-8\"))\n",
    "    blocked = set([str(x) for x in obj.get(\"blocked_ids\", [])] + [str(x) for x in obj.get(\"blocked_uids\", [])])\n",
    "elif PATH_BLOCKED_LEGACY.exists():\n",
    "    blocked = set(str(x) for x in json.loads(PATH_BLOCKED_LEGACY.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "def doc_key(d: dict) -> str:\n",
    "    return str(d.get(\"id\") or d.get(\"doc_uid\"))\n",
    "\n",
    "def count_blocked(rows):\n",
    "    return sum(1 for r in rows if doc_key(r) in blocked)\n",
    "\n",
    "print(\"blocked en prompt_regression:\", count_blocked(prompt_regression))\n",
    "print(\"blocked en val:\", count_blocked(val))\n",
    "print(\"blocked en test:\", count_blocked(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436234b7-96e4-4019-b86b-0ff09b670fb9",
   "metadata": {},
   "source": [
    "Con esto, podemos ver:\n",
    "- Estamos filtrando explícitamente docs_for_eval = [d for d in unannot_filtered if d[\"doc_uid\"] not in blocked] antes de crear splits.\n",
    "- El sanity check da: 0, 0, 0. Eso prueba que ningún documento usado en memoria/few-shot aparece en prompt_regression, val o test.\n",
    "\n",
    "La bajada de tamaños de (291, 36, 36, 10) a (279, 34, 34, 10) es exactamente lo esperable: hemos excluido algunos documentos que estaban en unannot_filtered y que coinciden con blocked_doc_uids_by_memory.json.\n",
    "\n",
    "A continuación, haremos dos comprobaciones adicionales (opcionales, pero buenas)\n",
    "1) ¿Cuántos docs se excluyeron? (Deben ser 16, 4 de cada etiqueta)\n",
    "2) Verificar que los doc_uid de memoria están efectivamente ausentes (Debe ser 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2243017-9775-4126-826b-22596e1ed0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total antes: 373\n",
      "Total eval sin leakage: 357\n",
      "Excluidos por memoria: 16\n"
     ]
    }
   ],
   "source": [
    "#1.¿Cuántos docs se excluyeron?\n",
    "print(\"Total antes:\", len(unannot_filtered))\n",
    "print(\"Total eval sin leakage:\", len(docs_for_eval))\n",
    "print(\"Excluidos por memoria:\", len(unannot_filtered) - len(docs_for_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c26f93-e40d-4bae-a748-4b89d3633534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersección blocked ∩ eval: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "# Preferimos el formato nuevo si existe\n",
    "PATH_BLOCKED_KEYS = ROOT / \"outputs\" / \"memory\" / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCKED_LEGACY = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "blocked = set()\n",
    "if PATH_BLOCKED_KEYS.exists():\n",
    "    obj = json.loads(PATH_BLOCKED_KEYS.read_text(encoding=\"utf-8\"))\n",
    "    blocked = set([str(x) for x in obj.get(\"blocked_ids\", [])] + [str(x) for x in obj.get(\"blocked_uids\", [])])\n",
    "elif PATH_BLOCKED_LEGACY.exists():\n",
    "    blocked = set(str(x) for x in json.loads(PATH_BLOCKED_LEGACY.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "def doc_key(d: dict) -> str:\n",
    "    return str(d.get(\"id\") or d.get(\"doc_uid\"))\n",
    "\n",
    "#2.Verificar que los doc_uid de memoria están efectivamente ausentes \n",
    "uids_eval = {doc_key(d) for d in docs_for_eval}\n",
    "missing = [u for u in blocked if u in uids_eval]\n",
    "print(\"Intersección blocked ∩ eval:\", len(missing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c8802-8264-49f2-ace7-a5c36d390734",
   "metadata": {},
   "source": [
    "## Qué sigue después (NB02 → NB04 / NB05)\n",
    "\n",
    "Ya tenemos:\n",
    "- splits limpios (train/val/test/prompt_regression)\n",
    "- memoria final\n",
    "- bloqueo anti-leakage consistente\n",
    "\n",
    "El siguiente notebook que toca es:\n",
    "\n",
    "**NB04 — Experimentos 1/2/3 (prompting)**\n",
    "1. Cargar prompt_regression.jsonl (para depurar prompts rápido)\n",
    "2. Definir los 3 prompts:\n",
    "    - Exp1: solo nombres de etiquetas (sin criterios)\n",
    "    - Exp2: criterios + memoria (tu memory_selected_FINAL)\n",
    "    - Exp3: criterios + memoria + few-shot adicional\n",
    "3. Generar predicciones JSON estricto (start/end/label) por documento\n",
    "4. Guardar:\n",
    "    - outputs/predictions/exp1.jsonl\n",
    "    - outputs/predictions/exp2.jsonl\n",
    "    - outputs/predictions/exp3.jsonl\n",
    "\n",
    "**NB05 — Evaluación**\n",
    "- Comparar pred vs gold (exact match + overlap)\n",
    "- Métricas por etiqueta\n",
    "- Error analysis (casos fallidos con texto y spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857d02d-89ae-43ef-864a-62e41190fe81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
