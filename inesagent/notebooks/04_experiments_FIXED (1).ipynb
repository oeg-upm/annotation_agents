{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d570ed8",
   "metadata": {},
   "source": [
    "# NB04 — Experimentos 1/2/3 (prompting) con salida JSON estricta\n",
    "\n",
    "Este notebook está “limpio” y organizado para:\n",
    "\n",
    "- cargar datos (gold + memoria) sin variables fantasma\n",
    "- definir prompts Exp1/Exp2/Exp3\n",
    "- generar predicciones con un modelo local (Transformers) en GPU\n",
    "- validar salida con **Pydantic v2** + verificación estricta de offsets/quote\n",
    "- guardar resultados en JSONL\n",
    "\n",
    "> Nota: **No** hardcodees tokens de Hugging Face aquí. Si el modelo es gated, haz login en terminal con `huggingface-cli login`.\n",
    ">\n",
    "> Para que no se me pierda el entorno, se puede guardar como fichero \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48d2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "PIP_USER (si existe): yes\n",
      "ENABLE_USER_SITE: True\n",
      "USER_SITE: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n"
     ]
    }
   ],
   "source": [
    "# 0) Sanity del entorno (debe ser inesagent_gpu)\n",
    "import os, sys, site\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "assert \"/home/jovyan/.conda/envs/inesagent_gpu/bin/python\" in sys.executable, \"❌ No estás en el kernel inesagent_gpu\"\n",
    "\n",
    "print(\"PIP_USER (si existe):\", os.environ.get(\"PIP_USER\"))\n",
    "print(\"ENABLE_USER_SITE:\", site.ENABLE_USER_SITE)\n",
    "print(\"USER_SITE:\", site.getusersitepackages())\n",
    "\n",
    "# GPU\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Si PIP_USER='yes', pip intentará instalar con --user y fallará (por el blindaje).\n",
    "# Para instalar paquetes desde notebook, usaremos siempre:  env -u PIP_USER  + sys.executable -m pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10322776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: []\n",
      "✅ Todo instalado.\n"
     ]
    }
   ],
   "source": [
    "# 1) Instalación opcional (solo si falta algo)\n",
    "#    Ejecuta esta celda SOLO si un import falla.\n",
    "import sys, importlib.util, subprocess, textwrap, os\n",
    "\n",
    "REQUIRED = [\n",
    "    \"pydantic>=2\",\n",
    "    \"jsonschema\",\n",
    "    \"transformers>=4.45,<4.47\",\n",
    "    \"accelerate>=0.34,<2.0\",\n",
    "    \"huggingface_hub>=0.30,<1.0\",\n",
    "    \"safetensors>=0.4\",\n",
    "    \"sentencepiece\",\n",
    "    # cuantización 4-bit (si tu runtime lo soporta)\n",
    "    \"bitsandbytes>=0.43\",\n",
    "    # utilidades\n",
    "    \"tqdm\",\n",
    "]\n",
    "\n",
    "def missing_pkgs(reqs):\n",
    "    missing = []\n",
    "    for r in reqs:\n",
    "        name = r.split(\"==\")[0].split(\">=\")[0].split(\"<\")[0].strip()\n",
    "        if importlib.util.find_spec(name) is None:\n",
    "            missing.append(r)\n",
    "    return missing\n",
    "\n",
    "miss = missing_pkgs(REQUIRED)\n",
    "print(\"Missing:\", miss)\n",
    "\n",
    "if miss:\n",
    "    cmd = f'env -u PIP_USER \"{sys.executable}\" -m pip install -U --no-cache-dir ' + \" \".join([repr(x) for x in miss])\n",
    "    print(\"Running:\", cmd)\n",
    "    # Ejecutamos como shell para poder usar env -u PIP_USER\n",
    "    r = subprocess.run(cmd, shell=True, text=True)\n",
    "    if r.returncode != 0:\n",
    "        raise RuntimeError(\"❌ pip install falló. Revisa el log arriba.\")\n",
    "    print(\"✅ Instalación terminada. Reinicia Kernel si actualizaste libs base (transformers/torch).\")\n",
    "else:\n",
    "    print(\"✅ Todo instalado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bca1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 2) Imports (una sola vez)\n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from typing import List, Dict, Any, Optional, Literal, Tuple\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d1f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n",
      "val_goldkeys: dict_keys(['id', 'text', 'tags', 'legacy_doc_uid'])\n",
      "/home/jovyan/inesagent/gold/corpus_annotated.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/memory/memory_selected_CURATED.json -> True\n",
      "/home/jovyan/inesagent/outputs/splits/val_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/test_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/prompt_regression_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/train_gold_FIXED.jsonl -> True (train opcional)\n"
     ]
    }
   ],
   "source": [
    "# 3) Paths + utilidades de carga\n",
    "#Utilidades\n",
    "\n",
    "import re\n",
    "\n",
    "def repair_invalid_unicode_escapes(s: str) -> str:\n",
    "    return re.sub(r'\\\\u(?![0-9a-fA-F]{4})', r'\\\\\\\\u', s)\n",
    "\n",
    "def normalize_quotes(s: str) -> str:\n",
    "    return (s.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "             .replace(\"’\", \"'\").replace(\"‘\", \"'\"))\n",
    "\n",
    "from pathlib import Path\n",
    "import json, hashlib\n",
    "\n",
    "def is_jsonl(p: Path) -> bool:\n",
    "    return p.suffix.lower() == \".jsonl\"\n",
    "\n",
    "def load_json(p: Path):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(p: Path):\n",
    "    rows = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "#Paths\n",
    "\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "PATH_GOLD    = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "PATH_MEMORY  = ROOT / \"outputs\" / \"memory\" / \"memory_selected_CURATED.json\"\n",
    "PATH_BLOCKED = ROOT / \"outputs\" / \"memory\" / \"blocked_keys_by_memory.json\"\n",
    "\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "PATH_VAL   = SPLITS_DIR / \"val_gold_FIXED.jsonl\"\n",
    "PATH_TEST  = SPLITS_DIR / \"test_gold_FIXED.jsonl\"\n",
    "PATH_PR    = SPLITS_DIR / \"prompt_regression_gold_FIXED.jsonl\"\n",
    "PATH_TRAIN = SPLITS_DIR / \"train_gold_FIXED.jsonl\" # opcional\n",
    "\n",
    "val_gold   = load_jsonl(SPLITS_DIR / \"val_gold_FIXED.jsonl\")\n",
    "test_gold  = load_jsonl(SPLITS_DIR / \"test_gold_FIXED.jsonl\")\n",
    "train_gold = load_jsonl(SPLITS_DIR / \"train_gold_FIXED.jsonl\")\n",
    "pr_gold    = load_jsonl(SPLITS_DIR / \"prompt_regression_gold_FIXED.jsonl\")\n",
    "\n",
    "print(\"val_goldkeys:\", val_gold[0].keys())\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"predictions\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "MVP_LABELS = [\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "\n",
    "\n",
    "\n",
    "for p in [PATH_GOLD, PATH_MEMORY, PATH_VAL, PATH_TEST, PATH_PR]:\n",
    "    print(p, \"->\", p.exists())\n",
    "print(PATH_TRAIN, \"->\", PATH_TRAIN.exists(), \"(train opcional)\")\n",
    "\n",
    "blocked_obj = load_json(PATH_BLOCKED)\n",
    "\n",
    "blocked = set()\n",
    "# soporta tanto formato nuevo como legacy\n",
    "if isinstance(blocked_obj, dict):\n",
    "    blocked |= set(map(str, blocked_obj.get(\"blocked_ids\", [])))\n",
    "    blocked |= set(map(str, blocked_obj.get(\"blocked_uids\", [])))\n",
    "elif isinstance(blocked_obj, list):\n",
    "    blocked |= set(map(str, blocked_obj))\n",
    "else:\n",
    "    raise ValueError(\"Formato de blocked inesperado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5082790-92b7-46fd-bd36-c8d33cad29b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 279\n",
      "val: 34\n",
      "test: 34\n",
      "prompt_reg: 10\n"
     ]
    }
   ],
   "source": [
    "# 3.1) load_jsonl y load splits, comprobamos tamaños\n",
    "print(\"train:\", len(train_gold))\n",
    "print(\"val:\", len(val_gold))\n",
    "print(\"test:\", len(test_gold))\n",
    "print(\"prompt_reg:\", len(pr_gold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb61f98-332c-48ba-84cc-6ca5e04fd988",
   "metadata": {},
   "source": [
    "## Tamaño de muestras\n",
    "- train: 279 → pool principal para construir memoria / ejemplos, o para ajuste posterior.\n",
    "- val: 34 →  tamaño típico para iterar prompts y medir estabilidad sin gastar demasiado (split del corpus).\n",
    "- test: 34 → mismo tamaño que val (equilibrado) (split del corpus).\n",
    "- prompt_reg: 10 → conjunto pequeño para detectar regresiones de prompt (ideal que sea pequeño y fijo).\n",
    "\n",
    "**(!)** Hacemos chequeo para comprobar y asegurarnos de que no hay solapamiento raro como:\n",
    "- los ids no se repiten entre splits\n",
    "- y que val/test no están dentro de blocked_ids si estás usando bloqueo por leakage.\n",
    "\n",
    "**Respuesta esperada:**\n",
    "`overlap val∩test: 0\n",
    "overlap val∩train: 0\n",
    "overlap test∩train: 0\n",
    "overlap pr∩train: 0\n",
    "overlap pr∩val: 0\n",
    "overlap pr∩test: 0 (o 10)`\n",
    "\n",
    "Que `prompt_reg ∩ test = 10` significa que ✅ prompt_reg es exactamente un subconjunto de test > que sea subconjunto de test no es un error, solo implica: No debes evaluar “test” completo y prompt_reg como si fueran dos métricas independientes, porque estarías duplicando información.\n",
    "\n",
    "**Opción A (recomendada): deja prompt_reg como subconjunto de test**, pero úsalo bien:\n",
    "- `prompt_reg`: lo usas para iterar prompts (rápido).\n",
    "- `val`: lo usas para elegir el mejor prompt.\n",
    "- `test`: lo usas solo al final, una vez, con el prompt congelado.\n",
    "\n",
    "Así no hay leakage ni trampa. Opción A no tocar splits, solo metodología (prompt_reg ⊂ test). Más útil para MVP e iteración rápida\n",
    "\n",
    "**Opción B: hacer prompt_reg independiente (si quieres “pureza”)**\n",
    "- La forma más sencilla es reconstruir `prompt_reg desde train` (o desde `val`) y garantizar no solapar.\n",
    "- Como ya tienes los archivos, puedes generar un nuevo `prompt_regression_gold.jsonl` (10 docs) desde train en una celda.\n",
    "\n",
    "Opción B es más \"paper-like\" para evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7552986d-72fb-4e7a-988e-2fc47bdb89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap val∩test: 0\n",
      "overlap val∩train: 0\n",
      "overlap test∩train: 0\n",
      "overlap pr∩train: 0\n",
      "overlap pr∩val: 0\n",
      "overlap pr∩test: 0\n"
     ]
    }
   ],
   "source": [
    "# 3.2) Chequeo sobre val/test vs train\n",
    "def ids(docs): \n",
    "    return {d[\"id\"] for d in docs}\n",
    "\n",
    "ids_val = ids(val_gold)\n",
    "ids_test = ids(test_gold)\n",
    "ids_train = ids(train_gold)\n",
    "ids_pr = ids(pr_gold)\n",
    "\n",
    "print(\"overlap val∩test:\", len(ids_val & ids_test))\n",
    "print(\"overlap val∩train:\", len(ids_val & ids_train))\n",
    "print(\"overlap test∩train:\", len(ids_test & ids_train))\n",
    "print(\"overlap pr∩train:\", len(ids_pr & ids_train))\n",
    "print(\"overlap pr∩val:\", len(ids_pr & ids_val))\n",
    "print(\"overlap pr∩test:\", len(ids_pr & ids_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1ce55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold docs: 373\n",
      "Memoria ejemplos: 10\n",
      "Blocked uids: 2\n"
     ]
    }
   ],
   "source": [
    "# 4) Cargar gold + memoria + blocked (anti-leakage)\n",
    "if not PATH_GOLD.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro gold: {PATH_GOLD}\")\n",
    "gold = load_jsonl(PATH_GOLD) if is_jsonl(PATH_GOLD) else load_json(PATH_GOLD)\n",
    "\n",
    "if not PATH_MEMORY.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro memoria: {PATH_MEMORY}\")\n",
    "memory_selected = load_json(PATH_MEMORY)\n",
    "\n",
    "if not PATH_BLOCKED.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro blocked: {PATH_BLOCKED}\")\n",
    "blocked = set(load_json(PATH_BLOCKED))\n",
    "\n",
    "print(\"Gold docs:\", len(gold))\n",
    "print(\"Memoria ejemplos:\", len(memory_selected))\n",
    "print(\"Blocked uids:\", len(blocked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad891cf-d511-4645-b6fc-3bf7f2d6f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_key(d: dict) -> str:\n",
    "    \"\"\"\n",
    "    Clave canónica:\n",
    "    - gold: id (numérico o string)\n",
    "    - legacy/unannotated: doc_uid\n",
    "    \"\"\"\n",
    "    if d.get(\"id\") is not None:\n",
    "        return str(d[\"id\"])\n",
    "    if d.get(\"doc_uid\") is not None:\n",
    "        return str(d[\"doc_uid\"])\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212f3b5c-c018-4f53-b29f-a096de28384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_MEMORY: /home/jovyan/inesagent/outputs/memory/memory_selected_CURATED.json\n",
      "PATH_BLOCKED: /home/jovyan/inesagent/outputs/memory/blocked_keys_by_memory.json\n",
      "Memory first keys: ['label', 'criterion', 'id', 'start', 'end']\n"
     ]
    }
   ],
   "source": [
    "print(\"PATH_MEMORY:\", PATH_MEMORY)\n",
    "print(\"PATH_BLOCKED:\", PATH_BLOCKED)\n",
    "print(\"Memory first keys:\", list(memory_selected[0].keys()) if memory_selected else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f566388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold MVP docs: 373\n",
      "Eval pool: 373\n",
      "gold_val: 37 gold_test: 37 gold_train_pool: 299\n"
     ]
    }
   ],
   "source": [
    "# 5) Construir gold_mvp (solo docs que contienen alguna de las 4 etiquetas) + split val/test/train_pool\n",
    "def filter_tags_mvp(tags: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    return [t for t in tags if t.get(\"tag\") in MVP_LABELS]\n",
    "\n",
    "def has_mvp_labels_gold(doc: dict) -> bool:\n",
    "    tags = doc.get(\"tags\", [])\n",
    "    if not isinstance(tags, list):\n",
    "        return False\n",
    "    return any(t.get(\"tag\") in MVP_LABELS for t in tags if isinstance(t, dict))\n",
    "\n",
    "gold_mvp = [d for d in gold if has_mvp_labels_gold(d)]\n",
    "print(\"Gold MVP docs:\", len(gold_mvp))\n",
    "\n",
    "\n",
    "# pool de evaluación sin leakage\n",
    "eval_pool = [d for d in gold_mvp if doc_key(d) not in blocked]\n",
    "random.shuffle(eval_pool)\n",
    "\n",
    "# tamaños robustos (si el corpus es pequeño)\n",
    "test_n = min(len(eval_pool), max(30, int(0.10 * len(eval_pool))))\n",
    "val_n  = min(max(0, len(eval_pool)-test_n), max(30, int(0.10 * len(eval_pool))))\n",
    "\n",
    "gold_test = eval_pool[:test_n]\n",
    "gold_val  = eval_pool[test_n:test_n+val_n]\n",
    "gold_train_pool = eval_pool[test_n+val_n:]\n",
    "\n",
    "print(\"Eval pool:\", len(eval_pool))\n",
    "print(\"gold_val:\", len(gold_val), \"gold_test:\", len(gold_test), \"gold_train_pool:\", len(gold_train_pool))\n",
    "\n",
    "\n",
    "def doc_key(d: dict) -> str:\n",
    "    return str(d.get('id') or d.get('doc_uid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa9fdf0-748d-4809-933c-4a6a96151779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'has_any_mvp': 373, 'total': 373})\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos que no sea un error que haya 373 docs con alguna de las 4 etiquetas\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for d in gold:\n",
    "    tags = d.get(\"tags\", [])\n",
    "    labs = {t.get(\"tag\") for t in tags if isinstance(t, dict)}\n",
    "    c[\"has_any_mvp\"] += int(bool(labs & set(MVP_LABELS)))\n",
    "    c[\"total\"] += 1\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c1b1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LABEL: OBJETO\n",
      "  CRITERION: P1.2 Inclusión del lote en el objeto — Se anota como “objeto” los lotes identificados que acompañan el objeto del contrato\n",
      "  EXAMPLE_SPAN: SERVICIOS DE RESTAURACIÓN DE UN CONJUNTO DE 45 OBRAS PERTENECIENTES AL FONDO HISTÓRICO DE LA BIBLIOTECA DEL SENADO (LOTE 1)\n",
      "- LABEL: PRECIO_DEL_CONTRATO\n",
      "  CRITERION: P2.3 Precios en otros formatos — En el caso de encontrar el precio del contrato especificado como cuantía o renta periódica, desglose de precios asociados a bienes o anualidades u otro formato, se anota como “precio_del_contrato” la oración que incluye la cuantía o precio unitario junto a la periodicidad o bien al que acompaña.\n",
      "  EXAMPLE_SPAN: Precio del contrato: por un importe póliza anual por persona asegurada de 3,67 €, y todo ello en los términos y condiciones que figuran en su oferta.\n",
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: El presente contrato tendrá un plazo de vigencia desde el 1 de enero de 2024 (o desde la fecha de formalización del contrato si fuera posterior a ésta) y hasta el 31 de diciembre de 2024.\n",
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio ...\n"
     ]
    }
   ],
   "source": [
    "# #6) Render de memoria (para Exp2/Exp3) usando texto real del gold\n",
    "\n",
    "# 1) Índice texto por id (normalizamos a string para que coincida con memoria)\n",
    "id_to_text = {str(d[\"id\"]): d[\"text\"] for d in gold_mvp}\n",
    "\n",
    "def render_memory_blocks(\n",
    "    memory: List[Dict[str, Any]],\n",
    "    id_to_text: Dict[str, str],\n",
    "    max_per_label: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Construye un bloque de 'memoria' para el prompt.\n",
    "\n",
    "    Importante:\n",
    "    - Tus ejemplos de memoria guardan el documento en `id` (o legacy `doc_uid`)\n",
    "    - El gold guarda el documento en `id`\n",
    "    - Convertimos todo a string para que las claves coincidan.\n",
    "    \"\"\"\n",
    "    # organizar por etiqueta\n",
    "    by_label: Dict[str, List[Dict[str, Any]]] = {lab: [] for lab in MVP_LABELS}\n",
    "    for ex in memory:\n",
    "        lab = ex.get(\"label\")\n",
    "        if lab in by_label:\n",
    "            by_label[lab].append(ex)\n",
    "\n",
    "    blocks = []\n",
    "    for lab in MVP_LABELS:\n",
    "        for ex in by_label[lab][:max_per_label]:\n",
    "            # 2) recuperar clave del documento desde memoria\n",
    "            docid = ex.get(\"id\")\n",
    "            if docid is None:\n",
    "                docid = ex.get(\"doc_uid\")  # fallback legacy\n",
    "\n",
    "            docid = str(docid) if docid is not None else \"\"\n",
    "\n",
    "            txt = id_to_text.get(docid, \"\")\n",
    "            s, e = ex.get(\"start\"), ex.get(\"end\")\n",
    "\n",
    "            # 3) defensivo: si no hay texto o offsets inválidos, marcamos placeholder\n",
    "            if not txt or not isinstance(s, int) or not isinstance(e, int) or not (0 <= s < e <= len(txt)):\n",
    "                span_txt = \"<MISSING_TEXT_OR_BAD_OFFSETS>\"\n",
    "            else:\n",
    "                span_txt = txt[s:e].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "            blocks.append(\n",
    "                f\"- LABEL: {lab}\\n\"\n",
    "                f\"  CRITERION: {ex.get('criterion','')}\\n\"\n",
    "                f\"  EXAMPLE_SPAN: {span_txt}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "memory_block = render_memory_blocks(memory_selected, id_to_text, max_per_label=4)\n",
    "print(memory_block[:1200], \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bddc4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewshot_extra spans (1 por etiqueta): 4\n",
      "EJEMPLO (formato correcto: UN SOLO JSON con varios spans)\n",
      "TEXTO:\n",
      "<<<TEXT>>>\n",
      "CONTRATO DE SUMINISTROS Y SERVICIOS Número de Expediente - 2023HyJ00034 SUMINISTRO EN RÉGIMEN DE ARRENDAMIENTO CON OPCIÓN A COMPRA DE LICENCIAS ILIMITADAS (ULA) DE PRODUCTOS ORACLE Y EL SOPORTE TÉCNICO ASOCIADO, ASÍ COMO LA ADQUISICIÓN DEL SERVICIO AVANZADO DE ASISTENCIA ACS REUNIDOS De una parte, Dña. Zahara De Filiberto Echeberria, en su calidad de DIRECTORA DE LA AGENCIA DE INFORMACIÓN Y CONTROL ALIMENTARIO (en adelante, AICA), en virtud de la Resolución de 7 de julio de 2019, de la Subsecretaría de Agricultura, Pesca y Alimentación, por la que se publica su nombramiento; de acuerdo con las facultades que le atribu\n",
      "...\n",
      "va. Además de lo anterior, el contratista se somete a la normativa nacional y de la Unión Europea en materia de protección de datos. Tercera. - Precio del contrato y revisión de precios. El precio del presente contrato, o en su caso, el importe máximo limitativo, cuando se refiera a realización de servicios retribuidos por precios unitarios, es de 169.131,96 € (Ciento Sesenta y Nueve Mil Ciento Treinta y Un Euros Con Noventa y Seis Céntimos) IVA incluido y comprende todos los gastos e impuestos que recaen sobre la contratación. A dicho importe se le descontarán, en su caso, los gastos de publicidad de la licitación del contrato, que serán deducidos en la primera certificación o factura. En cuanto a la revisión de precios o, en su caso, a la exclusión de la misma, se estará a lo establecido en el P\n",
      "...\n",
      "d del Vocal Asesor TIC de la Unidad de Tecnologías de la Información, Comunicaciones y Apoyo a la Planificación del OEITSS de los servicios realizados por la empresa adjudicataria y de la factura emitida por la misma. De acuerdo con lo establecido la cláusula 2.5.2. del Pliego de Cláusulas Administrativas no procede la revisión de precios. Cuarta: El plazo de prestación del servicio será de 24 meses, desde el 11 de diciembre de 2023 a 10 de diciembre de 2025. Las posibles prórrogas no superarán los 24 meses, según lo establecido ella cláusula 2.4.4. del Pliego de Cláusulas Administrativas. Quinta: El contratista se compromete a adscribir a la ejecución del contrato los medios personales y materiales suficientes para ello. Sexta: Para responder del cumplimiento de este contrato se ha constituido, a favo\n",
      "...\n",
      "o, cumpliendo los requisitos del art. 29 del LCSP, hasta un máximo total de 5 años, incluidas las prórrogas. Las prórrogas que se acuerden quedarán condicionadas a la suscripción previa anual del Acuerdo de Encomienda de Gestión que supone adscribir el Depósito de Medicamentos de SAMUR-Protección Civil al Servicio de Farmacia de Gargantilla Salud. En caso de incumplimiento del plazo total, la Administración podrá optar indistintamente por la resolución del contrato o por la imposición de las penalidades previstas en el pliego de cláusulas administrativas particulares. En su defecto se aplicarán las reglas de los artículos 192 y 193 de la Ley de Contratos del Sector Público. Al amparo de lo que establece el artículo 210.3 de la Ley de Contratos del Sector Público y conforme a lo previsto en el pliego de cláusulas administrativas particulares, para este contrato se establece un plazo de garantía de 1 año, desde e\n",
      "<<<END_TEXT>>>\n",
      "RESPUESTA (JSON):\n",
      "{\"spans\": [{\"tag\": \"OBJETO\", \"start\": 36, \"end\": 275, \"quote\": \"Número de Expediente - 2023HyJ00034 SUMINISTRO EN RÉGIMEN DE ARRENDAMIENTO CON OPCIÓN A COMPRA DE LICENCIAS ILIMITADAS (ULA) DE PRODUCTOS ORACLE Y EL SOPORTE TÉCNICO ASOCIADO, ASÍ COMO LA ADQUISICIÓN DEL SERVICIO AVANZADO DE ASISTENCIA ACS\"}, {\"tag\": \"PRECIO_DEL_CONTRATO\", \"start\": 980, \"end\": 1088, \"quote\": \"169.131,96 € (Ciento Sesenta y Nueve Mil Ciento Treinta y Un Euros Con Noventa y Seis Céntimos) IVA incluido\"}, {\"tag\": \"DURACION_TOTAL_DEL_CONTRATO\", \"start\": 1793, \"end\": 1906, \"quote\": \"El plazo de prestación del servicio será de 24 meses, desde el 11 de diciembre de 2023 a 10 de diciembre de 2025.\"}, {\"tag\": \"RESOLUCION\", \"start\": 2611, \"end\": 2835, \"quote\": \"En caso de incumplimiento del plazo total, la Administración podrá optar indistintamente por la resolución del contrato o por la imposición de las penalidades previstas en el pliego de cláusulas administrativas particulares.\"}]} ...\n"
     ]
    }
   ],
   "source": [
    "# 7) Few-shot extra automático para Exp3 (opcional): 1 span por etiqueta, PERO en un único JSON\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def pick_one_span_per_label(pool: List[Dict[str,Any]], labels: List[str]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Devuelve una lista de items: [{\"label\": lab, \"doc_key_id\": k, \"text\": full_text, \"span\": {...}}]\n",
    "    1 por etiqueta, evitando repetir doc si es posible.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    used_keys = set()\n",
    "\n",
    "    for lab in labels:\n",
    "        found = None\n",
    "        for d in pool:\n",
    "            k = doc_key(d)\n",
    "            if k in used_keys:\n",
    "                continue\n",
    "            tags = d.get(\"tags\", [])\n",
    "            if any(isinstance(t, dict) and t.get(\"tag\") == lab for t in tags):\n",
    "                found = d\n",
    "                break\n",
    "\n",
    "        # fallback: si no hay doc nuevo, permitimos reutilizar doc (mejor que quedarse sin ejemplo)\n",
    "        if found is None:\n",
    "            for d in pool:\n",
    "                tags = d.get(\"tags\", [])\n",
    "                if any(isinstance(t, dict) and t.get(\"tag\") == lab for t in tags):\n",
    "                    found = d\n",
    "                    break\n",
    "\n",
    "        if found:\n",
    "            k = doc_key(found)\n",
    "            used_keys.add(k)\n",
    "\n",
    "            tag = next(t for t in found[\"tags\"] if isinstance(t, dict) and t.get(\"tag\") == lab)\n",
    "            s, e = int(tag[\"start\"]), int(tag[\"end\"])\n",
    "            full = found[\"text\"]\n",
    "            quote = full[s:e]\n",
    "\n",
    "            out.append({\n",
    "                \"label\": lab,\n",
    "                \"doc_key_id\": str(k),\n",
    "                \"text\": full,\n",
    "                \"span\": {\"tag\": lab, \"start\": s, \"end\": e, \"quote\": quote}\n",
    "            })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def render_fewshot_extra_one_json(fewshot_docs: List[Dict[str,Any]], context_chars: int = 350) -> str:\n",
    "    \"\"\"\n",
    "    Renderiza UN SOLO ejemplo few-shot:\n",
    "      - Construye un mini-documento concatenando ventanas alrededor de cada span\n",
    "      - Devuelve UN JSON con spans (start/end relativos al mini-documento)\n",
    "    \"\"\"\n",
    "    if not fewshot_docs:\n",
    "        return \"\"\n",
    "\n",
    "    pieces = []\n",
    "    spans_out = []\n",
    "    cursor = 0\n",
    "    sep = \"\\n...\\n\"\n",
    "\n",
    "    for ex in fewshot_docs:\n",
    "        sp = ex[\"span\"]\n",
    "        full = ex[\"text\"]\n",
    "        s, e = int(sp[\"start\"]), int(sp[\"end\"])\n",
    "\n",
    "        w0 = max(0, s - context_chars)\n",
    "        w1 = min(len(full), e + context_chars)\n",
    "        window = full[w0:w1]\n",
    "\n",
    "        # offsets dentro de la ventana\n",
    "        rs, re = s - w0, e - w0\n",
    "        quote = window[rs:re]\n",
    "\n",
    "        # coloca separador si no es la primera pieza\n",
    "        if pieces:\n",
    "            pieces.append(sep)\n",
    "            cursor += len(sep)\n",
    "\n",
    "        pieces.append(window)\n",
    "\n",
    "        # offsets relativos al mini-documento concatenado\n",
    "        start2 = cursor + rs\n",
    "        end2   = cursor + re\n",
    "\n",
    "        spans_out.append({\n",
    "            \"tag\": sp[\"tag\"],\n",
    "            \"start\": start2,\n",
    "            \"end\": end2,\n",
    "            \"quote\": quote.replace(\"\\r\", \"\")  # opcional; si quieres cero cambios, quítalo\n",
    "        })\n",
    "\n",
    "        cursor += len(window)\n",
    "\n",
    "    mini_text = \"\".join(pieces)\n",
    "\n",
    "    # JSON único, limpio, sin escapes unicode (ensure_ascii=False)\n",
    "    resp = {\"spans\": spans_out}\n",
    "    resp_json = json.dumps(resp, ensure_ascii=False)\n",
    "\n",
    "    block = (\n",
    "        \"EJEMPLO (formato correcto: UN SOLO JSON con varios spans)\\n\"\n",
    "        \"TEXTO:\\n<<<TEXT>>>\\n\"\n",
    "        f\"{mini_text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "        \"RESPUESTA (JSON):\\n\"\n",
    "        f\"{resp_json}\"\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "# --- construir fewshot_extra ---\n",
    "fewshot_docs = pick_one_span_per_label(gold_train_pool, MVP_LABELS)\n",
    "fewshot_extra = render_fewshot_extra_one_json(fewshot_docs, context_chars=350)\n",
    "\n",
    "print(f\"fewshot_extra spans (1 por etiqueta): {len(fewshot_docs)}\")\n",
    "print(fewshot_extra[:8000], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b8fc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Esquema Pydantic (salida estricta) + verificación de offsets/quote\n",
    "Label = Literal[\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "\n",
    "class Span(BaseModel):\n",
    "    label: Label\n",
    "    start: int = Field(ge=0)\n",
    "    end: int = Field(ge=0)\n",
    "    quote: str = Field(min_length=1)\n",
    "\n",
    "    @field_validator(\"end\")\n",
    "    @classmethod\n",
    "    def end_after_start(cls, v, info):\n",
    "        start = info.data.get(\"start\")\n",
    "        if start is not None and v <= start:\n",
    "            raise ValueError(\"end must be > start\")\n",
    "        return v\n",
    "\n",
    "class Pred(BaseModel):\n",
    "    doc_key_id: Optional[str] = None\n",
    "    spans: List[Span] = Field(default_factory=list)\n",
    "\n",
    "def strict_verify(pred: Pred, text: str) -> Pred:\n",
    "    ok = []\n",
    "    for sp in pred.spans:\n",
    "        if sp.end > len(text):\n",
    "            continue\n",
    "        if text[sp.start:sp.end] != sp.quote:\n",
    "            continue\n",
    "        ok.append(sp)\n",
    "    return Pred(doc_key_id=pred.doc_key_id, spans=ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca894ce9-b6de-4eca-988e-641185d29477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Parseo robusto de JSON del modelo + sanitización (FIXED)\n",
    "import json, re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "def extract_all_balanced_json(text: str) -> List[str]:\n",
    "    \"\"\"Extrae TODOS los substrings {...} balanceados que contengan la clave 'spans'.\"\"\"\n",
    "    if not isinstance(text, str) or \"{\" not in text:\n",
    "        return []\n",
    "    out = []\n",
    "    starts = [i for i, ch in enumerate(text) if ch == \"{\"]\n",
    "    for s in starts:\n",
    "        depth = 0\n",
    "        for e in range(s, len(text)):\n",
    "            ch = text[e]\n",
    "            if ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    cand = text[s:e+1]\n",
    "                    if '\"spans\"' in cand or \"'spans'\" in cand:\n",
    "                        out.append(cand)\n",
    "                    break\n",
    "\n",
    "    # dedupe conservador (mantiene orden)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for j in out:\n",
    "        if j not in seen:\n",
    "            seen.add(j)\n",
    "            uniq.append(j)\n",
    "    return uniq\n",
    "\n",
    "def repair_invalid_unicode_escapes(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Solo repara \\\\uXXXX mal formados para que json.loads no explote.\n",
    "    Convierte \"\\\\u\" NO seguido de 4 hex en \"\\\\\\\\u\" (literal).\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r'\\\\u(?![0-9a-fA-F]{4})', r'\\\\\\\\u', s)\n",
    "\n",
    "def fill_offsets_from_quote(spans: List[Dict[str,Any]], text: str) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Rellena start/end buscando quote dentro de text.\n",
    "    Usa cursor para encontrar ocurrencias en orden y evitar duplicados triviales.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    cursor = 0\n",
    "    for sp in spans:\n",
    "        tag = sp.get(\"tag\") or sp.get(\"label\")\n",
    "        quote = sp.get(\"quote\")\n",
    "\n",
    "        if tag not in MVP_LABELS:\n",
    "            continue\n",
    "        if not isinstance(quote, str) or not quote.strip():\n",
    "            continue\n",
    "\n",
    "        idx = text.find(quote, cursor)\n",
    "        if idx == -1:\n",
    "            idx = text.find(quote)\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        start = idx\n",
    "        end = idx + len(quote)\n",
    "        out.append({\"tag\": tag, \"start\": start, \"end\": end, \"quote\": quote})\n",
    "        cursor = end\n",
    "    return out\n",
    "\n",
    "def sanitize_pred_dict(obj: Dict[str,Any], doc_key_id: str, text: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    - Filtra etiquetas no permitidas\n",
    "    - Mantiene start/end si vienen numéricos (pero NO los fuerza a ser correctos aquí)\n",
    "    - Rellena offsets faltantes con fill_offsets_from_quote\n",
    "    \"\"\"\n",
    "    spans = obj.get(\"spans\", [])\n",
    "    if not isinstance(spans, list):\n",
    "        spans = []\n",
    "\n",
    "    cleaned: List[Dict[str,Any]] = []\n",
    "    for sp in spans:\n",
    "        if not isinstance(sp, dict):\n",
    "            continue\n",
    "\n",
    "        tag = sp.get(\"tag\") or sp.get(\"label\")\n",
    "        quote = sp.get(\"quote\")\n",
    "\n",
    "        if tag not in MVP_LABELS:\n",
    "            continue\n",
    "        if not isinstance(quote, str) or not quote.strip():\n",
    "            continue\n",
    "\n",
    "        start = sp.get(\"start\", None)\n",
    "        end = sp.get(\"end\", None)\n",
    "\n",
    "        if start is not None and end is not None:\n",
    "            try:\n",
    "                start = int(start); end = int(end)\n",
    "            except Exception:\n",
    "                start = None; end = None\n",
    "\n",
    "        cleaned.append({\"tag\": tag, \"start\": start, \"end\": end, \"quote\": quote})\n",
    "\n",
    "    # Autofill offsets donde falten\n",
    "    need_fill = [sp for sp in cleaned if sp[\"start\"] is None or sp[\"end\"] is None]\n",
    "    have_ok  = [sp for sp in cleaned if sp[\"start\"] is not None and sp[\"end\"] is not None]\n",
    "\n",
    "    filled = fill_offsets_from_quote(need_fill, text)\n",
    "\n",
    "    final = have_ok + filled\n",
    "    return {\"doc_key_id\": str(doc_key_id), \"spans\": final}\n",
    "\n",
    "def parse_and_validate(raw: str, doc_key_id: str, text: str) -> Tuple[Dict[str,Any], str]:\n",
    "    \"\"\"\n",
    "    - Soporta multi-JSON (Exp3 concatenando varios objetos)\n",
    "    - Repara unicode escapes mal formados SOLO para poder parsear\n",
    "    - Sanitiza + rellena offsets\n",
    "    - Valida con Pydantic + strict_verify\n",
    "    \"\"\"\n",
    "    candidates = extract_all_balanced_json(raw)\n",
    "    if not candidates:\n",
    "        return {\"doc_key_id\": str(doc_key_id), \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, \"non_json_output\"\n",
    "\n",
    "    merged_spans: List[Dict[str,Any]] = []\n",
    "    parse_errors = 0\n",
    "\n",
    "    for js in candidates:\n",
    "        js2 = repair_invalid_unicode_escapes(js)\n",
    "        try:\n",
    "            obj = json.loads(js2)\n",
    "        except Exception:\n",
    "            parse_errors += 1\n",
    "            continue\n",
    "\n",
    "        spans = obj.get(\"spans\", [])\n",
    "        if isinstance(spans, list):\n",
    "            merged_spans.extend(spans)\n",
    "\n",
    "    if not merged_spans:\n",
    "        # si había candidatos pero ninguno parseó bien\n",
    "        if parse_errors > 0:\n",
    "            return {\n",
    "                \"doc_key_id\": str(doc_key_id),\n",
    "                \"spans\": [],\n",
    "                \"_error\": \"json_parse_error\",\n",
    "                \"_raw\": raw,\n",
    "                \"_exception\": f\"{parse_errors} candidate(s) failed to parse\"\n",
    "            }, \"json_parse_error\"\n",
    "        return {\"doc_key_id\": str(doc_key_id), \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, \"non_json_output\"\n",
    "\n",
    "    obj_merged = {\"spans\": merged_spans}\n",
    "    obj2 = sanitize_pred_dict(obj_merged, doc_key_id, text)\n",
    "\n",
    "    # Si tras sanitización/fill no queda nada => all_spans_discarded\n",
    "    if len(obj2.get(\"spans\", [])) == 0:\n",
    "        out = {\"doc_key_id\": str(doc_key_id), \"spans\": [], \"_error\": \"all_spans_discarded\", \"_raw\": raw}\n",
    "        out[\"_meta\"] = {\"n_spans_raw_total\": len(merged_spans), \"n_spans_after_fill\": 0, \"n_spans_final\": 0}\n",
    "        return out, \"all_spans_discarded\"\n",
    "\n",
    "    # Pydantic + strict_verify\n",
    "    try:\n",
    "        pred = Pred.model_validate({\n",
    "            \"doc_uid\": str(doc_key_id),  # Pred espera doc_uid; lo reutilizamos sin cambiar el modelo\n",
    "            \"spans\": [\n",
    "                {\"label\": sp[\"tag\"], \"start\": sp[\"start\"], \"end\": sp[\"end\"], \"quote\": sp[\"quote\"]}\n",
    "                for sp in obj2[\"spans\"]\n",
    "            ]\n",
    "        })\n",
    "        pred2 = strict_verify(pred, text)\n",
    "        out = {\"doc_key_id\": str(doc_key_id), \"spans\": [s.model_dump() for s in pred2.spans]}\n",
    "\n",
    "        if len(out[\"spans\"]) == 0 and len(obj2[\"spans\"]) > 0:\n",
    "            out[\"_error\"] = \"all_spans_discarded\"\n",
    "            out[\"_raw\"] = raw\n",
    "            out[\"_meta\"] = {\"n_spans_after_fill\": len(obj2[\"spans\"]), \"n_spans_final\": 0}\n",
    "            return out, \"all_spans_discarded\"\n",
    "\n",
    "        return out, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"doc_key_id\": str(doc_key_id), \"spans\": [], \"_error\": \"validation_error\", \"_raw\": raw, \"_exception\": repr(e)}, \"validation_error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae1472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BitsAndBytesConfig OK (4-bit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:35<00:00, 39.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado. device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 11) Carga del modelo (Transformers) — 4-bit si bitsandbytes está disponible\n",
    "# Si el modelo es gated (Llama), haz login en terminal:  huggingface-cli login\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"❌ CUDA no disponible. Este notebook asume GPU.\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Intento 4-bit (bnb). Si falla, cae a fp16 (más pesado).\n",
    "use_4bit = True\n",
    "bnb_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        print(\"✅ BitsAndBytesConfig OK (4-bit)\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ No puedo configurar 4-bit. Fallback fp16. Error:\", repr(e))\n",
    "        bnb_config = None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "if bnb_config is not None:\n",
    "    model_kwargs[\"quantization_config\"] = bnb_config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Modelo cargado. device:\", model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3565bf24-a584-4544-95fd-c16b4d979b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token: <|eot_id|>\n",
      "tokenizer.eos_token_id: 128009\n",
      "tokenizer.pad_token: <|eot_id|>\n",
      "tokenizer.pad_token_id: 128009\n",
      "model.config.eos_token_id: [128001, 128008, 128009]\n",
      "model.config.pad_token_id: None\n",
      "model.generation_config.eos_token_id: [128001, 128008, 128009]\n",
      "model.generation_config.pad_token_id: None\n"
     ]
    }
   ],
   "source": [
    "#11.1 Celda de diagnóstico\n",
    "print(\"tokenizer.eos_token:\", tokenizer.eos_token)\n",
    "print(\"tokenizer.eos_token_id:\", tokenizer.eos_token_id)\n",
    "print(\"tokenizer.pad_token:\", tokenizer.pad_token)\n",
    "print(\"tokenizer.pad_token_id:\", tokenizer.pad_token_id)\n",
    "\n",
    "print(\"model.config.eos_token_id:\", model.config.eos_token_id)\n",
    "print(\"model.config.pad_token_id:\", model.config.pad_token_id)\n",
    "\n",
    "# transformers usa a menudo generation_config en lugar de config\n",
    "print(\"model.generation_config.eos_token_id:\", getattr(model.generation_config, \"eos_token_id\", None))\n",
    "print(\"model.generation_config.pad_token_id:\", getattr(model.generation_config, \"pad_token_id\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a02ec5-5fff-4db1-959c-9a36e8c76bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK eos_id: 128009 | pad_id: 128009\n"
     ]
    }
   ],
   "source": [
    "#11.2 Evitar mensajes de transformers diciendo que como el token no tiene pad definido, usa EOS para padding\n",
    "# Quita warnings y define padding consistentemente\n",
    "# --- FIX robusto de EOS/PAD para quitar warnings de padding ---\n",
    "\n",
    "# 1) Determinar eos_token_id \"real\"\n",
    "eos_id = tokenizer.eos_token_id\n",
    "\n",
    "# Si tokenizer no lo tiene, intenta sacarlo del modelo\n",
    "if eos_id is None:\n",
    "    eos_id = model.config.eos_token_id\n",
    "\n",
    "# Si eos_id es lista/tupla, coge el primero\n",
    "if isinstance(eos_id, (list, tuple)):\n",
    "    eos_id = eos_id[0]\n",
    "\n",
    "# 2) Si sigue siendo None, algo está realmente mal con el tokenizer/model\n",
    "assert eos_id is not None, \"ERROR: eos_token_id es None. El tokenizer/model no tiene EOS configurado.\"\n",
    "\n",
    "# 3) Asegura PAD en tokenizer\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = eos_id\n",
    "    # opcional: token “pad” como string también\n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4) Asegura PAD/EOS en generation_config (lo que usa generate)\n",
    "model.generation_config.eos_token_id = eos_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 5) También en config, por coherencia\n",
    "model.config.eos_token_id = eos_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"OK eos_id:\", eos_id, \"| pad_id:\", tokenizer.pad_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3321e4df-7b2a-4388-a301-41caf42b7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos alineación de quotes a offsets \n",
    "def align_quotes_to_offsets(spans: List[Dict[str,Any]], text: str) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    for sp in spans:\n",
    "        lab = sp.get(\"label\") or sp.get(\"tag\")\n",
    "        q = sp.get(\"quote\")\n",
    "        if lab not in MVP_LABELS or not isinstance(q, str) or not q.strip():\n",
    "            continue\n",
    "\n",
    "        start = text.find(q)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        end = start + len(q)\n",
    "        out.append({\"label\": lab, \"start\": start, \"end\": end, \"quote\": q})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeed5eb7-dfb2-4c82-adaf-5d2fc4a9ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Prompts Exp1/Exp2/Exp3 (MVP 4 etiquetas + offsets estrictos)\n",
    "LABELS = MVP_LABELS  # [\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "LABELS_TXT = \", \".join(LABELS)\n",
    "\n",
    "SYSTEM_BASE = (\n",
    "    \"Eres un anotador jurídico experto en contratos del sector público en España.\\n\"\n",
    "    \"Tu tarea es EXTRAER FRAGMENTOS (quotes) del texto y etiquetarlos.\\n\\n\"\n",
    "\n",
    "    f\"ETIQUETAS PERMITIDAS (OBLIGATORIO): {LABELS_TXT}.\\n\"\n",
    "    \"PROHIBIDO inventar otras etiquetas.\\n\\n\"\n",
    "    \n",
    "    \"IMPORTANTE: NO escapes Unicode. No uses secuencias ni escapes como \\\\u00fa. Escribe utf-8 directamente (á, é, í, ó, ú, ñ, €) como 'público', 'contratación', etc.\\n\"\n",
    "\n",
    "    \"FORMATO DE SALIDA (OBLIGATORIO):\\n\"\n",
    "    \"- Devuelve EXACTAMENTE un JSON válido y NADA más.\\n\"\n",
    "    \"- NO uses markdown (sin ```).\\n\"\n",
    "    \"- Cada span debe tener EXACTAMENTE: tag, quote.\\n\"\n",
    "    \"- Si no hay spans: {\\\"spans\\\": []}\\n\\n\"\n",
    "\n",
    "    \"REGLAS CRÍTICAS:\\n\"\n",
    "    \"- quote debe ser un substring EXACTO del texto proporcionado (copiar/pegar del texto).\\n\"\n",
    "    \"- No inventes texto. No reformules. No normalices.\\n\"\n",
    "    \"- No incluyas <<<TEXT>>> ni <<<END_TEXT>>> dentro de quote.\\n\\n\"\n",
    "\n",
    "    \"LÍMITES:\\n\"\n",
    "    \"- Devuelve como máximo 6 spans en total.\\n\"\n",
    "    \"- No repitas spans.\\n\"\n",
    ")\n",
    "\n",
    "def build_user_exp1(text: str) -> str:\n",
    "    return (\n",
    "        \"Extrae spans SOLO para estas etiquetas: OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION.\\n\"\n",
    "        \"Devuelve SOLO JSON válido con clave 'spans'.\\n\"\n",
    "        \"Cada span debe tener: tag, quote.\\n\\n\"\n",
    "        \"TEXTO:\\n<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "    )\n",
    "\n",
    "def build_user_exp2(text: str, memory_block: str) -> str:\n",
    "    return (\n",
    "        \"Usa la MEMORIA como guía. Extrae spans SOLO para estas etiquetas: OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION.\\n\"\n",
    "        \"Devuelve SOLO JSON válido con clave 'spans'.\\n\"\n",
    "        \"Cada span debe tener: tag, quote.\\n\\n\"\n",
    "        \"MEMORIA:\\n\"\n",
    "        f\"{memory_block}\\n\\n\"\n",
    "        \"TEXTO:\\n<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "    )\n",
    "\n",
    "def build_user_exp3(text: str, memory_block: str, fewshot_extra: str) -> str:\n",
    "    return (\n",
    "        \"Usa MEMORIA + FEW-SHOT EXTRA como guía. Extrae spans SOLO para estas etiquetas: OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION.\\n\"\n",
    "        \"Devuelve SOLO JSON válido con clave 'spans'.\\n\"\n",
    "        \"Cada span debe tener: tag, quote.\\n\\n\"\n",
    "        \"MEMORIA:\\n\"\n",
    "        f\"{memory_block}\\n\\n\"\n",
    "        \"FEW-SHOT EXTRA:\\n\"\n",
    "        f\"{fewshot_extra}\\n\\n\"\n",
    "        \"TEXTO:\\n<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fcd341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Generación chat (sin warnings) + predictor seguro\n",
    "@torch.no_grad()\n",
    "def generate_chat(system: str, user: str, max_new_tokens: int = 900-1200, temperature: float = 0.0, top_p: float = 0.9) -> str:\n",
    "    messages = [{\"role\":\"system\",\"content\":system}, {\"role\":\"user\",\"content\":user}]\n",
    "    enc = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc.get(\"attention_mask\")\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    temperature=None, #no necesitamos temperatura ni top_p asi que podríamos omitirlo para evitar los warnings\n",
    "    top_p=None, #no necesitamos temperatura ni top_p asi que podríamos omitirlo para evitar los warnings\n",
    "    )\n",
    "    \n",
    "    if temperature and temperature > 0: \n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    out = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def predict_one(doc_id: str, text: str, exp: int) -> Dict[str,Any]:\n",
    "    if exp == 1:\n",
    "        user = build_user_exp1(text)\n",
    "    elif exp == 2:\n",
    "        user = build_user_exp2(text, memory_block)\n",
    "    elif exp == 3:\n",
    "        user = build_user_exp3(text, memory_block, fewshot_extra)\n",
    "    else:\n",
    "        raise ValueError(\"exp debe ser 1,2,3\")\n",
    "\n",
    "    raw = generate_chat(SYSTEM_BASE, user, max_new_tokens=900, temperature=0.0)\n",
    "    pred, err = parse_and_validate(raw, doc_id, text)\n",
    "\n",
    "# Solo por conveniencia: si hubo error y parse_and_validate no guardó raw (por cambios futuros), lo añadimos.\n",
    "    if err and \"_raw\" not in pred:\n",
    "        pred[\"_raw\"] = raw\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1adc0-7f07-4fd8-a675-82a3a6145ed8",
   "metadata": {},
   "source": [
    "## Instrucciones del prompt y experimentos\n",
    "\n",
    "En `predict_one`:\n",
    "- Exp1 llama: `build_user_exp1(text)` → solo “instrucciones + texto” (system + user), sin memoria ni ejemplos extra.\n",
    "- Exp2 llama: `build_user_exp2(text, memory_block)` → instrucciones + memoria + texto\n",
    "- Exp3 llama: `build_user_exp3(text, memory_block, fewshot_extra)` → instrucciones + memoria + few-shot + texto\n",
    "\n",
    "Y en todos los casos el system es SYSTEM_BASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e0a124e-814c-4498-907b-445547f158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.0.0. Definimos run_experiment + save_jsonl + rutas de salida (esto se ejecuta una vez), después se ejecutan los experimentos\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def run_experiment(docs: List[Dict[str,Any]], exp: int, name: str, n_limit: Optional[int]=None) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    counter = Counter()\n",
    "    docs2 = docs if n_limit is None else docs[:n_limit]\n",
    "    for i,d in enumerate(docs2, start=1):\n",
    "        k = doc_key(d)  # <- usa la función canónica\n",
    "        pred = predict_one(k, d[\"text\"], exp=exp)  # <- key, no doc_uid\n",
    "        if \"_error\" in pred:\n",
    "            counter[pred[\"_error\"]] += 1\n",
    "        out.append(pred)\n",
    "        if i % 5 == 0:\n",
    "            print(f\"{name}: {i}/{len(docs2)} | errors:\", dict(counter))\n",
    "    print(\"DONE\", name, \"| total:\", len(out), \"| errors:\", dict(counter))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _jsonable(x):\n",
    "    if isinstance(x, BaseException):\n",
    "        return repr(x)\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return list(x)\n",
    "    return x\n",
    "\n",
    "def save_jsonl(rows: List[Dict[str,Any]], path: Path):\n",
    "    import json\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False, default=_jsonable) + \"\\n\")\n",
    "\n",
    "OUT1 = OUT_DIR / \"exp1_gold_val.jsonl\"\n",
    "OUT2 = OUT_DIR / \"exp2_gold_val.jsonl\"\n",
    "OUT3 = OUT_DIR / \"exp3_gold_val.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0860a0e-ab0f-46f1-9d23-bf2eadc62a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP1: 5/5 | errors: {}\n",
      "DONE EXP1 | total: 5 | errors: {}\n",
      "Saved: /home/jovyan/inesagent/outputs/predictions/exp1_gold_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 13.0) Ejecutamos solo Exp1 (quick test)\n",
    "#Como estamos ajustando prompts/parsers: usa n_limit=5 para pruebas. Cuando vaya bien, subimos a 20/35\n",
    "\n",
    "pred1 = run_experiment(gold_val, exp=1, name=\"EXP1\", n_limit=5)\n",
    "save_jsonl(pred1, OUT1)\n",
    "print(\"Saved:\", OUT1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a858a263-5586-4cc6-bcdb-819790b00a85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 13.0.1) Ejecutamos solo Exp2 (quick test) con 5 muestras\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pred2 = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEXP2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m save_jsonl(pred2, OUT2)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, OUT2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(docs, exp, name, n_limit)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs2, start=\u001b[32m1\u001b[39m):\n\u001b[32m     12\u001b[39m     k = doc_key(d)  \u001b[38;5;66;03m# <- usa la función canónica\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     pred = \u001b[43mpredict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# <- key, no doc_uid\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pred:\n\u001b[32m     15\u001b[39m         counter[pred[\u001b[33m\"\u001b[39m\u001b[33m_error\u001b[39m\u001b[33m\"\u001b[39m]] += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mpredict_one\u001b[39m\u001b[34m(doc_id, text, exp)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mexp debe ser 1,2,3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     raw = \u001b[43mgenerate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSYSTEM_BASE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m900\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     pred, err = parse_and_validate(raw, doc_id, text)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Solo por conveniencia: si hubo error y parse_and_validate no guardó raw (por cambios futuros), lo añadimos.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mgenerate_chat\u001b[39m\u001b[34m(system, user, max_new_tokens, temperature, top_p)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     21\u001b[39m     gen_kwargs.update(\u001b[38;5;28mdict\u001b[39m(do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m gen_ids = out[\u001b[32m0\u001b[39m, input_ids.shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(gen_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/generation/utils.py:2215\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2207\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2208\u001b[39m         input_ids=input_ids,\n\u001b[32m   2209\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2210\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2211\u001b[39m         **model_kwargs,\n\u001b[32m   2212\u001b[39m     )\n\u001b[32m   2214\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2215\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2216\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2220\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2226\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2227\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2228\u001b[39m         batch_size=batch_size,\n\u001b[32m   2229\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2234\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2235\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/generation/utils.py:3206\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3203\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3205\u001b[39m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3206\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3208\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3209\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3210\u001b[39m     outputs,\n\u001b[32m   3211\u001b[39m     model_kwargs,\n\u001b[32m   3212\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3213\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1189\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1203\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pretraining_tp > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    933\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    934\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    935\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    942\u001b[39m         position_embeddings,\n\u001b[32m    943\u001b[39m     )\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    673\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    689\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:582\u001b[39m, in \u001b[36mLlamaSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    580\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    581\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     key_states, value_states = \u001b[43mpast_key_value\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m key_states = repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m.num_key_value_groups)\n\u001b[32m    585\u001b[39m value_states = repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m.num_key_value_groups)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/cache_utils.py:408\u001b[39m, in \u001b[36mDynamicCache.update\u001b[39m\u001b[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[33;03m    Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[33;03m    to the number of layers in the model.\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.key_cache)\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    410\u001b[39m     key_states: torch.Tensor,\n\u001b[32m    411\u001b[39m     value_states: torch.Tensor,\n\u001b[32m    412\u001b[39m     layer_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    413\u001b[39m     cache_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    414\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    415\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[32m    417\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m \u001b[33;03m        A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    431\u001b[39m     \u001b[38;5;66;03m# Update the number of seen tokens\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 13.0.1) Ejecutamos solo Exp2 (quick test) con 5 muestras\n",
    "pred2 = run_experiment(gold_val, exp=2, name=\"EXP2\", n_limit=5)\n",
    "save_jsonl(pred2, OUT2)\n",
    "print(\"Saved:\", OUT2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7110c3f-8335-4fad-a219-e6bdfedf538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n",
      "n_spans_raw (ejemplos): []\n",
      "n_spans_kept (ejemplos): []\n"
     ]
    }
   ],
   "source": [
    "#b. Ver el origen de all_spans_discarded de forma agregada para saber si el mismatch es siempre el mismo patrón (saltos de línea, espacios, comillas, etc.)\n",
    "#Si n_spans_kept es >0, confirma que el problema está solo en strict_verify\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "errs = Counter([p.get(\"_error\",\"\") for p in pred1 if p.get(\"_error\")])\n",
    "print(errs)\n",
    "\n",
    "metas = [p.get(\"_meta\", {}) for p in pred1 if p.get(\"_error\") == \"all_spans_discarded\"]\n",
    "print(\"n_spans_raw (ejemplos):\", [m.get(\"n_spans_raw\") for m in metas[:10]])\n",
    "print(\"n_spans_kept (ejemplos):\", [m.get(\"n_spans_kept\") for m in metas[:10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4797506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP1: 5/5 | errors: {}\n",
      "DONE EXP1 | total: 5 | errors: {}\n",
      "EXP2: 5/5 | errors: {}\n",
      "DONE EXP2 | total: 5 | errors: {}\n",
      "EXP3: 5/5 | errors: {'all_spans_discarded': 5}\n",
      "DONE EXP3 | total: 5 | errors: {'all_spans_discarded': 5}\n",
      "Saved: /home/jovyan/inesagent/outputs/predictions/exp1_gold_val.jsonl /home/jovyan/inesagent/outputs/predictions/exp2_gold_val.jsonl /home/jovyan/inesagent/outputs/predictions/exp3_gold_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 13) Ejecutar Exp1/Exp2/Exp3 sobre gold_val (rápido) y guardar JSONL con 5 muestras. Ajustar muestras: n_limit=20/35/50\n",
    "\n",
    "pred1 = run_experiment(gold_val, exp=1, name=\"EXP1\", n_limit=5)\n",
    "pred2 = run_experiment(gold_val, exp=2, name=\"EXP2\", n_limit=5)\n",
    "pred3 = run_experiment(gold_val, exp=3, name=\"EXP3\", n_limit=5)\n",
    "\n",
    "save_jsonl(pred1, OUT1)\n",
    "save_jsonl(pred2, OUT2)\n",
    "save_jsonl(pred3, OUT3)\n",
    "\n",
    "print(\"Saved:\", OUT1, OUT2, OUT3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c7740-7e45-4648-b049-3df9cc3d359d",
   "metadata": {},
   "source": [
    "## Errors: {'all_spans_discarded'}\n",
    "`EXP3: 5/5 | errors: {'all_spans_discarded': 5}`\n",
    "`DONE EXP3 | total: 5 | errors: {'all_spans_discarded': 5}`\n",
    "\n",
    "en Exp3 el modelo está “copiando” el patrón del few-shot y devolviendo offsets plantilla (0–275, 1045–1153, 1345–1458, 2465–…) que no corresponden al texto real. Por eso `strict_verify` compara `text[start:end]` vs `quote` y no coincide en ninguno, y acabas en `all_spans_discarded`.\n",
    "\n",
    "**Por qué Exp1/Exp2 sí y Exp3 no**\n",
    "- Exp1/Exp2: le pediste `tag+quote` y rellenas offsets buscando el quote en el texto → suele funcionar.\n",
    "- Exp3: le metes few-shot con offsets (o con “ventanas” con offsets relativos) y el modelo aprende una “estructura” fija y repite números (36–275, 1045–1153…) aunque el texto cambie.\n",
    "\n",
    "Fíjate: en tus 5 docs Exp3 siempre usa casi los mismos rangos. Eso NO puede ser real.\n",
    "\n",
    "**¿De dónde salían esos offsets “1045/1345/2465…”?** De tu few-shot extra: en algún momento has generado ejemplos con:\n",
    "\n",
    "- texto recortado/ventana\n",
    "- offsets relativos al window\n",
    "- y/o valores “de muestra” (36, 275, 500, 724…)\n",
    "\n",
    "El modelo está imitando el formato y recicla números.\n",
    "- Solución práctica: en Exp3 ignora los start/end del modelo\n",
    "- Para Exp3 tienes que forzar el mismo comportamiento que en Exp1: usar SOLO quote y recalcular offsets siempre.\n",
    "\n",
    "**Parche mínimo (sin rehacer nada): “modo quote-only”**\n",
    "- En `parse_and_validate`, después de parsear/mergear, añade esto:\n",
    "    - si `exp==3`: borra start/end de todos los spans antes de `sanitize_pred_dict` (o dentro).\n",
    "    - o más general: si `strict_verify` deja 0 spans, reintenta recalculando offsets desde quote para TODOS.\n",
    "\n",
    "Como ahora tu `parse_and_validate` no recibe `exp`, la opción más limpia es la segunda: reintento automático si `strict_verify` deja 0. Añadimos este “rescate” dentro de `parse_and_validate` (después de `strict_verify`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2938c-38ee-4294-88db-44e54865194e",
   "metadata": {},
   "source": [
    "## Errores con offsets del modelo\n",
    "\n",
    "Sabiendo que los resultados del debug fueron, para el caso [1]:\n",
    "\n",
    "    doc_uid: e65775141ab5c82bd0bd1f89e4873090c43a9569\n",
    "    error: all_spans_discarded\n",
    "    meta: {'n_spans_raw': 4, 'n_spans_kept': 4, 'n_spans_final': 0}\n",
    "\n",
    "    RAW (inicio):\n",
    "     {\"spans\": [{\"tag\": \"OBJETO\", \"start\": 143, \"end\": 153, \"quote\": \"Obras de sustituci\\u00f3n del sistema de climatizaci\\u00f3n en el auditorio del edificio de Biolog\\u00eda de la Facultad de Ciencias de la Universidad Aut\\u00f3noma de Madrid\"}, \n",
    "\n",
    "y\n",
    "\n",
    "    [1] label=OBJETO start=143 end=153\n",
    "    quote: 'Obras de sustitución del sistema de climatización en el auditorio del edificio de Biología de la Facultad de Ciencias de la Universidad Autónoma de Madrid'\n",
    "    text[start:end]: 'bre y repr' **(!!)**\n",
    "    match: False\n",
    "\n",
    "Podemos decir que el modelo te está dando offsets (`143`…) como si el texto empezara en “Obras…”, pero en tu `text` real esa frase está en `1573`.\n",
    "Eso pasa cuando el modelo no está calculando offsets sobre el texto completo, sino sobre una versión recortada o distinta del texto (p.ej. solo la sección de “CLÁUSULAS” o un extracto), o simplemente inventa offsets (muy común).\n",
    "\n",
    "Como ya confirmaste que `find()` da `1573`, la solución práctica es:\n",
    "1) Dejar de creer los offsets del modelo\n",
    "Tu pipeline debe tratar start/end del modelo como “sugerencias” y hacer esto:\n",
    "- usar el quote como verdad\n",
    "- recalcular offsets en el texto real con find(quote)\n",
    "- y solo aceptar si el quote aparece (idealmente una sola vez)\n",
    "\n",
    "Esto exactamente lo que acabas de implementar con el `strict_verify` reparador. Con ese cambio, el span OBJETO quedará:\n",
    "- `quote = “…Obras de sustitución…”`\n",
    "- `pos = 1573`\n",
    "- `start=1573, end=1573+len(quote)`\n",
    "- y ya no habrá `all_spans_discarded`.\n",
    "\n",
    "2) ¿Por qué el modelo devuelve `143` entonces?\n",
    "Puede ser cualquiera de estas (todas típicas):\n",
    "    A) El modelo “no sabe” calcular offsets globales\n",
    "        - Muchísimos LLMs fallan con offsets en textos largos. A veces ponen números pequeños “porque sí”.\n",
    "    B) Tu prompt/plantilla induce al modelo a pensar que el texto empieza en otro punto\n",
    "        - Si el modelo se fija en una sección posterior (“CLÁUSULAS CONTRACTUALES…”) puede tomar esa como inicio mental.\n",
    "    C) Texto muy largo + atención limitada\n",
    "\n",
    "Aunque le metas todo el texto, puede “anclar” el cálculo de offsets a lo que tiene más cerca en contexto.\n",
    "\n",
    "Unicode NO: en tu caso el quote está como \\u00f3 pero eso al parsear vuelve a “ó”. No cambia la longitud del string original de tu text, y además el mismatch es de 143 vs 1573 (enorme), no de 1–2 chars.\n",
    "\n",
    "**(*) Opción recomendada (robusta y simple)**: Cambiar el contrato de salida:\n",
    "- que el modelo NO devuelva offsets, solo label + quote, y yo calculo offsets siempre haciendo `find()` y añadiendo `start/end`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2307d-d4ba-417a-9709-457f5d51848a",
   "metadata": {},
   "source": [
    "**La causa más probable (99%): normalización de texto**\n",
    "`strict_verify` suele fallar por:\n",
    "- `\\r\\n` vs `\\n`\n",
    "- espacios múltiples vs uno\n",
    "- comillas tipográficas “ ” vs \"\n",
    "- guiones largos – vs -\n",
    "- OCR con caracteres raros\n",
    "- el modelo devolviendo quote sin exactamente las mismas nuevas líneas\n",
    "\n",
    "Tu verificación es “exact string match” y es demasiado estricta para texto con OCR/ruido.\n",
    "\n",
    "Hacemos un arreglo (estratégicamente):\n",
    "\n",
    "**Estrategia 1**: si quote no coincide, buscarlo literalmente en el texto y corregir offsets\n",
    "- Si el modelo te da quote, intenta text.find(quote).\n",
    "- Si aparece una sola vez, reemplaza start/end por esa posición.\n",
    "- Si aparece varias veces, o no aparece, descarta.\n",
    "\n",
    "Esto mantiene precisión (solo aceptamos quotes que están realmente en el texto), pero no dependemos de offsets del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf9a20-88a8-47ba-bece-85fb5c320b75",
   "metadata": {},
   "source": [
    "## ¿Los offsets son correctos con respecto al texto?\n",
    "\n",
    "✅ Sí, si pasaron por `strict_verify()` (y/o por `fill_offsets_from_quote()` + `verify`).\n",
    "Esto es validación dura.\n",
    "\n",
    "## ¿Los offsets son correctos con respecto a las anotaciones GOLD (mismo span que gold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45cb0773-3a7b-45d8-aeaf-7e932ea8d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK spans: 20 | BAD spans: 0\n",
      "OK spans: 16 | BAD spans: 0\n",
      "OK spans: 0 | BAD spans: 0\n"
     ]
    }
   ],
   "source": [
    "#Checks de offsets contra texto (100% determinista)\n",
    "def check_offsets_vs_text(pred_rows, gold_val):\n",
    "    # index por id\n",
    "    id_to_text = {str(doc_key(d)): d[\"text\"] for d in gold_val}\n",
    "\n",
    "    ok = 0\n",
    "    bad = 0\n",
    "    for r in pred_rows:\n",
    "        k = str(r[\"doc_key_id\"])\n",
    "        t = id_to_text.get(k)\n",
    "        if t is None:\n",
    "            print(\"MISSING TEXT FOR:\", k)\n",
    "            bad += 1\n",
    "            continue\n",
    "\n",
    "        for sp in r.get(\"spans\", []):\n",
    "            s, e, q = sp[\"start\"], sp[\"end\"], sp[\"quote\"]\n",
    "            if t[s:e] != q:\n",
    "                bad += 1\n",
    "                print(\"\\nBAD:\", k, sp[\"label\"], (s,e))\n",
    "                print(\"expected:\", repr(t[s:e][:120]))\n",
    "                print(\"got     :\", repr(q[:120]))\n",
    "            else:\n",
    "                ok += 1\n",
    "    print(\"OK spans:\", ok, \"| BAD spans:\", bad)\n",
    "check_offsets_vs_text(pred1, gold_val[:5])\n",
    "check_offsets_vs_text(pred2, gold_val[:5])\n",
    "check_offsets_vs_text(pred3, gold_val[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dedb75-f9bf-4cd2-9757-00bfc59a0eef",
   "metadata": {},
   "source": [
    "**Si te da BAD spans: 0, offsets perfectos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30d2bacf-116c-4e32-a8ee-dddce272d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOC 416404459 | gold tags: 3 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 45\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 12\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 11\n",
      "  pred RESOLUCION -> gold has none\n",
      "\n",
      "DOC 588320436 | gold tags: 6 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 75\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 26\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 56\n",
      "  pred RESOLUCION -> gold has none\n",
      "\n",
      "DOC 81854174 | gold tags: 4 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 63\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 14\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 57\n",
      "  pred RESOLUCION -> gold has none\n",
      "\n",
      "DOC 2132006064 | gold tags: 6 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 186\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 84\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 105\n",
      "  RESOLUCION: best overlap chars = 128\n",
      "\n",
      "DOC 954871603 | gold tags: 5 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 72\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 15\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 0\n",
      "  RESOLUCION: best overlap chars = 172\n"
     ]
    }
   ],
   "source": [
    "# Check contra gold tags (match con anotación)\n",
    "# Primero necesitas mapear el doc del split al doc del gold completo (por id).\n",
    "# En tu gold real los tags son tipo: {\"start\":..., \"end\":..., \"tag\":\"OBJETO\", \"token_start\":..., \"token_end\":...}\n",
    "\n",
    "# gold_full: lista de docs gold cargados de PATH_GOLD (con keys: id, text, tags)\n",
    "gold_full = gold\n",
    "gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "\n",
    "def overlap(a_start, a_end, b_start, b_end):\n",
    "    return max(0, min(a_end,b_end) - max(a_start,b_start))\n",
    "\n",
    "def check_against_gold(pred_rows, gold_full):\n",
    "    gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "    for r in pred_rows:\n",
    "        k = str(r[\"doc_key_id\"])\n",
    "        g = gold_by_id.get(k)\n",
    "        if not g:\n",
    "            print(\"No gold for:\", k)\n",
    "            continue\n",
    "\n",
    "        gold_tags = [t for t in g.get(\"tags\", []) if t.get(\"tag\") in MVP_LABELS]\n",
    "        print(\"\\nDOC\", k, \"| gold tags:\", len(gold_tags), \"| pred spans:\", len(r.get(\"spans\", [])))\n",
    "\n",
    "        for sp in r.get(\"spans\", []):\n",
    "            lab = sp[\"label\"]\n",
    "            s,e = sp[\"start\"], sp[\"end\"]\n",
    "\n",
    "            candidates = [t for t in gold_tags if t.get(\"tag\")==lab]\n",
    "            if not candidates:\n",
    "                print(\"  pred\", lab, \"-> gold has none\")\n",
    "                continue\n",
    "\n",
    "            # mira mejor solape\n",
    "            best = max(overlap(s,e, t[\"start\"], t[\"end\"]) for t in candidates)\n",
    "            print(f\"  {lab}: best overlap chars = {best}\")\n",
    "\n",
    "check_against_gold(pred1, gold_full=gold)  # 'gold' = tu gold cargado de PATH_GOLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee2df7b7-286b-48e5-9c52-dc4776af12de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOC 416404459 | gold tags: 3 | pred spans: 1\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 12\n",
      "\n",
      "DOC 588320436 | gold tags: 6 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 75\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 26\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 56\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 0\n",
      "\n",
      "DOC 81854174 | gold tags: 4 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 63\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 14\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 57\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 0\n",
      "\n",
      "DOC 2132006064 | gold tags: 6 | pred spans: 3\n",
      "  OBJETO: best overlap chars = 186\n",
      "  RESOLUCION: best overlap chars = 128\n",
      "  RESOLUCION: best overlap chars = 0\n",
      "\n",
      "DOC 954871603 | gold tags: 5 | pred spans: 4\n",
      "  OBJETO: best overlap chars = 72\n",
      "  PRECIO_DEL_CONTRATO: best overlap chars = 15\n",
      "  DURACION_TOTAL_DEL_CONTRATO: best overlap chars = 270\n",
      "  RESOLUCION: best overlap chars = 172\n"
     ]
    }
   ],
   "source": [
    "# Check pred2 contra gold tags (match con anotación)\n",
    "# Primero necesitas mapear el doc del split al doc del gold completo (por id).\n",
    "# En tu gold real los tags son tipo: {\"start\":..., \"end\":..., \"tag\":\"OBJETO\", \"token_start\":..., \"token_end\":...}\n",
    "\n",
    "# gold_full: lista de docs gold cargados de PATH_GOLD (con keys: id, text, tags)\n",
    "gold_full = gold\n",
    "gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "\n",
    "def overlap(a_start, a_end, b_start, b_end):\n",
    "    return max(0, min(a_end,b_end) - max(a_start,b_start))\n",
    "\n",
    "def check_against_gold(pred_rows, gold_full):\n",
    "    gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "    for r in pred_rows:\n",
    "        k = str(r[\"doc_key_id\"])\n",
    "        g = gold_by_id.get(k)\n",
    "        if not g:\n",
    "            print(\"No gold for:\", k)\n",
    "            continue\n",
    "\n",
    "        gold_tags = [t for t in g.get(\"tags\", []) if t.get(\"tag\") in MVP_LABELS]\n",
    "        print(\"\\nDOC\", k, \"| gold tags:\", len(gold_tags), \"| pred spans:\", len(r.get(\"spans\", [])))\n",
    "\n",
    "        for sp in r.get(\"spans\", []):\n",
    "            lab = sp[\"label\"]\n",
    "            s,e = sp[\"start\"], sp[\"end\"]\n",
    "\n",
    "            candidates = [t for t in gold_tags if t.get(\"tag\")==lab]\n",
    "            if not candidates:\n",
    "                print(\"  pred\", lab, \"-> gold has none\")\n",
    "                continue\n",
    "\n",
    "            # mira mejor solape\n",
    "            best = max(overlap(s,e, t[\"start\"], t[\"end\"]) for t in candidates)\n",
    "            print(f\"  {lab}: best overlap chars = {best}\")\n",
    "\n",
    "check_against_gold(pred2, gold_full=gold)  # 'gold' = tu gold cargado de PATH_GOLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9d1de50-c8da-47a2-9bd6-069796e23b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DOC 416404459 | gold tags: 3 | pred spans: 0\n",
      "\n",
      "DOC 588320436 | gold tags: 6 | pred spans: 0\n",
      "\n",
      "DOC 81854174 | gold tags: 4 | pred spans: 0\n",
      "\n",
      "DOC 2132006064 | gold tags: 6 | pred spans: 0\n",
      "\n",
      "DOC 954871603 | gold tags: 5 | pred spans: 0\n"
     ]
    }
   ],
   "source": [
    "# Check pred3 contra gold tags (match con anotación)\n",
    "# Primero necesitas mapear el doc del split al doc del gold completo (por id).\n",
    "# En tu gold real los tags son tipo: {\"start\":..., \"end\":..., \"tag\":\"OBJETO\", \"token_start\":..., \"token_end\":...}\n",
    "\n",
    "# gold_full: lista de docs gold cargados de PATH_GOLD (con keys: id, text, tags)\n",
    "gold_full = gold\n",
    "gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "\n",
    "def overlap(a_start, a_end, b_start, b_end):\n",
    "    return max(0, min(a_end,b_end) - max(a_start,b_start))\n",
    "\n",
    "def check_against_gold(pred_rows, gold_full):\n",
    "    gold_by_id = {str(d[\"id\"]): d for d in gold_full}\n",
    "    for r in pred_rows:\n",
    "        k = str(r[\"doc_key_id\"])\n",
    "        g = gold_by_id.get(k)\n",
    "        if not g:\n",
    "            print(\"No gold for:\", k)\n",
    "            continue\n",
    "\n",
    "        gold_tags = [t for t in g.get(\"tags\", []) if t.get(\"tag\") in MVP_LABELS]\n",
    "        print(\"\\nDOC\", k, \"| gold tags:\", len(gold_tags), \"| pred spans:\", len(r.get(\"spans\", [])))\n",
    "\n",
    "        for sp in r.get(\"spans\", []):\n",
    "            lab = sp[\"label\"]\n",
    "            s,e = sp[\"start\"], sp[\"end\"]\n",
    "\n",
    "            candidates = [t for t in gold_tags if t.get(\"tag\")==lab]\n",
    "            if not candidates:\n",
    "                print(\"  pred\", lab, \"-> gold has none\")\n",
    "                continue\n",
    "\n",
    "            # mira mejor solape\n",
    "            best = max(overlap(s,e, t[\"start\"], t[\"end\"]) for t in candidates)\n",
    "            print(f\"  {lab}: best overlap chars = {best}\")\n",
    "\n",
    "check_against_gold(pred3, gold_full=gold)  # 'gold' = tu gold cargado de PATH_GOLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467963d4-3d26-4d74-94ae-32e0c08fac14",
   "metadata": {},
   "source": [
    "**Interpretación rápida:**\n",
    "- best overlap chars = 0 → el modelo eligió un sitio distinto al gold para esa etiqueta.\n",
    "- overlap grande pero no total → el modelo eligió un subspan o superspan.\n",
    "- overlap cercano al largo → casi coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf622ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== exp1_gold_val.jsonl ===\n",
      "doc_key_id: 416404459 | spans: 4 | error: None\n",
      "  OBJETO 243 288 | quote[:80]= VIGILANCIA DE LA SALUD (MEDICINA DEL TRABAJO)\n",
      "  PRECIO_DEL_CONTRATO 2216 2290 | quote[:80]= 377.772,00 €, más el correspondiente IVA (6.869,52 €), Total: 384.641,52 €\n",
      "  DURACION_TOTAL_DEL_CONTRATO 2363 2374 | quote[:80]= CUATRO años\n",
      "doc_key_id: 588320436 | spans: 4 | error: None\n",
      "  OBJETO 2273 2461 | quote[:80]= Suministro de repuesto y accesorios para los sistemas de armas de aeronaves De a\n",
      "  PRECIO_DEL_CONTRATO 3355 3408 | quote[:80]= CIENTO CINCUENTA MIL EUROS, IVA EXENTO (150.000,00 €)\n",
      "  DURACION_TOTAL_DEL_CONTRATO 3717 3773 | quote[:80]= desde su formalización hasta el 30 de noviembre de 2024.\n",
      "\n",
      "=== exp2_gold_val.jsonl ===\n",
      "doc_key_id: 416404459 | spans: 1 | error: None\n",
      "  PRECIO_DEL_CONTRATO 2178 2304 | quote[:80]= El PRECIO del presente contrato es de 377.772,00 €, más el correspondiente IVA (\n",
      "doc_key_id: 588320436 | spans: 4 | error: None\n",
      "  OBJETO 2273 2461 | quote[:80]= Suministro de repuesto y accesorios para los sistemas de armas de aeronaves De a\n",
      "  PRECIO_DEL_CONTRATO 3355 3408 | quote[:80]= CIENTO CINCUENTA MIL EUROS, IVA EXENTO (150.000,00 €)\n",
      "  DURACION_TOTAL_DEL_CONTRATO 3717 3773 | quote[:80]= desde su formalización hasta el 30 de noviembre de 2024.\n",
      "\n",
      "=== exp3_gold_val.jsonl ===\n",
      "doc_key_id: 416404459 | spans: 0 | error: all_spans_discarded\n",
      "doc_key_id: 588320436 | spans: 0 | error: all_spans_discarded\n"
     ]
    }
   ],
   "source": [
    "# 14) Supervisor: inspección rápida de outputs guardados\n",
    "def read_jsonl(path: Path, n: int = 3):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i >= n: break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "for path in [OUT1, OUT2, OUT3]:\n",
    "    print(\"\\n===\", path.name, \"===\")\n",
    "    rows = read_jsonl(path, n=2)\n",
    "    for r in rows:\n",
    "        print(\"doc_key_id:\", r.get(\"doc_key_id\"), \"| spans:\", len(r.get(\"spans\",[])), \"| error:\", r.get(\"_error\"))\n",
    "        for sp in r.get(\"spans\", [])[:3]:\n",
    "            print(\" \", sp[\"label\"], sp[\"start\"], sp[\"end\"], \"| quote[:80]=\", sp[\"quote\"][:80].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844e586-8945-4a68-b35f-c43154db5444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c285d8-63bd-487d-b7e4-82acef772e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f6df2-10a4-49f2-bbad-9fe859216e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032550a4-6f6a-4dce-ad31-6a2ec424f37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
