{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909da99b-a2ce-4a90-8706-352d16358bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n",
      "MEM_DIR exists: True /home/jovyan/inesagent/outputs/memory\n"
     ]
    }
   ],
   "source": [
    "#Celda 1 — Imports + paths\n",
    "from pathlib import Path\n",
    "import json, re, hashlib\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "MEM_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "\n",
    "PATH_VAL_OLD = SPLITS_DIR / \"val_gold.jsonl\"        # el que tenía doc_uid\n",
    "PATH_TEST_OLD = SPLITS_DIR / \"test_gold.jsonl\"\n",
    "PATH_TRAIN_OLD = SPLITS_DIR / \"train_gold.jsonl\"\n",
    "PATH_PR_OLD = SPLITS_DIR / \"prompt_regression_gold.jsonl\"\n",
    "\n",
    "PATH_GOLD = ROOT / \"gold\" / \"corpus_annotated.jsonl\"  # gold con id/text/tags\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"MEM_DIR exists:\", MEM_DIR.exists(), MEM_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9693705-87be-4a77-911e-f83fa64e075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 2 — loaders + normalización + hash\n",
    "def load_json(p: Path):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(p: Path):\n",
    "    rows = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def save_json(p: Path, obj):\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def save_jsonl(p: Path, rows: List[Dict[str,Any]]):\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def sha1_str(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547fd39d-05cf-4ca3-a1e0-2cc84f51f4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legacy_to_id: 357 miss: 0\n",
      "0 ddfd9d0d476258da87bc3add8c5e286f010c6234 -> -844396723\n",
      "1 2431e385374c0db888716e5c17ffbb74c06442b3 -> -1829156687\n",
      "2 70fb5e9adcfbaff0f2dcd5ab63b47170d7e670dd -> 993677416\n"
     ]
    }
   ],
   "source": [
    "#Celda 3 — Construir mapping legacy doc_uid -> gold id - Esto usa tus splits antiguos (doc_uid+text) y el gold (id+text+tags)\n",
    "gold_all = load_jsonl(PATH_GOLD)\n",
    "gold_by_hash = {sha1_str(norm_text(d.get(\"text\",\"\"))): d for d in gold_all}\n",
    "\n",
    "legacy_sources = []\n",
    "for p in [PATH_VAL_OLD, PATH_TEST_OLD, PATH_TRAIN_OLD, PATH_PR_OLD]:\n",
    "    if p.exists():\n",
    "        legacy_sources.extend(load_jsonl(p))\n",
    "\n",
    "legacy_to_id = {}\n",
    "miss = 0\n",
    "\n",
    "for d in legacy_sources:\n",
    "    legacy = d.get(\"doc_uid\")\n",
    "    t = d.get(\"text\",\"\")\n",
    "    if not legacy or not t:\n",
    "        continue\n",
    "    h = sha1_str(norm_text(t))\n",
    "    g = gold_by_hash.get(h)\n",
    "    if not g:\n",
    "        miss += 1\n",
    "        continue\n",
    "    legacy_to_id[legacy] = g[\"id\"]\n",
    "\n",
    "print(\"legacy_to_id:\", len(legacy_to_id), \"miss:\", miss)\n",
    "# sanity: print 3 ejemplos\n",
    "for i,(k,v) in enumerate(list(legacy_to_id.items())[:3]):\n",
    "    print(i, k, \"->\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55540f7-5b05-40ed-88be-00619a1915f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed files: 4\n",
      "  removed_doc_uids.json -> removed_ids.json\n",
      "  README_removed_doc_uids.txt -> README_removed_ids.txt\n",
      "  blocked_doc_uids_by_memory.json -> blocked_ids_by_memory.json\n",
      "  blocked_doc_uids.json -> blocked_ids.json\n"
     ]
    }
   ],
   "source": [
    "#Celda 4: renombramos en disco ficheros en outputs/memory que contengan doc_uids en el nombre\n",
    "def rename_files_in_dir(dir_path: Path):\n",
    "    renamed = []\n",
    "    for p in dir_path.glob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        new_name = p.name\n",
    "        new_name = new_name.replace(\"doc_uids\", \"ids\")\n",
    "        new_name = new_name.replace(\"doc_uid\", \"id\")\n",
    "        if new_name != p.name:\n",
    "            new_path = p.with_name(new_name)\n",
    "            p.rename(new_path)\n",
    "            renamed.append((p.name, new_name))\n",
    "    return renamed\n",
    "\n",
    "renamed = rename_files_in_dir(MEM_DIR)\n",
    "print(\"Renamed files:\", len(renamed))\n",
    "for old,new in renamed:\n",
    "    print(\" \", old, \"->\", new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46276ad7-b983-4469-a7b5-15423029e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrar contenido JSON / JSONL\n",
    "# Celda 5 — transformador recursivo\n",
    "def replace_legacy_ids(obj: Any) -> Any:\n",
    "    # Recursivo: dict, list, str, etc.\n",
    "    if isinstance(obj, dict):\n",
    "        new = {}\n",
    "        for k,v in obj.items():\n",
    "            nk = k\n",
    "            if nk == \"doc_uid\": nk = \"id\"\n",
    "            if nk == \"doc_uids\": nk = \"ids\"\n",
    "            if nk == \"blocked_doc_uids\": nk = \"blocked_ids\"\n",
    "            if nk == \"removed_doc_uids\": nk = \"removed_ids\"\n",
    "\n",
    "            nv = replace_legacy_ids(v)\n",
    "\n",
    "            # si el valor es legacy doc_uid en campo id-like\n",
    "            if nk in {\"id\", \"ids\", \"blocked_ids\", \"removed_ids\"}:\n",
    "                if isinstance(nv, str) and nv in legacy_to_id:\n",
    "                    nv = legacy_to_id[nv]\n",
    "                elif isinstance(nv, list):\n",
    "                    nv = [legacy_to_id.get(x, x) if isinstance(x, str) else x for x in nv]\n",
    "\n",
    "            new[nk] = nv\n",
    "        return new\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        return [replace_legacy_ids(x) for x in obj]\n",
    "\n",
    "    if isinstance(obj, str):\n",
    "        # OJO: no sustituimos strings arbitrarios salvo que sean exactamente una key de legacy_to_id\n",
    "        # Esto evita “corromper” textos.\n",
    "        return legacy_to_id.get(obj, obj)\n",
    "\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c3a127-19cb-4a1f-bb26-0ce955156b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrated files: 10\n"
     ]
    }
   ],
   "source": [
    "#Celda 6 — migrar todos los ficheros en outputs/memory\n",
    "def migrate_file(p: Path):\n",
    "    suffix = p.suffix.lower()\n",
    "    if suffix == \".json\":\n",
    "        data = load_json(p)\n",
    "        new = replace_legacy_ids(data)\n",
    "        save_json(p, new)\n",
    "        return True\n",
    "    elif suffix == \".jsonl\":\n",
    "        rows = load_jsonl(p)\n",
    "        new_rows = [replace_legacy_ids(r) for r in rows]\n",
    "        save_jsonl(p, new_rows)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "migrated = 0\n",
    "for p in MEM_DIR.glob(\"*\"):\n",
    "    if p.is_file() and p.suffix.lower() in {\".json\", \".jsonl\"}:\n",
    "        ok = migrate_file(p)\n",
    "        if ok:\n",
    "            migrated += 1\n",
    "\n",
    "print(\"Migrated files:\", migrated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d923a07-2a44-4c6d-90f7-20e8e41c5f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files still containing doc_uid/doc_uids: []\n"
     ]
    }
   ],
   "source": [
    "#Celda 7 — grep-like (sin terminal) verificamos si queda algo de doc_uids en memoria\n",
    "def file_contains(p: Path, pattern: str) -> bool:\n",
    "    try:\n",
    "        txt = p.read_text(encoding=\"utf-8\")\n",
    "        return pattern in txt\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "left = []\n",
    "for p in MEM_DIR.glob(\"*\"):\n",
    "    if p.is_file() and p.suffix.lower() in {\".json\",\".jsonl\"}:\n",
    "        if file_contains(p, \"doc_uid\") or file_contains(p, \"doc_uids\"):\n",
    "            left.append(p.name)\n",
    "\n",
    "print(\"Files still containing doc_uid/doc_uids:\", left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0834f712-0d3c-49d1-9374-ab30a1beb735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
