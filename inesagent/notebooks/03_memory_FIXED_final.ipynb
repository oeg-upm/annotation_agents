{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c513e39-4eb8-440e-bb3a-a6fb800a22f4",
   "metadata": {},
   "source": [
    "**Notebook 03 — Construcción de memoria/few-shot (guías+gold) + lista de excluidos (anti-leakage)**\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "Este notebook construye la memoria few-shot que se usará en los prompts del agente, a partir de:\n",
    "- Guías de anotación MVP (guidelines_MVP_defs.txt)\n",
    "- Corpus gold anotado\n",
    "\n",
    "**Qué hace exactamente este NB03**\n",
    "- Lee la guía en formato TXT y extrae citas literales entre corchetes [...]\n",
    "→ solo con fines de auditoría y alineación conceptual (no se usan directamente en el prompt).\n",
    "- Carga el gold y construye un pool de spans reales para las 4 etiquetas MVP:\n",
    "    - OBJETO\n",
    "    - PRECIO_DEL_CONTRATO\n",
    "    - DURACION_TOTAL_DEL_CONTRATO\n",
    "    - RESOLUCION\n",
    "- Selecciona un conjunto reducido y representativo de ejemplos (memoria), uno por criterio/subtipo cuando aplica.\n",
    "- Genera:\n",
    "    - outputs/memory/memory_examples.json\n",
    "    - outputs/memory/blocked_doc_uids.json (para evitar leakage en val/test/prompt_regression en NB02)\n",
    "\n",
    "**Importante**\n",
    "- ❌ No se usa DOCX (antes las guías estaban en DOCX pero no cogía los corchetes)\n",
    "- ❌ No se usa corpus sin anotar\n",
    "- ✅ Toda la memoria sale del gold\n",
    "\n",
    "En este NB03 NO necesitamos cargar el corpus sin anotar para construir memoria, porque la memoria la construimos desde el gold (como decidimos). Solo usaremos las guías para ver/confirmar criterios y ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefdbc13-715d-401c-968a-edf184ff634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "assert ROOT.exists()\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189f0820-7a6a-431e-8bc6-73dde6fc06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_GUIDE existe: True\n",
      "/home/jovyan/inesagent/config/guidelines_MVP_defs.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_GUIDE = ROOT / \"config\" / \"guidelines_MVP_defs.txt\"\n",
    "\n",
    "print(\"PATH_GUIDE existe:\", PATH_GUIDE.exists())\n",
    "print(PATH_GUIDE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6e7c96-a905-40ed-9836-b64c30477339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports y configuración base  \n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86f91f9-6abf-49b0-98ed-37240ab271f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "#Rutas del proyecto (servidor) \n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "# Guía MVP en TXT (fuente de criterios / ejemplos)\n",
    "PATH_GUIDE = ROOT / \"config\" / \"guidelines_MVP_defs.txt\"\n",
    "assert PATH_GUIDE.exists(), f\"No encuentro {PATH_GUIDE}\"\n",
    "\n",
    "# Gold anotado\n",
    "PATH_GOLD = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "assert PATH_GOLD.exists(), f\"No encuentro {PATH_GOLD}\"\n",
    "\n",
    "# Salidas\n",
    "OUT_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATH_MEMORY = OUT_DIR / \"memory_examples.json\"\n",
    "PATH_BLOCKED = OUT_DIR / \"blocked_doc_uids.json\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1c6f10-fa9e-4a81-b6e2-6469ced97a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuracion MVP \n",
    "MVP_LABELS = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074771ba-692a-421a-b29a-8d4453f7b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilidades generales \n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return \" \".join((s or \"\").lower().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c03b7aa-6317-43fd-bc57-a304bf487313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citas únicas extraídas de la guía: 28\n",
      "Ejemplos:\n",
      "- suministro de armarios consigna con apertura individual mediante cerradura electrónica para los edificios del Banco de España en Lumbreras\n",
      "- Explotación de la aplicación móvil para personal de la Armada Española del expediente 2022/EA02/00001693E\n",
      "- SERVICIOS DE RESTAURACIÓN DE UN CONJUNTO DE 45 OBRAS PERTENECIENTES AL FONDO HISTÓRICO DE LA BIBLIOTECA DEL SENADO (LOTE 1)\n",
      "- SERVICIO         DE         CARGA         Y         DESCARGA         DE    ELEMENTOS  ESCENOGRÁFICOS Y DELA CARROZA PARA LA FUNDACIÓN DEL  TEATRO REAL F.S.P.\n",
      "- 14.567 €\n"
     ]
    }
   ],
   "source": [
    "#leer guia TXT y extraer citas [...] Solo auditoría / trazabilidad, no afecta a la memoria final\n",
    "text_guide = PATH_GUIDE.read_text(encoding=\"utf-8\")\n",
    "\n",
    "BRACKET_RE = re.compile(r\"\\[(.+?)\\]\", flags=re.DOTALL)\n",
    "\n",
    "quotes = [q.strip() for q in BRACKET_RE.findall(text_guide) if q.strip()]\n",
    "\n",
    "# deduplicar preservando orden\n",
    "seen = set()\n",
    "quotes_unique = []\n",
    "for q in quotes:\n",
    "    key = normalize(q)\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        quotes_unique.append(q)\n",
    "\n",
    "print(\"Citas únicas extraídas de la guía:\", len(quotes_unique))\n",
    "print(\"Ejemplos:\")\n",
    "for q in quotes_unique[:5]:\n",
    "    print(\"-\", q[:200])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91307fd-b7a5-464d-a562-b6cd6b8b5c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs gold: 373\n"
     ]
    }
   ],
   "source": [
    "#Cargar gold y preparar indices \n",
    "gold = load_jsonl(PATH_GOLD)\n",
    "print(\"Docs gold:\", len(gold))\n",
    "\n",
    "# En gold, la clave canónica es 'id'. Solo si faltase (caso raro) usamos sha1(texto).\n",
    "gold_id_set = set()\n",
    "for d in gold:\n",
    "    if \"id\" in d:\n",
    "        gold_id_set.add(d[\"id\"])\n",
    "\n",
    "key_to_text = {}\n",
    "key_to_tags = {}\n",
    "key_type = {}  # key -> 'id' o 'uid'\n",
    "\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    if \"id\" in d:\n",
    "        k = d[\"id\"]\n",
    "        key_type[k] = \"id\"\n",
    "    else:\n",
    "        k = stable_uid(txt)\n",
    "        key_type[k] = \"uid\"\n",
    "    key_to_text[k] = txt\n",
    "    key_to_tags[k] = d.get(\"tags\", [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b2140-466f-4e4a-bbf8-9921b42849a1",
   "metadata": {},
   "source": [
    "Esta salida quiere decir:\n",
    "- _**“Docs con al menos 1 span MVP: 373”**_:  En corpus_annotated.jsonl, hay 373 documentos que tienen al menos una anotación de OBJETO / PRECIO_DEL_CONTRATO / DURACION_TOTAL_DEL_CONTRATO / RESOLUCION.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e383845-87bb-4a43-be0d-9872f7767ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJETO spans: 925\n",
      "PRECIO_DEL_CONTRATO spans: 436\n",
      "DURACION_TOTAL_DEL_CONTRATO spans: 468\n",
      "RESOLUCION spans: 526\n"
     ]
    }
   ],
   "source": [
    "#Pool de spans MVP desde gold \n",
    "pool = defaultdict(list)\n",
    "\n",
    "for k, txt in key_to_text.items():\n",
    "    for t in key_to_tags.get(k, []):\n",
    "        lab = t.get(\"tag\")\n",
    "        if lab not in MVP_LABELS.values():\n",
    "            continue\n",
    "        s, e = int(t[\"start\"]), int(t[\"end\"])\n",
    "        if 0 <= s < e <= len(txt):\n",
    "            span_txt = txt[s:e].strip()\n",
    "            if span_txt:\n",
    "                pool[lab].append({\n",
    "                    # por compatibilidad, mantenemos el nombre 'doc_uid',\n",
    "                    # pero aquí es el 'id' de gold (o uid si el doc no tenía id)\n",
    "                    \"doc_uid\": k,\n",
    "                    \"key_type\": key_type.get(k, \"id\"),\n",
    "                    \"start\": s,\n",
    "                    \"end\": e,\n",
    "                    \"text\": span_txt\n",
    "                })\n",
    "\n",
    "for lab in MVP_LABELS.values():\n",
    "    print(lab, \"spans:\", len(pool[lab]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a600baf4-d49e-425f-8cfe-d31b6441ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccion de memoria (simple, robusta) - estrategia: P1,P2 (1 ej), P4,P9 (4 ejs variados) \n",
    "memory = []\n",
    "blocked_uids = set()\n",
    "\n",
    "def add_example(label, criterion, ex):\n",
    "    memory.append({\n",
    "        \"label\": label,\n",
    "        \"criterion\": criterion,\n",
    "        \"doc_uid\": ex[\"doc_uid\"],\n",
    "        \"key_type\": ex.get(\"key_type\",\"id\"),\n",
    "        \"start\": ex[\"start\"],\n",
    "        \"end\": ex[\"end\"]\n",
    "    })\n",
    "    blocked_uids.add(ex[\"doc_uid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fed821-ffa4-4c83-86b9-85db7b13ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P1 y P2 (1 ejemplo de cada una, ya que tienen su criterio definido) \n",
    "for code in [\"P1\", \"P2\"]:\n",
    "    label = MVP_LABELS[code]\n",
    "    ex = random.choice(pool[label])\n",
    "    add_example(\n",
    "        label=label,\n",
    "        criterion=f\"{code} — ejemplo representativo (gold)\",\n",
    "        ex=ex\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecde5307-6018-47b0-ba39-ea931bb03e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P4 y P9 (variedad de ejemplos) \n",
    "def pick_k(label, k):\n",
    "    xs = pool[label].copy()\n",
    "    random.shuffle(xs)\n",
    "    return xs[:k]\n",
    "\n",
    "for code, k in [(\"P4\", 4), (\"P9\", 4)]:\n",
    "    label = MVP_LABELS[code]\n",
    "    for ex in pick_k(label, k):\n",
    "        add_example(\n",
    "            label=label,\n",
    "            criterion=f\"{code} — ejemplo representativo (gold)\",\n",
    "            ex=ex\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60664b4f-c124-42ff-a6e2-7b438708f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory examples: 10\n",
      "Docs bloqueados (id): 10\n",
      "Docs bloqueados (uid): 0\n",
      "Guardado:\n",
      "- /home/jovyan/inesagent/outputs/memory/memory_examples.json\n",
      "- /home/jovyan/inesagent/outputs/memory/blocked_keys_by_memory.json\n",
      "- /home/jovyan/inesagent/outputs/memory/blocked_doc_uids.json (legacy)\n"
     ]
    }
   ],
   "source": [
    "#Validacion y guardado \n",
    "print(\"Memory examples:\", len(memory))\n",
    "\n",
    "# Separar bloqueos por tipo de clave (id vs uid)\n",
    "blocked_ids = sorted({k for k in blocked_uids if isinstance(k, int) or (isinstance(k,str) and k in gold_id_set)})\n",
    "blocked_uids_only = sorted({k for k in blocked_uids if k not in set(blocked_ids)})\n",
    "\n",
    "print(\"Docs bloqueados (id):\", len(blocked_ids))\n",
    "print(\"Docs bloqueados (uid):\", len(blocked_uids_only))\n",
    "\n",
    "# 1) Memoria (ejemplos)\n",
    "PATH_MEMORY.write_text(\n",
    "    json.dumps(memory, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# 2) Fichero nuevo robusto (recomendado)\n",
    "PATH_BLOCKED_KEYS = OUT_DIR / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCKED_KEYS.write_text(\n",
    "    json.dumps({\"blocked_ids\": blocked_ids, \"blocked_uids\": blocked_uids_only}, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# 3) Fichero legacy (por compatibilidad con notebooks antiguos)\n",
    "PATH_BLOCKED.write_text(\n",
    "    json.dumps(sorted(blocked_uids), ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Guardado:\")\n",
    "print(\"-\", PATH_MEMORY)\n",
    "print(\"-\", PATH_BLOCKED_KEYS)\n",
    "print(\"-\", PATH_BLOCKED, \"(legacy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664b3c1-cfaa-42f8-9cf2-a09845708423",
   "metadata": {},
   "source": [
    "**La guía TXT tiene dos patrones de subtipo:**\n",
    "- P1.1 <título en la misma línea> ✅\n",
    "- P4.01 sin título (solo el código) y el contenido va en la línea siguiente ✅\n",
    "- y además hay P4.09 (con 0 delante o sin 0): ambos deben entrar.\n",
    "\n",
    "RE_SUB actual exige “algo” después del código (porque captura (.+?)), así que se salta los casos P4.01 que vienen solos.\n",
    "\n",
    "Corregimos regex + parser que:\n",
    "- Detecta P1/P2/P4/P9 main (sin confundir P1.1 con P1.)\n",
    "- Detecta subtipos con y sin título en la misma línea (P4.01 solo)\n",
    "- Soporta P4.9 y P4.09 (uno o dos dígitos)\n",
    "- Captura Criterio: tanto para main como para subtipo\n",
    "- Captura ejemplos [...] incluso si el bloque de ejemplos ocupa varias líneas\n",
    "- Al final imprime lo que pedías: tipos detectados + subtipos por tipo + nº de citas por tipo/subtipo\n",
    "\n",
    "Requisito previo: tener text_guide con el contenido del TXT (ya lo tienes al leer guidelines_MVP_defs.txt). \n",
    "\n",
    "**Por qué esta versión sí captura tus casos raros**\n",
    "- P4.01: ahora RE_SUB acepta que el título sea vacío ((.*)), por tanto lo detecta como subtipo aunque no haya texto en esa línea.\n",
    "- P4.9 y P4.09: \\d{1,2} admite 1 o 2 dígitos.\n",
    "- No confunde P1.1 con P1.: RE_MAIN tiene (?!\\d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e53fd44-7d91-4e9f-86ab-c49f24bbe2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos detectados: ['P1', 'P2', 'P4', 'P9']\n",
      "P1 subtipos (incluyendo códigos tipo P4.01): 3 | +MAIN_pseudo: 1\n",
      "P2 subtipos (incluyendo códigos tipo P4.01): 3 | +MAIN_pseudo: 1\n",
      "P4 subtipos (incluyendo códigos tipo P4.01): 9 | +MAIN_pseudo: 0\n",
      "P9 subtipos (incluyendo códigos tipo P4.01): 12 | +MAIN_pseudo: 0\n",
      "\n",
      "P1 subtipos detectados: ['P1.1', 'P1.2', 'P1.3'] \n",
      "\n",
      "P2 subtipos detectados: ['P2.1', 'P2.2', 'P2.3'] \n",
      "\n",
      "P4 subtipos detectados: ['P4.01', 'P4.02', 'P4.03', 'P4.04', 'P4.05', 'P4.06', 'P4.07', 'P4.08', 'P4.09'] \n",
      "\n",
      "P9 subtipos detectados: ['P9.01', 'P9.02', 'P9.03', 'P9.04', 'P9.05', 'P9.06', 'P9.07', 'P9.08', 'P9.09', 'P9.10', 'P9.11', 'P9.12'] \n"
     ]
    }
   ],
   "source": [
    "#parsear guia TXT en estructura (tipos/subtipos+criterio) \n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# MAIN: P1. Objeto  (pero NO P1.1 ...)\n",
    "RE_MAIN = re.compile(r\"^\\s*(P(?:1|2|4|9))\\.(?!\\d)\\s*(.+?)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "# SUBTIPO:\n",
    "# - P1.1 Título...\n",
    "# - P4.01\n",
    "# - P4.9\n",
    "# - P4.09\n",
    "# Permitimos título opcional (puede estar vacío)\n",
    "RE_SUB = re.compile(r\"^\\s*(P(?:1|2|4|9)\\.(?:\\d{1,2})(?:\\.\\d+)*)\\s*(.*)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "RE_CRIT = re.compile(r\"^\\s*Criterio\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE)\n",
    "RE_EJ   = re.compile(r\"^\\s*Ejemplos?\\s*:\\s*(.*)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "BRACKET_RE = re.compile(r\"\\[(.+?)\\]\", flags=re.DOTALL)\n",
    "\n",
    "def norm_line(s: str) -> str:\n",
    "    return (s or \"\").replace(\"\\ufeff\", \"\").strip()\n",
    "\n",
    "guide = {\n",
    "    \"P1\": {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}},\n",
    "    \"P2\": {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}},\n",
    "    \"P4\": {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}},\n",
    "    \"P9\": {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}},\n",
    "}\n",
    "\n",
    "def ensure_sub(main: str, subcode: str):\n",
    "    guide[main][\"subtypes\"].setdefault(subcode, {\"title\": \"\", \"criterion\": \"\", \"examples\": []})\n",
    "\n",
    "lines = [norm_line(ln) for ln in text_guide.splitlines()]\n",
    "lines = [ln for ln in lines if ln]  # remove empty\n",
    "\n",
    "current_code = None     # \"P1\" o \"P4.01\" etc.\n",
    "current_main = None     # \"P1\" etc.\n",
    "in_examples_block = False\n",
    "\n",
    "for ln in lines:\n",
    "    # 1) MAIN\n",
    "    m = RE_MAIN.match(ln)\n",
    "    if m:\n",
    "        main = m.group(1).upper()\n",
    "        title = m.group(2).strip()\n",
    "        if main in guide:\n",
    "            guide[main][\"title\"] = title\n",
    "            current_main = main\n",
    "            current_code = main\n",
    "            in_examples_block = False\n",
    "        continue\n",
    "\n",
    "    # 2) SUBTIPO (título puede estar vacío)\n",
    "    m = RE_SUB.match(ln)\n",
    "    if m:\n",
    "        subcode = m.group(1).upper()\n",
    "        title = (m.group(2) or \"\").strip()\n",
    "        main = subcode.split(\".\")[0]\n",
    "        if main in guide:\n",
    "            ensure_sub(main, subcode)\n",
    "            # Si el título va en la misma línea, lo guardamos; si no, se queda vacío\n",
    "            if title:\n",
    "                guide[main][\"subtypes\"][subcode][\"title\"] = title\n",
    "            current_main = main\n",
    "            current_code = subcode\n",
    "            in_examples_block = False\n",
    "        continue\n",
    "\n",
    "    # 3) CRITERIO (se asigna al bloque actual: subtipo si estamos en subtipo, si no main)\n",
    "    m = RE_CRIT.match(ln)\n",
    "    if m and current_code:\n",
    "        crit = m.group(1).strip()\n",
    "        if \".\" in current_code:\n",
    "            main = current_code.split(\".\")[0]\n",
    "            ensure_sub(main, current_code)\n",
    "            guide[main][\"subtypes\"][current_code][\"criterion\"] = crit\n",
    "        else:\n",
    "            guide[current_code][\"criterion\"] = crit\n",
    "        in_examples_block = False\n",
    "        continue\n",
    "\n",
    "    # 4) EJEMPLOS: activa modo ejemplos; puede haber texto después de \"Ejemplos:\" en la misma línea\n",
    "    m = RE_EJ.match(ln)\n",
    "    if m:\n",
    "        in_examples_block = True\n",
    "        ln = (m.group(1) or \"\").strip()\n",
    "        # seguimos a extracción de corchetes debajo (no continue)\n",
    "\n",
    "    # 5) Extraer quotes si estamos en bloque de ejemplos o si la línea contiene corchetes\n",
    "    if in_examples_block or (\"[\" in ln and \"]\" in ln):\n",
    "        for mm in BRACKET_RE.finditer(ln):\n",
    "            q = (mm.group(1) or \"\").strip()\n",
    "            if not q:\n",
    "                continue\n",
    "\n",
    "            if current_code and \".\" in current_code:\n",
    "                main = current_code.split(\".\")[0]\n",
    "                ensure_sub(main, current_code)\n",
    "                guide[main][\"subtypes\"][current_code][\"examples\"].append(q)\n",
    "\n",
    "            elif current_code in guide:\n",
    "                # ejemplo a nivel etiqueta main -> pseudo-subtipo MAIN\n",
    "                pseudo = f\"{current_code}.MAIN\"\n",
    "                ensure_sub(current_code, pseudo)\n",
    "                if not guide[current_code][\"subtypes\"][pseudo][\"title\"]:\n",
    "                    guide[current_code][\"subtypes\"][pseudo][\"title\"] = \"(ejemplo a nivel etiqueta)\"\n",
    "                # criterio del main si existe\n",
    "                if guide[current_code][\"criterion\"] and not guide[current_code][\"subtypes\"][pseudo][\"criterion\"]:\n",
    "                    guide[current_code][\"subtypes\"][pseudo][\"criterion\"] = guide[current_code][\"criterion\"]\n",
    "                guide[current_code][\"subtypes\"][pseudo][\"examples\"].append(q)\n",
    "\n",
    "# ---- Report solicitado\n",
    "print(\"Tipos detectados:\", [k for k in guide if guide[k][\"title\"]])\n",
    "for k in [\"P1\",\"P2\",\"P4\",\"P9\"]:\n",
    "    subs = guide[k][\"subtypes\"]\n",
    "    # recuento subtipos \"reales\" (sin contar P1.MAIN / P2.MAIN pseudo)\n",
    "    real = [s for s in subs if not s.endswith(\".MAIN\")]\n",
    "    print(f\"{k} subtipos (incluyendo códigos tipo P4.01): {len(real)} | +MAIN_pseudo: {len(subs)-len(real)}\")\n",
    "\n",
    "# opcional: ver rápidamente qué subtipos detectó\n",
    "for k in [\"P1\",\"P2\",\"P4\",\"P9\"]:\n",
    "    real = sorted([s for s in guide[k][\"subtypes\"] if not s.endswith(\".MAIN\")])\n",
    "    print(f\"\\n{k} subtipos detectados:\", real[:30], (\"...\" if len(real)>30 else \"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5003bdd-1708-44ae-a280-ad06072512f6",
   "metadata": {},
   "source": [
    " **¿Por qué no hay MAIN de P9?**\n",
    " En el TXT, de P9 no hay ejemplos “a nivel main” (P9 sin decimal) con corchetes [...], o al menos no los estás capturando bajo el bloque current_code == \"P9\".\n",
    "- En tu parser, el “MAIN_pseudo” solo se crea cuando ocurre esto:\n",
    "- Estamos dentro de un bloque main (por ejemplo current_code == \"P9\")\n",
    "- Aparece una cita [...] en líneas de “Ejemplos” entonces guardamos esa cita como P9.MAIN (pseudo-subtipo)\n",
    "\n",
    "En tu extracto de P9, justo después de:\n",
    "\n",
    "`P9. Resolución\n",
    "Criterio: ...\n",
    "P9.01\n",
    "Ejemplos: [...]\n",
    "P9.02\n",
    "Ejemplos: [...]\n",
    "...`\n",
    "\n",
    "\n",
    "Todas las citas [...] aparecen bajo P9.01, P9.02, etc. No hay una sección tipo:\n",
    "`P9. Resolución`\n",
    "`Ejemplos: [ ... ]` antes de P9.01.\n",
    "- Así que el parser nunca necesita crear P9.MAIN: ya está asignando los ejemplos al subtipo concreto, y por eso te sale:\n",
    "- P9 real subtypes = 12\n",
    "- +MAIN_pseudo = 0\n",
    "\n",
    "Mientras que en P1 y P2 sí estás viendo MAIN_pseudo: 1 porque en el TXT sí hay un ejemplo general en el bloque P1. y P2.:\n",
    "- P1: Ejemplos: ... [suministro de armarios ...] antes de P1.1\n",
    "- P2: Ejemplos: ... [14.567 €] antes de P2.1\n",
    "- En P4 te sale MAIN_pseudo: 1 por otra razón interesante: tu P4 “tiene Ejemplos” antes del primer subtipo o te está entrando algún [...] cuando current_code == \"P4\", probablemente por el bloque inicial de P4 si en el TXT hay alguna cita entre corchetes en la sección general (o si alguna línea con corchetes aparece antes de detectar P4.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa9b993-5072-49b7-9495-fdd77f9b4d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_selected_AUTO tamaño: 16\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/memory_selected_AUTO.json\n"
     ]
    }
   ],
   "source": [
    "#creamos memory_selected_AUTO.json (aleatorio) (MVP 14-16) con convención P4/P9 fija \n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "FIXED_CRITERION = {\n",
    "    \"P4\": \"P4 – criterio único (subtipos solo ejemplificativos)\",\n",
    "    \"P9\": \"P9 – criterio único (subtipos solo ejemplificativos)\",\n",
    "}\n",
    "\n",
    "P_TO_LABEL = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "\n",
    "def pick_one(label):\n",
    "    xs = pool.get(label, [])\n",
    "    return random.choice(xs) if xs else None\n",
    "\n",
    "memory_selected_AUTO = []\n",
    "\n",
    "# P1: 1 MAIN + 1 por subtipo real (los que existan)\n",
    "p1_subs = [s for s in guide[\"P1\"][\"subtypes\"].keys() if s != \"P1.MAIN\"]\n",
    "# Asegura MAIN\n",
    "p1_codes = [\"P1.MAIN\"] + sorted([s for s in p1_subs if s.startswith(\"P1.\")])[:3]  # si hay 3 subtipos en tu guía\n",
    "for code in p1_codes:\n",
    "    ex = pick_one(P_TO_LABEL[\"P1\"])\n",
    "    if not ex: \n",
    "        continue\n",
    "    info = guide[\"P1\"][\"subtypes\"].get(code, {\"title\":\"\", \"criterion\":\"\"})\n",
    "    criterion = f\"{code} {info.get('title','')}\".strip()\n",
    "    if info.get(\"criterion\"):\n",
    "        criterion += f\" — {info['criterion']}\"\n",
    "    memory_selected_AUTO.append({\"label\": P_TO_LABEL[\"P1\"], \"criterion\": criterion, **{k: ex[k] for k in [\"doc_uid\",\"start\",\"end\"]}})\n",
    "\n",
    "# P2: 1 MAIN + 1 por subtipo real\n",
    "p2_subs = [s for s in guide[\"P2\"][\"subtypes\"].keys() if s != \"P2.MAIN\"]\n",
    "p2_codes = [\"P2.MAIN\"] + sorted([s for s in p2_subs if s.startswith(\"P2.\")])[:3]\n",
    "for code in p2_codes:\n",
    "    ex = pick_one(P_TO_LABEL[\"P2\"])\n",
    "    if not ex: \n",
    "        continue\n",
    "    info = guide[\"P2\"][\"subtypes\"].get(code, {\"title\":\"\", \"criterion\":\"\"})\n",
    "    criterion = f\"{code} {info.get('title','')}\".strip()\n",
    "    if info.get(\"criterion\"):\n",
    "        criterion += f\" — {info['criterion']}\"\n",
    "    memory_selected_AUTO.append({\"label\": P_TO_LABEL[\"P2\"], \"criterion\": criterion, **{k: ex[k] for k in [\"doc_uid\",\"start\",\"end\"]}})\n",
    "\n",
    "# P4/P9: 4 ejemplos de variedad (por ahora random, se curará después)\n",
    "for _ in range(4):\n",
    "    ex = pick_one(P_TO_LABEL[\"P4\"])\n",
    "    if ex:\n",
    "        memory_selected_AUTO.append({\"label\": P_TO_LABEL[\"P4\"], \"criterion\": FIXED_CRITERION[\"P4\"], **{k: ex[k] for k in [\"doc_uid\",\"start\",\"end\"]}})\n",
    "for _ in range(4):\n",
    "    ex = pick_one(P_TO_LABEL[\"P9\"])\n",
    "    if ex:\n",
    "        memory_selected_AUTO.append({\"label\": P_TO_LABEL[\"P9\"], \"criterion\": FIXED_CRITERION[\"P9\"], **{k: ex[k] for k in [\"doc_uid\",\"start\",\"end\"]}})\n",
    "\n",
    "print(\"memory_selected_AUTO tamaño:\", len(memory_selected_AUTO))\n",
    "PATH_AUTO = OUT_DIR / \"memory_selected_AUTO.json\"\n",
    "PATH_AUTO.write_text(json.dumps(memory_selected_AUTO, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado:\", PATH_AUTO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaca369d-015a-4a83-bbf5-d2bebf851aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs bloqueados por memoria | ids: 16 | uids: 0\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/blocked_keys_by_memory.json\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/blocked_doc_uids_by_memory.json (legacy)\n"
     ]
    }
   ],
   "source": [
    "#Este es el anti-leakage real: bloquea los docs usados en la memoria\n",
    "blocked_keys = sorted({ex[\"doc_uid\"] for ex in memory_selected_AUTO})\n",
    "\n",
    "blocked_ids = sorted({k for k in blocked_keys if isinstance(k, int) or (isinstance(k,str) and k in gold_id_set)})\n",
    "blocked_uids_only = sorted({k for k in blocked_keys if k not in set(blocked_ids)})\n",
    "\n",
    "# Nuevo formato robusto\n",
    "PATH_BLOCK = OUT_DIR / \"blocked_keys_by_memory.json\"\n",
    "PATH_BLOCK.write_text(\n",
    "    json.dumps({\"blocked_ids\": blocked_ids, \"blocked_uids\": blocked_uids_only}, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Legacy para compatibilidad\n",
    "PATH_BLOCK_LEGACY = OUT_DIR / \"blocked_doc_uids_by_memory.json\"\n",
    "PATH_BLOCK_LEGACY.write_text(json.dumps(blocked_keys, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Docs bloqueados por memoria | ids:\", len(blocked_ids), \"| uids:\", len(blocked_uids_only))\n",
    "print(\"Guardado:\", PATH_BLOCK)\n",
    "print(\"Guardado:\", PATH_BLOCK_LEGACY, \"(legacy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737334a-47ad-4a96-9ee2-0a146bde7289",
   "metadata": {},
   "source": [
    "**Construir pool de spans (solo gold_mvp, excluyendo removed)**\n",
    "\n",
    "OJO: aquí hay dos modos:\n",
    "- Modo A (recomendado): memoria NO debe venir de removed → excluimos removed del pool.\n",
    "- Modo B: permites memoria desde removed pero luego bloqueas evaluación.\n",
    "\n",
    "Como ya dijimos “retiramos esos documentos para evitar leakage”, vamos con Modo A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93beb4cc-e279-41f4-85e5-ff0327541d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_selected tamaño: 16\n",
      "Docs únicos usados: 16\n"
     ]
    }
   ],
   "source": [
    "#curated memory P1/P2 por subtipo con criterio, P4/P9 por variedad \n",
    "P_TO_LABEL = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "\n",
    "FIXED_CRITERION = {\n",
    "    \"P4\": \"P4 – criterio único (subtipos solo ejemplificativos)\",\n",
    "    \"P9\": \"P9 – criterio único (subtipos solo ejemplificativos)\",\n",
    "}\n",
    "\n",
    "def fallback_random(label: str):\n",
    "    xs = pool.get(label, [])\n",
    "    return random.choice(xs) if xs else None\n",
    "\n",
    "memory_selected = []\n",
    "used_doc_uids = set()\n",
    "\n",
    "def add_example(label, criterion, ex):\n",
    "    memory_selected.append({\n",
    "        \"label\": label,\n",
    "        \"criterion\": criterion,\n",
    "        \"doc_uid\": ex[\"doc_uid\"],\n",
    "        \"start\": ex[\"start\"],\n",
    "        \"end\": ex[\"end\"],\n",
    "    })\n",
    "    used_doc_uids.add(ex[\"doc_uid\"])\n",
    "\n",
    "# --- P1/P2: 1 ejemplo por subtipo (si tiene criterio; si no, igual 1 por subtipo si hay)\n",
    "for main in [\"P1\", \"P2\"]:\n",
    "    label = P_TO_LABEL[main]\n",
    "    for subcode, info in guide[main][\"subtypes\"].items():\n",
    "        title = (info.get(\"title\") or \"\").strip()\n",
    "        crit  = (info.get(\"criterion\") or \"\").strip()\n",
    "        examples = info.get(\"examples\", [])\n",
    "\n",
    "        criterion_str = f\"{subcode} {title}\".strip()\n",
    "        if crit:\n",
    "            criterion_str += f\" — {crit}\"\n",
    "\n",
    "        # No usamos quotes para localizar offsets (porque ya evitamos removed).\n",
    "        # En vez de eso: sample directo del pool para ese label.\n",
    "        fb = fallback_random(label)\n",
    "        if fb:\n",
    "            add_example(label, criterion_str + \" (gold)\", fb)\n",
    "\n",
    "# --- P4/P9: variedad formal (sin subtipo conceptual)\n",
    "dur_patterns = [\n",
    "    (\"formalizacion\", re.compile(r\"\\bformalizaci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"fechas\", re.compile(r\"\\b\\d{1,2}\\s+de\\s+[a-záéíóú]+\\s+de\\s+\\d{4}\\b\", re.IGNORECASE)),\n",
    "    (\"anos\", re.compile(r\"\\b\\d+\\s+a[nñ]os?\\b\", re.IGNORECASE)),\n",
    "    (\"meses\", re.compile(r\"\\b\\d+\\s+mes(?:es)?\\b\", re.IGNORECASE)),\n",
    "    (\"dias\", re.compile(r\"\\b\\d+\\s+d[ií]as?\\b\", re.IGNORECASE)),\n",
    "    (\"prorroga\", re.compile(r\"\\bpr[oó]rroga\\b\", re.IGNORECASE)),\n",
    "]\n",
    "res_patterns = [\n",
    "    (\"lcsp\", re.compile(r\"\\bLCSP\\b|\\bLey\\s+9/2017\\b|\\bart[íi]culo(?:s)?\\b\", re.IGNORECASE)),\n",
    "    (\"causas\", re.compile(r\"\\bcausas?\\s+de\\s+resoluci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"incumplimiento\", re.compile(r\"\\bincumplim\", re.IGNORECASE)),\n",
    "    (\"rescision\", re.compile(r\"\\brescisi[oó]n\\b|\\brescind\", re.IGNORECASE)),\n",
    "    (\"extincion\", re.compile(r\"\\bextinci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"mutuo_acuerdo\", re.compile(r\"\\bmutuo\\s+acuerdo\\b\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "def pick_varied(label: str, k: int, patterns):\n",
    "    xs = pool[label].copy()\n",
    "    random.shuffle(xs)\n",
    "    picked = []\n",
    "    used_uids = set()\n",
    "\n",
    "    # 1) uno por patrón si se puede\n",
    "    for name, rx in patterns:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        for ex in xs:\n",
    "            if ex[\"doc_uid\"] in used_uids:\n",
    "                continue\n",
    "            if rx.search(ex[\"text\"]):\n",
    "                picked.append(ex)\n",
    "                used_uids.add(ex[\"doc_uid\"])\n",
    "                break\n",
    "\n",
    "    # 2) completar\n",
    "    for ex in xs:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        if ex[\"doc_uid\"] in used_uids:\n",
    "            continue\n",
    "        picked.append(ex)\n",
    "        used_uids.add(ex[\"doc_uid\"])\n",
    "\n",
    "    return picked[:k]\n",
    "\n",
    "for main, label, patterns in [\n",
    "    (\"P4\", P_TO_LABEL[\"P4\"], dur_patterns),\n",
    "    (\"P9\", P_TO_LABEL[\"P9\"], res_patterns),\n",
    "]:\n",
    "    for ex in pick_varied(label, k=4, patterns=patterns):\n",
    "        add_example(label, FIXED_CRITERION[main], ex)\n",
    "\n",
    "print(\"memory_selected tamaño:\", len(memory_selected))\n",
    "print(\"Docs únicos usados:\", len(used_doc_uids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd9ab0d-64f4-4cbb-bde1-aee08faca99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errores de validación: 16\n",
      "\n",
      "[1] OBJETO :: P1.MAIN (ejemplo a nivel etiqueta) — Se anota como “objeto” la información resumida y concisa que describe la prestación principal del contrato definida en el título, antecedentes o en las cláusulas. Se anota el objeto tantas veces como aparezca en el contrato. Se anota como “objeto” los lotes identificados que acompañan el objeto del contrato. (gold)\n",
      "doc_uid: -474927492\n",
      "span: \n",
      "\n",
      "[2] OBJETO :: P1.1 Inclusión de expedientes en objeto — Se anotan como “objeto” los números de expediente que acompañan al objeto del contrato. Dicho expediente debe encontrarse adyacente (precediendo o siguiendo) a la definición de la prestación principal. (gold)\n",
      "doc_uid: 607056704\n",
      "span: \n",
      "\n",
      "[3] OBJETO :: P1.2 Inclusión del lote en el objeto — Se anota como “objeto” los lotes identificados que acompañan el objeto del contrato (gold)\n",
      "doc_uid: 954871603\n",
      "span: \n",
      "\n",
      "[4] OBJETO :: P1.3 Delimitación del objeto si está entrecomillado — Si el objeto está entrecomillado o con comillas repetidas, se anota como “objeto” el objeto del contrato excluyendo todas las comillas que lo acompañan. (gold)\n",
      "doc_uid: 1743757741\n",
      "span: \n",
      "\n",
      "[5] PRECIO_DEL_CONTRATO :: P2.MAIN (ejemplo a nivel etiqueta) — Se etiqueta como “precio_del_contrato” la cuantía total del contrato sin IVA o IGIC, encontrado en antecedentes o en cláusulas del contrato, tanto si está escrito en letra como si está escrito en número junto la divisa correspondiente. Se anota el precio tantas veces como aparezca en el contrato. (gold)\n",
      "doc_uid: -1550131930\n",
      "span: \n",
      "\n",
      "[6] PRECIO_DEL_CONTRATO :: P2.1 Aparición de importes totales con IVA o IGIC — Si el importe de un contrato no aparece sin el IVA o IGIC, se etiqueta como “precio_del_contrato” el precio íntegro con IVA/IGIC precisando específicamente que el importe incluye el IVA/IGIC. (gold)\n",
      "doc_uid: -390138560\n",
      "span: \n",
      "\n",
      "[7] PRECIO_DEL_CONTRATO :: P2.2 Aparición de importes máximos — Se etiqueta como “precio_del_contrato” el precio máximo establecido si el importe total del contrato no aparece (con o sin IVA/IGIC). (gold)\n",
      "doc_uid: -2060846141\n",
      "span: \n",
      "\n",
      "[8] PRECIO_DEL_CONTRATO :: P2.3 Precios en otros formatos — En el caso de encontrar el precio del contrato especificado como cuantía o renta periódica, desglose de precios asociados a bienes o anualidades u otro formato, se anota como “precio_del_contrato” la oración que incluye la cuantía o precio unitario junto a la periodicidad o bien al que acompaña. (gold)\n",
      "doc_uid: -1518316567\n",
      "span: \n",
      "\n",
      "[9] DURACION_TOTAL_DEL_CONTRATO :: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: -498426076\n",
      "span: \n",
      "\n",
      "[10] DURACION_TOTAL_DEL_CONTRATO :: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: 413333747\n",
      "span: \n",
      "\n",
      "[11] DURACION_TOTAL_DEL_CONTRATO :: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: -77639475\n",
      "span: \n",
      "\n",
      "[12] DURACION_TOTAL_DEL_CONTRATO :: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: 53077466\n",
      "span: \n",
      "\n",
      "[13] RESOLUCION :: P9 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: 578676032\n",
      "span: \n",
      "\n",
      "[14] RESOLUCION :: P9 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: 1788076231\n",
      "span: \n",
      "\n",
      "[15] RESOLUCION :: P9 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: 1532102578\n",
      "span: \n",
      "\n",
      "[16] RESOLUCION :: P9 – criterio único (subtipos solo ejemplificativos)\n",
      "doc_uid: -794062986\n",
      "span: \n",
      "\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/memory_selected_CURATED.json\n"
     ]
    }
   ],
   "source": [
    "#validacion offsets y guardado de memoria \n",
    "errors = 0\n",
    "for ex in memory_selected:\n",
    "    uid = str(ex[\"doc_uid\"])\n",
    "    txt = key_to_text.get(key, \"\")\n",
    "    s, e = ex[\"start\"], ex[\"end\"]\n",
    "    if not txt or not (0 <= s < e <= len(txt)) or not txt[s:e].strip():\n",
    "        errors += 1\n",
    "\n",
    "print(\"Errores de validación:\", errors)\n",
    "\n",
    "\n",
    "# Preview (robusto: doc_uid puede ser int o str)\n",
    "for i, ex in enumerate(memory_selected[:16], 1):\n",
    "    uid = str(ex[\"doc_uid\"])              # <- clave como string\n",
    "    txt = key_to_text.get(key, \"\")        # <- evita KeyError\n",
    "    span = txt[ex[\"start\"]:ex[\"end\"]].replace(\"\\n\", \" \") if txt else \"\"\n",
    "\n",
    "    print(f\"\\n[{i}] {ex['label']} :: {ex['criterion']}\")\n",
    "    print(\"doc_uid:\", uid[:12] + \"...\" if len(uid) > 12 else uid)\n",
    "    print(\"span:\", span[:260])\n",
    "\n",
    "PATH_MEM_CURATED = OUT_DIR / \"memory_selected_CURATED.json\"\n",
    "PATH_MEM_CURATED.write_text(json.dumps(memory_selected, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\nGuardado:\", PATH_MEM_CURATED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5d59b5-6dff-43fd-9c56-8310c24c65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selección de memoria (1 ejemplo por criterio/subtipo)\n",
    "#Aquí definimos exactamente lo que irá al prompt en Experimento 2/3 - enfoque híbrido, manual/semi-automático\n",
    "\n",
    "# Rellenamos esta lista a mano con los ejemplos que elijamos \n",
    "# (doc_uid + start/end + label + criterion)\n",
    "memory_selected = [\n",
    "    # Ejemplo (rellenamos con los elegidos):\n",
    "    # {\n",
    "    #   \"label\": \"PRECIO_DEL_CONTRATO\",\n",
    "    #   \"criterion\": \"P1.2\",\n",
    "    #   \"doc_uid\": \"<sha1>\",\n",
    "    #   \"start\": 2877,\n",
    "    #   \"end\": 3026\n",
    "    # },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e93f4-014e-44ef-9111-9ca3c07cbeec",
   "metadata": {},
   "source": [
    "## **README_guidelines_MVP_defs**\n",
    "\n",
    "## Estructura de criterios y subtipos en la guía MVP\n",
    "\n",
    "Las etiquetas incluidas en esta guía MVP no presentan una estructura homogénea de subtipos.\n",
    "\n",
    "### Etiquetas con subtipos conceptuales explícitos\n",
    "Las etiquetas **P1. Objeto** y **P2. Precio del contrato** incluyen:\n",
    "\n",
    "- Un **criterio general** aplicable a la etiqueta principal.\n",
    "- **Subtipos explícitos** (por ejemplo, P1.1, P1.2, P2.1, etc.) que:\n",
    "  - introducen **criterios adicionales o específicos**,\n",
    "  - delimitan casos particulares de anotación,\n",
    "  - y requieren ejemplos diferenciados.\n",
    "\n",
    "En estos casos, la memoria y los ejemplos se organizan **por subtipo**, ya que cada subtipo representa una variación funcional relevante del segmento jurídico anotado.\n",
    "\n",
    "### Etiquetas sin subtipos conceptuales diferenciados\n",
    "Las etiquetas **P4. Duración del contrato** y **P9. Resolución** presentan:\n",
    "\n",
    "- Un **único criterio general** por etiqueta.\n",
    "- Subdivisiones internas (por ejemplo, P4.01, P4.02, P9.03, etc.) que:\n",
    "  - **no introducen nuevos criterios de anotación**,\n",
    "  - sino que agrupan **variantes ejemplificadas** de la misma función jurídica (fechas, plazos, fórmulas, referencias legales, etc.).\n",
    "\n",
    "En estos casos, los subtipos se consideran **variantes de realización textual**, no categorías conceptuales independientes. Por tanto, los ejemplos se emplean para ilustrar diversidad expresiva, sin modificar el criterio de anotación.\n",
    "\n",
    "## Uso de los ejemplos de la guía y prevención de leakage\n",
    "\n",
    "Todos los ejemplos incluidos en esta guía MVP aparecen entre corchetes `[...]` y corresponden a citas literales de contratos reales.\n",
    "\n",
    "Para evitar leakage entre entrenamiento, memoria y evaluación:\n",
    "\n",
    "- Los documentos que contienen cualquiera de estos extractos literales se excluyen de:\n",
    "  - `prompt_regression`\n",
    "  - `validation`\n",
    "  - `test`\n",
    "- Dichos documentos pueden mantenerse en el conjunto de entrenamiento, pero **nunca se emplean para evaluación**, dado que los ejemplos de la guía se utilizan como memoria o few-shot en el prompt del agente.\n",
    "\n",
    "Esta separación garantiza que el agente no sea evaluado sobre segmentos contractuales que ha visto explícitamente como ejemplos.\n",
    "\n",
    "**Cómo reflejar esto en el código / estructuras (para que no se  olvide)**\n",
    "\n",
    "Te recomiendo que, en la memoria (memory_examples.json), sigas esta convención:\n",
    "\n",
    "- Para P1 / P2:\n",
    "\n",
    "{\n",
    "  \"label\": \"OBJETO\",\n",
    "  \"criterion\": \"P1.2 Inclusión del lote en el objeto\",\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "- Para P4 / P9:\n",
    "\n",
    "{\n",
    "  \"label\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "  \"criterion\": \"P4 (criterio general)\",\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "O, si quieres ser aún más explícita:\n",
    "\n",
    "{\n",
    "  \"label\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "  \"criterion\": \"P4 – criterio único (subtipos solo ejemplificativos)\",\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "**Eso hace que:**\n",
    "- el LLM no “busque” diferencias conceptuales donde no las hay,\n",
    "- y tú puedas justificar por qué no hay un ejemplo “por subtipo” en P4/P9.\n",
    "\n",
    "**Justificación científica (por si la necesitas en paper o memoria)**\n",
    "\n",
    "“Los subtipos definidos en la guía se interpretan como categorías conceptuales únicamente cuando introducen criterios de anotación diferenciados; en caso contrario, se tratan como variantes de realización textual de una misma función jurídica.”\n",
    "\n",
    "**Resumen claro**\n",
    "\n",
    "✔️ No todas las etiquetas necesitan subtipos con criterio\n",
    "✔️ P1 y P2 sí → subtipos conceptuales\n",
    "✔️ P4 y P9 no → subtipos ejemplificativos\n",
    "✔️ Lo documentas explícitamente → no hay ambigüedad\n",
    "✔️ El agente aprende qué fijarse (criterio) y qué es solo variación formal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ebcb7-5d56-4ddf-9c9b-a45c89c3117a",
   "metadata": {},
   "source": [
    "Continuamos por donde íbamos:\n",
    "**En NB03, el siguiente tramo lógico (ya con guidelines_MVP_defs pulida) es:**\n",
    "\n",
    "- Extraer, desde guidelines_MVP_defs, los ejemplos entre corchetes [...] y guardarlos como “citas prohibidas” (para evitar leakage).\n",
    "- Localizar esas citas en el gold (Corpus_anotado) y construir un fichero removed_doc_uids.json con los docs que las contienen.\n",
    "- Rehacer el “Explorador de candidatos desde el gold”, pero excluyendo esos removed_doc_uids para que lo que elijas como memoria no contamine val/test/prompt_regression.\n",
    "\n",
    "Esto encaja con tu decisión híbrida: memoria/few-shot desde gold (offsets), y exclusión de evaluación solo para documentos que contienen exactamente los extractos usados en la memoria.\n",
    "\n",
    "A continuación te dejo las celdas “limpias” para pegar en NB03, retomando justo donde dices. (Las asumo después de que ya tienes gold_mvp listo y con doc_uid, doc_id, text, y spans filtrados a tus 4 etiquetas; el formato de tags como lista de dict con start/end/tag es el que tú ya confirmaste, y el corpus gold está en Corpus_anotado.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d77080-0707-4f8c-8f0c-b87ecbe9f2b8",
   "metadata": {},
   "source": [
    "**Con esto ya tenemos la garantía de que lo que estamos eligiendo para memoria no pertenece a docs contaminados por citas de las guías.**\n",
    "Siguientes pasos:\n",
    "\n",
    "1. Lee config/guidelines_MVP_defs.txt\n",
    "2. Extrae tipos (P1, P2, P4, P9) y subtipos (P1.1, P1.2, P4.01, etc.)\n",
    "3. Extrae el Criterio: cuando existe (P1/P2 y sus subtipos)\n",
    "4. Carga tu gold (gold/corpus_annotated.json o .jsonl, ajustable)\n",
    "5. Construye memory_selected eligiendo un ejemplo aleatorio por tipo y subtipo (con offsets correctos)\n",
    "6. Asigna criterion así:\n",
    "    - P1/P2: usa el texto “Criterio:” real del txt (si lo encuentra)\n",
    "    - P4/P9: usa el criterio explícito que acordamos:\n",
    "        - \"P4 – criterio único (subtipos solo ejemplificativos)\"\n",
    "        - \"P9 – criterio único (subtipos solo ejemplificativos)\"\n",
    "\n",
    "**Nota importante (metodológica): como la selección es aleatoria, no garantiza que el span elegido “represente” semánticamente el subtipo. Sirve para validar pipeline y formato. Luego sustituyes por ejemplos verdaderamente alineados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e738fd-b69c-46d5-bc06-682dd8fc259f",
   "metadata": {},
   "source": [
    "Tendríamos 2 formas de hacer la selección: manual de memoria (con mi criterion explícito para P4/P9) o automáticamente (random). Optamos por random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da098325-ca5d-46d7-9aad-60272d6d48f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN: 14078\n",
      "Primeros 200 repr: 'Guía de anotación MVP_defs (P1, P2, P4, P9)\\nP1. Objeto\\nCriterio: Se anota como “objeto” la información resumida y concisa que describe la prestación principal del contrato definida en el título, antecedentes o en las cláusulas. Se anota el objeto tantas veces como aparezca en el contrato. Se anota como “objeto” los lotes identificados que acompañan el objeto del contrato. \\nEjemplos: El objeto del presente contrato es la realización, por parte del contratista, del [suministro de armarios consigna con apertura individual mediante cerradura electrónica para los edificios del Banco de España en Lumbreras] de acuerdo con las especificaciones contenidas en el Pliego de Prescripciones Técnicas. \\nP1.1 Inclusión de expedientes en objeto \\nCriterio: Se anotan como “objeto” los números de expediente q'\n",
      "Mains encontrados (P1./P2./P4./P9.): 4\n",
      "Subtipos encontrados tipo P1.1: 27\n",
      "Subtipos encontrados tipo P4.01: 21\n",
      "\n",
      "Ejemplos de líneas P*:\n",
      "- 'P1. Objeto'\n",
      "- 'P1.1 Inclusión de expedientes en objeto '\n",
      "- 'P1.2 Inclusión del lote en el objeto'\n",
      "- 'P1.3 Delimitación del objeto si está entrecomillado '\n",
      "- 'P2. Precio del contrato'\n",
      "- 'P2.1 Aparición de importes totales con IVA o IGIC'\n",
      "- 'P2.2 Aparición de importes máximos'\n",
      "- 'P2.3 Precios en otros formatos'\n",
      "- 'P4. Duración del contrato '\n",
      "- 'P4.01'\n",
      "- 'P4.02'\n",
      "- 'P4.03'\n",
      "- 'P4.04'\n",
      "- 'P4.05'\n",
      "- 'P4.06'\n",
      "- 'P4.07'\n",
      "- 'P4.08'\n",
      "- 'P4.09'\n",
      "- 'P9. Resolución'\n",
      "- 'P9.01'\n",
      "- 'P9.02'\n",
      "- 'P9.03'\n",
      "- 'P9.04'\n",
      "- 'P9.05'\n",
      "- 'P9.06'\n",
      "- 'P9.07'\n",
      "- 'P9.08'\n",
      "- 'P9.09'\n",
      "- 'P9.10'\n",
      "- 'P9.11'\n",
      "- 'P9.12'\n"
     ]
    }
   ],
   "source": [
    "#debug para ver qué ve el parser \n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_GUIDE_TXT = ROOT / \"config\" / \"guidelines_MVP_defs.txt\"\n",
    "\n",
    "raw = PATH_GUIDE_TXT.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "raw = raw.lstrip(\"\\ufeff\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"\\xa0\", \" \")\n",
    "\n",
    "print(\"LEN:\", len(raw))\n",
    "print(\"Primeros 200 repr:\", repr(raw[:800]))\n",
    "\n",
    "# ¿Hay cabeceras y subtipos en el texto?\n",
    "print(\"Mains encontrados (P1./P2./P4./P9.):\", len(re.findall(r\"(?m)^\\s*P[1249]\\.\\s+\\S+\", raw)))\n",
    "print(\"Subtipos encontrados tipo P1.1:\", len(re.findall(r\"(?m)^\\s*P[1249]\\.\\d{1,2}\\b\", raw)))\n",
    "print(\"Subtipos encontrados tipo P4.01:\", len(re.findall(r\"(?m)^\\s*P[1249]\\.\\d{2}\\b\", raw)))\n",
    "\n",
    "# Muestra 20 líneas candidatas que empiezan por P algo\n",
    "cand = re.findall(r\"(?m)^\\s*(P[1249]\\.[^\\n]{0,80})$\", raw)\n",
    "print(\"\\nEjemplos de líneas P*:\")\n",
    "for x in cand[:50]:\n",
    "    print(\"-\", repr(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc735711-0712-4af2-9959-e54e45adf02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos detectados: ['P1', 'P2', 'P4', 'P9']\n",
      "P1 subtipos: 3 | ejemplo: ['P1.1', 'P1.2', 'P1.3']\n",
      "P2 subtipos: 3 | ejemplo: ['P2.1', 'P2.2', 'P2.3']\n",
      "P4 subtipos: 9 | ejemplo: ['P4.01', 'P4.02', 'P4.03', 'P4.04', 'P4.05', 'P4.06']\n",
      "P9 subtipos: 12 | ejemplo: ['P9.01', 'P9.02', 'P9.03', 'P9.04', 'P9.05', 'P9.06']\n",
      "Docs gold: 373\n",
      "Spans en pool por etiqueta:\n",
      " - DURACION_TOTAL_DEL_CONTRATO : 468\n",
      " - OBJETO : 925\n",
      " - PRECIO_DEL_CONTRATO : 436\n",
      " - RESOLUCION : 526\n",
      "\n",
      "Total memory_selected_AUTO: 18\n",
      "Errores de validación: 0\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/memory_selected_AUTO.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NB03 — Baseline: construir memory_selected_AUTO.json (random/heurístico) desde guidelines_MVP_defs.txt + gold\n",
    "# NB03 — Baseline: construir memory_selected_AUTO.json (random/heurístico) desde guidelines_MVP_defs.txt + gold\n",
    "\n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================\n",
    "# 0) Rutas + seed\n",
    "# =========================\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_GUIDE_TXT = ROOT / \"config\" / \"guidelines_MVP_defs.txt\"\n",
    "\n",
    "PATH_GOLD_JSON  = ROOT / \"gold\" / \"corpus_annotated.json\"    # opcional\n",
    "PATH_GOLD_JSONL = ROOT / \"gold\" / \"corpus_annotated.jsonl\"   # recomendado\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# 1) Utilidades\n",
    "# =========================\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def norm_all(text: str) -> str:\n",
    "    # normaliza BOM, CRLF/CR y NBSP\n",
    "    return text.lstrip(\"\\ufeff\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"\\xa0\", \" \")\n",
    "\n",
    "def norm_line(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "# MVP labels\n",
    "P_TO_LABEL = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "MVP_LABELS = set(P_TO_LABEL.values())\n",
    "\n",
    "FIXED_CRITERION = {\n",
    "    \"P4\": \"P4 – criterio único (subtipos solo ejemplificativos)\",\n",
    "    \"P9\": \"P9 – criterio único (subtipos solo ejemplificativos)\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 2) Parsear guía TXT (mains + subtipos + criterio)\n",
    "# =========================\n",
    "if not PATH_GUIDE_TXT.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_GUIDE_TXT}\")\n",
    "\n",
    "raw_txt = norm_all(PATH_GUIDE_TXT.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "lines = [ln.strip() for ln in raw_txt.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "RE_MAIN = re.compile(r\"^\\s*(P(?:1|2|4|9))\\.\\s+(.+?)\\s*$\", re.IGNORECASE)\n",
    "RE_SUB  = re.compile(r\"^\\s*(P(?:1|2|4|9))\\s*\\.\\s*(\\d{1,2})\\s*(.*)\\s*$\", re.IGNORECASE)\n",
    "RE_CRIT = re.compile(r\"^\\s*Criterio\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def fmt_subcode(main: str, num: str) -> str:\n",
    "    n = int(num)\n",
    "    # P4/P9 suelen ir como 2 dígitos (P4.01, P9.12), P1/P2 como 1 dígito (P1.1)\n",
    "    if main in {\"P4\",\"P9\"}:\n",
    "        return f\"{main}.{n:02d}\"\n",
    "    return f\"{main}.{n}\"\n",
    "\n",
    "guide = {k: {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}} for k in [\"P1\",\"P2\",\"P4\",\"P9\"]}\n",
    "current_code = None\n",
    "\n",
    "def ensure_sub(main, subcode):\n",
    "    guide[main][\"subtypes\"].setdefault(subcode, {\"title\": \"\", \"criterion\": \"\"})\n",
    "\n",
    "for ln in lines:\n",
    "    ln = norm_line(ln)\n",
    "\n",
    "    m1 = RE_MAIN.match(ln)\n",
    "    if m1:\n",
    "        main = m1.group(1).upper()\n",
    "        guide[main][\"title\"] = m1.group(2).strip()\n",
    "        current_code = main\n",
    "        continue\n",
    "\n",
    "    m2 = RE_SUB.match(ln)\n",
    "    if m2:\n",
    "        main = m2.group(1).upper()\n",
    "        num  = m2.group(2)\n",
    "        title = (m2.group(3) or \"\").strip()  # puede estar vacío (P4.01 / P9.03...)\n",
    "        subcode = fmt_subcode(main, num)\n",
    "        ensure_sub(main, subcode)\n",
    "        guide[main][\"subtypes\"][subcode][\"title\"] = title\n",
    "        current_code = subcode\n",
    "        continue\n",
    "\n",
    "    m3 = RE_CRIT.match(ln)\n",
    "    if m3 and current_code:\n",
    "        crit = m3.group(1).strip()\n",
    "        if \".\" in current_code:\n",
    "            main = current_code.split(\".\")[0]\n",
    "            ensure_sub(main, current_code)\n",
    "            guide[main][\"subtypes\"][current_code][\"criterion\"] = crit\n",
    "        else:\n",
    "            guide[current_code][\"criterion\"] = crit\n",
    "        continue\n",
    "\n",
    "print(\"Tipos detectados:\", [k for k in guide.keys() if guide[k][\"title\"]])\n",
    "for k in [\"P1\",\"P2\",\"P4\",\"P9\"]:\n",
    "    subs = sorted(guide[k][\"subtypes\"].keys())\n",
    "    print(f\"{k} subtipos:\", len(subs), \"| ejemplo:\", subs[:6])\n",
    "\n",
    "# =========================\n",
    "# 3) Cargar GOLD (COMPLETO)\n",
    "# =========================\n",
    "if PATH_GOLD_JSONL.exists():\n",
    "    gold = load_jsonl(PATH_GOLD_JSONL)\n",
    "elif PATH_GOLD_JSON.exists():\n",
    "    gold = load_json(PATH_GOLD_JSON)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No encuentro gold/corpus_annotated.jsonl ni .json\")\n",
    "\n",
    "print(\"Docs gold:\", len(gold))\n",
    "\n",
    "# =========================\n",
    "# 4) Pool de spans por etiqueta\n",
    "# =========================\n",
    "pool = {lab: [] for lab in MVP_LABELS}\n",
    "\n",
    "key_to_text = {}\n",
    "for d in gold:\n",
    "    text = d.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    uid = stable_uid(text)\n",
    "    key_to_text[uid] = text\n",
    "    doc_id = d.get(\"id\")\n",
    "\n",
    "    for t in d.get(\"tags\", []):\n",
    "        lab = t.get(\"tag\")\n",
    "        if lab not in MVP_LABELS:\n",
    "            continue\n",
    "        s = int(t[\"start\"]); e = int(t[\"end\"])\n",
    "        if not (0 <= s < e <= len(text)):\n",
    "            continue\n",
    "        span_txt = text[s:e]\n",
    "        if not span_txt.strip():\n",
    "            continue\n",
    "        pool[lab].append({\"doc_id\": doc_id, \"doc_uid\": uid, \"start\": s, \"end\": e, \"text\": span_txt})\n",
    "\n",
    "print(\"Spans en pool por etiqueta:\")\n",
    "for lab in sorted(pool.keys()):\n",
    "    print(\" -\", lab, \":\", len(pool[lab]))\n",
    "\n",
    "def pick_random_span(label: str):\n",
    "    if not pool[label]:\n",
    "        raise ValueError(f\"No hay spans en pool para {label}\")\n",
    "    return random.choice(pool[label])\n",
    "\n",
    "# =========================\n",
    "# 5) Construir memory_selected AUTO\n",
    "#    - P1/P2: 1 por main + 1 por subtipo (random)\n",
    "#    - P4/P9: 4 por variedad formal (heurística simple)\n",
    "# =========================\n",
    "memory_selected = []\n",
    "\n",
    "# 5.1 MAIN (P1/P2/P4/P9)\n",
    "for main in [\"P1\",\"P2\",\"P4\",\"P9\"]:\n",
    "    label = P_TO_LABEL[main]\n",
    "    ex = pick_random_span(label)\n",
    "    if main in FIXED_CRITERION:\n",
    "        criterion = FIXED_CRITERION[main]\n",
    "    else:\n",
    "        criterion = f\"{main} {guide[main]['title']} — {guide[main]['criterion']}\".strip(\" —\")\n",
    "    memory_selected.append({\"label\": label, \"criterion\": criterion, \"doc_uid\": ex[\"doc_uid\"], \"start\": ex[\"start\"], \"end\": ex[\"end\"]})\n",
    "\n",
    "# 5.2 SUBTIPOS (solo P1/P2; P4/P9 los tratamos como variedad formal)\n",
    "for main in [\"P1\",\"P2\"]:\n",
    "    label = P_TO_LABEL[main]\n",
    "    for subcode, subinfo in sorted(guide[main][\"subtypes\"].items()):\n",
    "        ex = pick_random_span(label)\n",
    "        title = (subinfo.get(\"title\") or \"\").strip()\n",
    "        crit  = (subinfo.get(\"criterion\") or \"\").strip()\n",
    "        criterion = f\"{subcode} {title} — {crit}\".strip(\" —\")\n",
    "        memory_selected.append({\"label\": label, \"criterion\": criterion, \"doc_uid\": ex[\"doc_uid\"], \"start\": ex[\"start\"], \"end\": ex[\"end\"]})\n",
    "\n",
    "# 5.3 Variedad formal P4/P9: 4 ejemplos cada una (sin subtipos)\n",
    "def pick_variety(label: str, patterns: list, k: int = 4):\n",
    "    candidates = pool[label].copy()\n",
    "    random.shuffle(candidates)\n",
    "    picked, used_uids = [], set()\n",
    "\n",
    "    for _, rx in patterns:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        for ex in candidates:\n",
    "            if ex[\"doc_uid\"] in used_uids:\n",
    "                continue\n",
    "            if rx.search(ex[\"text\"]):\n",
    "                picked.append(ex)\n",
    "                used_uids.add(ex[\"doc_uid\"])\n",
    "                break\n",
    "\n",
    "    # completar si falta (sin repetir doc_uid)\n",
    "    for ex in candidates:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        if ex[\"doc_uid\"] in used_uids:\n",
    "            continue\n",
    "        picked.append(ex)\n",
    "        used_uids.add(ex[\"doc_uid\"])\n",
    "\n",
    "    return picked\n",
    "\n",
    "dur_patterns = [\n",
    "    (\"formalizacion\", re.compile(r\"\\bformalizaci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"fechas\", re.compile(r\"\\b\\d{1,2}\\s+de\\s+[a-záéíóú]+\\s+de\\s+\\d{4}\\b\", re.IGNORECASE)),\n",
    "    (\"anos\", re.compile(r\"\\b\\d+\\s+a[nñ]os?\\b\", re.IGNORECASE)),\n",
    "    (\"meses\", re.compile(r\"\\b\\d+\\s+mes(?:es)?\\b\", re.IGNORECASE)),\n",
    "    (\"dias\", re.compile(r\"\\b\\d+\\s+d[ií]as?\\b\", re.IGNORECASE)),\n",
    "    (\"prorroga\", re.compile(r\"\\bpr[oó]rroga\\b\", re.IGNORECASE)),\n",
    "]\n",
    "res_patterns = [\n",
    "    (\"lcsp\", re.compile(r\"\\bLCSP\\b|\\bLey\\s+9/2017\\b|\\bart[íi]culo(?:s)?\\b\", re.IGNORECASE)),\n",
    "    (\"causas\", re.compile(r\"\\bcausas?\\s+de\\s+resoluci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"incumplimiento\", re.compile(r\"\\bincumplim\", re.IGNORECASE)),\n",
    "    (\"rescision\", re.compile(r\"\\brescisi[oó]n\\b|\\brescind\", re.IGNORECASE)),\n",
    "    (\"extincion\", re.compile(r\"\\bextinci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"mutuo_acuerdo\", re.compile(r\"\\bmutuo\\s+acuerdo\\b\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "for ex in pick_variety(P_TO_LABEL[\"P4\"], dur_patterns, k=4):\n",
    "    memory_selected.append({\"label\": P_TO_LABEL[\"P4\"], \"criterion\": FIXED_CRITERION[\"P4\"], \"doc_uid\": ex[\"doc_uid\"], \"start\": ex[\"start\"], \"end\": ex[\"end\"]})\n",
    "for ex in pick_variety(P_TO_LABEL[\"P9\"], res_patterns, k=4):\n",
    "    memory_selected.append({\"label\": P_TO_LABEL[\"P9\"], \"criterion\": FIXED_CRITERION[\"P9\"], \"doc_uid\": ex[\"doc_uid\"], \"start\": ex[\"start\"], \"end\": ex[\"end\"]})\n",
    "\n",
    "# =========================\n",
    "# 6) Validación rápida\n",
    "# =========================\n",
    "errors = 0\n",
    "for ex in memory_selected:\n",
    "    uid = ex[\"doc_uid\"]\n",
    "    txt = key_to_text.get(uid)\n",
    "    if not txt:\n",
    "        errors += 1\n",
    "        continue\n",
    "    s, e = ex[\"start\"], ex[\"end\"]\n",
    "    if not (0 <= s < e <= len(txt)):\n",
    "        errors += 1\n",
    "        continue\n",
    "    if not txt[s:e].strip():\n",
    "        errors += 1\n",
    "\n",
    "print(\"\\nTotal memory_selected_AUTO:\", len(memory_selected))\n",
    "print(\"Errores de validación:\", errors)\n",
    "\n",
    "OUT_AUTO = OUT_DIR / \"memory_selected_AUTO.json\"\n",
    "OUT_AUTO.write_text(json.dumps(memory_selected, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado:\", OUT_AUTO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff9097-e7df-4d89-a39b-a227d99e2717",
   "metadata": {},
   "source": [
    "Perfecto, lo que vemos al ejecutarla:\n",
    "- Cuántos tipos (P1/P2/P4/P9) detecta en la guía\n",
    "- Cuántos subtipos detecta por cada tipo\n",
    "- Cuántos spans hay por cada etiqueta en el gold\n",
    "\n",
    "Un memory_selected_AUTO.json ya creado con:\n",
    "- label\n",
    "- criterion (con tu convención explícita en P4/P9)\n",
    "- doc_uid\n",
    "- start/end\n",
    "\n",
    "**SIGUIENTE AJUSTE (para que ya sea “anti-leakage” de verdad)**\n",
    "- Cambiar selección random por selección semánticamente alineada (tú eliges).\n",
    "- Construir blocked_doc_uids_by_memory.json con los doc_uid usados en la memoria (y solo esos), para excluirlos de val/test/prompt_regression.\n",
    "\n",
    "Hemos detectado:\n",
    "- P1: 3 subtipos\n",
    "- P2: 3 subtipos\n",
    "- P4: 6 subtipos (ejemplificativos)\n",
    "- P9: 12 subtipos (ejemplificativos)\n",
    "\n",
    "Si seleccionamos **_1 ejemplo por subtipo_**, te vas a meter 3+3+6+12 = **24 ejemplos en memoria, que para un 7B puede ser demasiado**.\n",
    "\n",
    "Para MVP, haz esto:\n",
    "- P1: 1 ejemplo para P1 (general) + 1 por cada subtipo (3) → 4 ejemplos\n",
    "- P2: 1 ejemplo para P2 (general) + 1 por cada subtipo (3) → 4 ejemplos\n",
    "- P4: criterio único → selecciona 3–4 ejemplos que cubran variedad (fecha fija, duración en meses/años, “desde formalización”, “plazo máximo”, prórroga)\n",
    "- P9: criterio único → selecciona 3–4 ejemplos (causas LCSP, resolución por incumplimiento, rescisión automática, “extinción”, etc.)\n",
    "\n",
    "**Total memoria recomendada MVP: 14–16 ejemplos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c34a56-90d8-497f-b416-4f2477ee3bb2",
   "metadata": {},
   "source": [
    "**Qué significa el recuento**\n",
    "Esto NO significa documentos; significa spans anotados en el gold. Dado que antes vimos:\n",
    "- 373 documentos con al menos un span MVP\n",
    "\n",
    "la lectura correcta es:\n",
    "\n",
    "Cada documento suele tener:\n",
    "- 1–3 OBJETO (título + cláusula + lote, etc.)\n",
    "- 1–2 RESOLUCION\n",
    "- 1–2 DURACION\n",
    "- 1 PRECIO (a veces más si hay anualidades)\n",
    "\n",
    "➡️ Es exactamente el patrón jurídico esperado.\n",
    "➡️ No hay desbalance patológico.\n",
    "➡️ No necesitas sampling agresivo.\n",
    "\n",
    "**¿Es demasiado para un MVP?**\n",
    "No. Al contrario:\n",
    "- Para prompting: usarás solo 14–16 ejemplos en memoria.\n",
    "- Para evaluación: tendrás suficientes casos variados para medir overlap y exact match con estabilidad estadística.\n",
    "- Para generalización: la diversidad interna es alta (bien).\n",
    "\n",
    "373 docs es una base sólida para:\n",
    "- un MVP serio,\n",
    "- un paper corto,\n",
    "- o una demo reproducible.\n",
    "\n",
    "**Decisión clave (ya podemos fijarla)**\n",
    "\n",
    "Con estos números, te recomiendo cerrar ya estas decisiones:\n",
    "\n",
    "**✔️ Memoria / few-shot (prompt)**\n",
    "\n",
    "**OBJETO (P1)**\n",
    "- 1 ejemplo general\n",
    "- 1 por cada subtipo (3)\n",
    "- → 4 ejemplos\n",
    "\n",
    "**PRECIO (P2)**\n",
    "- 1 ejemplo general\n",
    "- 1 por cada subtipo (3)\n",
    "- → 4 ejemplos\n",
    "\n",
    "**DURACIÓN (P4)**\n",
    "criterio único\n",
    "- 3–4 ejemplos representativos\n",
    "\n",
    "**RESOLUCIÓN (P9)**\n",
    "criterio único\n",
    "- 3–4 ejemplos representativos\n",
    "\n",
    "\n",
    "🔢 Total memoria: 14–16 ejemplos\n",
    "Perfecto para Mistral 7B / Llama 3.1 8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72ec14-5823-400b-a572-f465ee6f3aac",
   "metadata": {},
   "source": [
    "**Sustituimos la memoria aleatoria por una memoria curada**\n",
    "Genera memory_selected curada automáticamente siguiendo tu metodología:\n",
    "- P1 / P2: alineación por criterio/subtipo usando la guía (guidelines_MVP_defs.docx):\n",
    "    - Para cada subtipo (Heading 3) busca un ejemplo entre corchetes [...]\n",
    "    - Localiza ese extracto literal en el gold\n",
    "    - Recupera el span anotado (offsets) que lo cubre y lo añade con criterion = \"<código subtipo> <título> — <criterio>\"\n",
    "\n",
    "- P4 / P9: selección por variedad formal (no por subtipo), con buckets heurísticos (fechas, “formalización”, meses/años/días, prórroga, LCSP, causas, etc.) y criterion fijo:\n",
    "    - \"P4 – criterio único (subtipos solo ejemplificativos)\"\n",
    "    - \"P9 – criterio único (subtipos solo ejemplificativos)\"\n",
    "\n",
    "Al final te deja:\n",
    "- memory_selected listo para copiar/pegar (es una lista de dicts con label/criterion/doc_uid/start/end)\n",
    "- y guarda outputs/memory/memory_selected_CURATED.json\n",
    "\n",
    "Nota: si algún subtipo no se puede resolver (porque el extracto literal no aparece tal cual en el gold, o no cae dentro de un span anotado), el código hace fallback a un span aleatorio de esa etiqueta y lo marca explícitamente en criterion con “(fallback)”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a5626-8d89-4ac9-ab71-f2e320fdfd0a",
   "metadata": {},
   "source": [
    "**!! DISCLAIMER**: con la siguiente celda, capturamos todos los quotes de las guías en el gold. Resulta que **algunos de los ejemplos que aparecen en las guías no está en el gold** (se ve que no los incluyeron en el corpus). ejemplos: \n",
    "\n",
    "`ValueError: [SIN FALLBACK] No encuentro el quote de P1.MAIN en GOLD.\n",
    "Quote: **suministro de armarios consigna con apertura individual mediante cerradura electrónica para los edificios del Banco de España en Lumbreras**`\n",
    "\n",
    "`ValueError: [SIN FALLBACK] No encuentro el quote del subtipo P2.1 en GOLD.\n",
    "Quote: 29999.53 € (IVA incuido)`\n",
    "\n",
    "Así que como queremos la memoria curada sin fallbacks, lo correcto es **si no se puede alinear, NO lo incluimos** (y seguimos con el resto P1.1/P1.2/P1.3). Esto sigue cumpliendo la metodología, porque el “main” es redundante: los subtipos ya cubren el concepto, y además P4/P9 ya van por criterio único.\n",
    "\n",
    "Solución mínima (y metodológicamente limpia): si estamos en modo \"sin fallback\" y el gold es un subset (373 docs), tenemos que tratar como \"opcioales\" TODOS los subtipos cuyo ejemplo literal no esté presente en este subset.\n",
    "- P1/P2: intentar alinear por subtipo con cita literal;\n",
    "    - si no se encuentra en el gold actual → omitimos ese subtipo con warning (igual que hicimos con .MAIN), sin fallback.\n",
    "- P4/P9: seguir como estabas (variedad formal por heurísticas), sin depender de la guía para encontrar citas.\n",
    "\n",
    "Añadimos parche  para sustituir el bucle de P1/P2 “por subtipos” (incluye MAIN y subtipos) sin cambiar el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3110a982-7bab-4078-aec8-bcf4a743bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos detectados: ['P1', 'P2', 'P4', 'P9']\n",
      "P1 subtipos: 3 | +MAIN_pseudo: 1 | ejemplo: ['P1.1', 'P1.2', 'P1.3']\n",
      "P2 subtipos: 3 | +MAIN_pseudo: 1 | ejemplo: ['P2.1', 'P2.2', 'P2.3']\n",
      "P4 subtipos: 9 | +MAIN_pseudo: 0 | ejemplo: ['P4.01', 'P4.02', 'P4.03', 'P4.04', 'P4.05', 'P4.06', 'P4.07', 'P4.08', 'P4.09']\n",
      "P9 subtipos: 12 | +MAIN_pseudo: 0 | ejemplo: ['P9.01', 'P9.02', 'P9.03', 'P9.04', 'P9.05', 'P9.06', 'P9.07', 'P9.08', 'P9.09', 'P9.10', 'P9.11', 'P9.12']\n",
      "Docs gold: 373\n",
      "Spans por etiqueta en gold (pool):\n",
      " - DURACION_TOTAL_DEL_CONTRATO : 468\n",
      " - OBJETO : 925\n",
      " - PRECIO_DEL_CONTRATO : 436\n",
      " - RESOLUCION : 526\n",
      "[WARN] P1.MAIN: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "[WARN] P1.1: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "[WARN] P1.3: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "[WARN] P2.MAIN: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "[WARN] P2.1: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "[WARN] P2.2: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\n",
      "\n",
      "[RESUMEN] subtipos omitidos (no presentes en GOLD actual): 6\n",
      "- P1.MAIN | quote_not_in_gold | suministro de armarios consigna con apertura individual mediante cerradura electrónica para los edificios del Banco de España en Lumbreras\n",
      "- P1.1 | quote_not_in_gold | Explotación de la aplicación móvil para personal de la Armada Española del expediente 2022/EA02/00001693E\n",
      "- P1.3 | quote_not_in_gold | SERVICIO         DE         CARGA         Y         DESCARGA         DE    ELEMENTOS  ESCENOGRÁFICOS Y DELA CARROZA PARA LA FUNDACIÓN DEL  TEATRO REAL F.S.P.\n",
      "- P2.MAIN | quote_not_in_gold | 14.567 €\n",
      "- P2.1 | quote_not_in_gold | 29999.53 € (IVA incuido)\n",
      "- P2.2 | quote_not_in_gold | 950.130,00 euros\n",
      "\n",
      "memory_selected tamaño: 10\n",
      "Docs únicos usados: 10\n",
      "Errores de validación: 0\n",
      "Resumen por etiqueta: {'OBJETO': 1, 'PRECIO_DEL_CONTRATO': 1, 'DURACION_TOTAL_DEL_CONTRATO': 4, 'RESOLUCION': 4}\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/memory_selected_CURATED.json\n",
      "Guardado: /home/jovyan/inesagent/outputs/memory/blocked_doc_uids_by_memory.json\n",
      "Guardado huérfanos: /home/jovyan/inesagent/outputs/memory/guidelines_orphan_examples.json | n: 6\n"
     ]
    }
   ],
   "source": [
    "#construimos memory_selected curada\n",
    "# NB03 — Construcción de memoria curada (SIN fallbacks) desde guidelines_MVP_defs.txt + gold\n",
    "# NB03 — Construcción de memoria curada (SIN fallbacks) desde guidelines_MVP_defs.txt + gold\n",
    "\n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_GUIDE_TXT = ROOT / \"config\" / \"guidelines_MVP_defs.txt\"\n",
    "\n",
    "PATH_GOLD_JSON  = ROOT / \"gold\" / \"corpus_annotated.json\"\n",
    "PATH_GOLD_JSONL = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "P_TO_LABEL = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "MVP_LABELS = set(P_TO_LABEL.values())\n",
    "\n",
    "FIXED_CRITERION = {\n",
    "    \"P4\": \"P4 – criterio único (subtipos solo ejemplificativos)\",\n",
    "    \"P9\": \"P9 – criterio único (subtipos solo ejemplificativos)\",\n",
    "}\n",
    "\n",
    "BRACKET_RE = re.compile(r\"\\[(.+?)\\]\")\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def doc_key(d: dict, txt: str) -> str:\n",
    "    \"\"\"Clave estable del documento.\n",
    "    - Si el gold trae id (int/str), usamos ese id.\n",
    "    - Si no, usamos un hash sha1 del texto.\n",
    "    Siempre devolvemos str.\n",
    "    \"\"\"\n",
    "    if \"id\" in d and d[\"id\"] is not None and str(d[\"id\"]).strip() != \"\":\n",
    "        return str(d[\"id\"])\n",
    "    return stable_uid(txt)\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def norm_all(text: str) -> str:\n",
    "    return text.lstrip(\"\\ufeff\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"\\xa0\", \" \")\n",
    "\n",
    "def normalize_for_match(s: str) -> str:\n",
    "    # normalización robusta para matching\n",
    "    s = (s or \"\").lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# 1) Parsear guía TXT: mains/subtipos/criterio + ejemplos [...]\n",
    "# =========================\n",
    "if not PATH_GUIDE_TXT.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_GUIDE_TXT}\")\n",
    "\n",
    "raw_txt = norm_all(PATH_GUIDE_TXT.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "lines = [ln.strip() for ln in raw_txt.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "RE_MAIN = re.compile(r\"^\\s*(P(?:1|2|4|9))\\.\\s+(.+?)\\s*$\", re.IGNORECASE)\n",
    "RE_SUB  = re.compile(r\"^\\s*(P(?:1|2|4|9))\\s*\\.\\s*(\\d{1,2})\\s*(.*)\\s*$\", re.IGNORECASE)\n",
    "RE_CRIT = re.compile(r\"^\\s*Criterio\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def fmt_subcode(main: str, num: str) -> str:\n",
    "    n = int(num)\n",
    "    if main in {\"P4\",\"P9\"}:\n",
    "        return f\"{main}.{n:02d}\"\n",
    "    return f\"{main}.{n}\"\n",
    "\n",
    "guide = {k: {\"title\": \"\", \"criterion\": \"\", \"subtypes\": {}} for k in [\"P1\",\"P2\",\"P4\",\"P9\"]}\n",
    "current_code = None\n",
    "\n",
    "def ensure_sub(main, subcode):\n",
    "    guide[main][\"subtypes\"].setdefault(subcode, {\"title\": \"\", \"criterion\": \"\", \"examples\": []})\n",
    "\n",
    "for ln in lines:\n",
    "    ln_norm = \" \".join(ln.split())\n",
    "\n",
    "    m1 = RE_MAIN.match(ln_norm)\n",
    "    if m1:\n",
    "        main = m1.group(1).upper()\n",
    "        guide[main][\"title\"] = m1.group(2).strip()\n",
    "        current_code = main\n",
    "        continue\n",
    "\n",
    "    m2 = RE_SUB.match(ln_norm)\n",
    "    if m2:\n",
    "        main = m2.group(1).upper()\n",
    "        num  = m2.group(2)\n",
    "        title = (m2.group(3) or \"\").strip()\n",
    "        subcode = fmt_subcode(main, num)\n",
    "        ensure_sub(main, subcode)\n",
    "        guide[main][\"subtypes\"][subcode][\"title\"] = title\n",
    "        current_code = subcode\n",
    "        continue\n",
    "\n",
    "    m3 = RE_CRIT.match(ln_norm)\n",
    "    if m3 and current_code:\n",
    "        crit = m3.group(1).strip()\n",
    "        if \".\" in current_code:\n",
    "            main = current_code.split(\".\")[0]\n",
    "            ensure_sub(main, current_code)\n",
    "            guide[main][\"subtypes\"][current_code][\"criterion\"] = crit\n",
    "        else:\n",
    "            guide[current_code][\"criterion\"] = crit\n",
    "        continue\n",
    "\n",
    "    # ejemplos entre corchetes en cualquier línea -> se asignan al bloque actual (main o subtipo)\n",
    "    for m in BRACKET_RE.finditer(ln):\n",
    "        ex = m.group(1).strip()\n",
    "        if not ex:\n",
    "            continue\n",
    "        if current_code and \".\" in current_code:\n",
    "            main = current_code.split(\".\")[0]\n",
    "            ensure_sub(main, current_code)\n",
    "            guide[main][\"subtypes\"][current_code][\"examples\"].append(ex)\n",
    "        elif current_code in guide:\n",
    "            pseudo = f\"{current_code}.MAIN\"\n",
    "            ensure_sub(current_code, pseudo)\n",
    "            guide[current_code][\"subtypes\"][pseudo][\"title\"] = \"(ejemplo a nivel etiqueta)\"\n",
    "            guide[current_code][\"subtypes\"][pseudo][\"criterion\"] = guide[current_code][\"criterion\"]\n",
    "            guide[current_code][\"subtypes\"][pseudo][\"examples\"].append(ex)\n",
    "\n",
    "print(\"Tipos detectados:\", [k for k in guide.keys() if guide[k][\"title\"]])\n",
    "for k in [\"P1\",\"P2\",\"P4\",\"P9\"]:\n",
    "    subs = sorted(guide[k][\"subtypes\"].keys())\n",
    "    main_pseudo = 1 if f\"{k}.MAIN\" in guide[k][\"subtypes\"] else 0\n",
    "    real_subs = [s for s in subs if not s.endswith(\".MAIN\")]\n",
    "    print(f\"{k} subtipos:\", len(real_subs), \"| +MAIN_pseudo:\", main_pseudo, \"| ejemplo:\", real_subs[:15])\n",
    "\n",
    "# =========================\n",
    "# 2) Cargar GOLD COMPLETO + construir índices + pool MVP\n",
    "# =========================\n",
    "if PATH_GOLD_JSONL.exists():\n",
    "    gold = load_jsonl(PATH_GOLD_JSONL)\n",
    "elif PATH_GOLD_JSON.exists():\n",
    "    gold = load_json(PATH_GOLD_JSON)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No encuentro gold/corpus_annotated.jsonl ni .json\")\n",
    "\n",
    "print(\"Docs gold:\", len(gold))\n",
    "\n",
    "key_to_text = {}\n",
    "key_to_tags = {}\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    key = doc_key(d, txt)\n",
    "    key_to_text[key] = txt\n",
    "    key_to_tags[key] = d.get(\"tags\", [])\n",
    "\n",
    "# Pool de spans MVP\n",
    "pool = defaultdict(list)\n",
    "for key, txt in key_to_text.items():\n",
    "    for t in key_to_tags.get(key, []):\n",
    "        lab = t.get(\"tag\")\n",
    "        if lab not in MVP_LABELS:\n",
    "            continue\n",
    "        s = int(t[\"start\"]); e = int(t[\"end\"])\n",
    "        if 0 <= s < e <= len(txt):\n",
    "            span_txt = txt[s:e]\n",
    "            if span_txt.strip():\n",
    "                pool[lab].append({\"id\": key, \"start\": s, \"end\": e, \"text\": span_txt})\n",
    "\n",
    "print(\"Spans por etiqueta en gold (pool):\")\n",
    "for lab in sorted(pool.keys()):\n",
    "    print(\" -\", lab, \":\", len(pool[lab]))\n",
    "\n",
    "# =========================\n",
    "# 3) Matching robusto quote -> (uid, qstart, qend) + recuperar span anotado\n",
    "# =========================\n",
    "gold_norm = [(key, normalize_for_match(txt)) for key, txt in key_to_text.items()]\n",
    "\n",
    "def find_quote_in_gold_robust(quote: str):\n",
    "    qn = normalize_for_match(quote)\n",
    "    if not qn:\n",
    "        return None\n",
    "\n",
    "    # (A) búsqueda por contención en texto normalizado\n",
    "    for key, tnorm in gold_norm:\n",
    "        if qn in tnorm:\n",
    "            txt = key_to_text[key]\n",
    "\n",
    "            # (B) intentamos match exacto en original\n",
    "            pos = txt.find(quote)\n",
    "            if pos != -1:\n",
    "                return key, pos, pos + len(quote)\n",
    "\n",
    "            # (C) regex flexible por tokens (espacios variables)\n",
    "            toks = [re.escape(tok) for tok in quote.split() if tok.strip()]\n",
    "            if len(toks) >= 4:\n",
    "                pat = r\"\\s+\".join(toks)\n",
    "                m = re.search(pat, txt, flags=re.IGNORECASE)\n",
    "                if m:\n",
    "                    return key, m.start(), m.end()\n",
    "\n",
    "            # Si vimos contención normalizada pero no podemos mapear offsets, seguimos buscando\n",
    "    return None\n",
    "\n",
    "def pick_annotated_span_covering(key: str, label: str, q_start: int, q_end: int):\n",
    "    txt = key_to_text[key]\n",
    "    best = None\n",
    "    best_score = -1\n",
    "\n",
    "    for t in key_to_tags.get(key, []):\n",
    "        if t.get(\"tag\") != label:\n",
    "            continue\n",
    "        s = int(t[\"start\"]); e = int(t[\"end\"])\n",
    "        if not (0 <= s < e <= len(txt)):\n",
    "            continue\n",
    "\n",
    "        overlap = max(0, min(e, q_end) - max(s, q_start))\n",
    "        covers = (s <= q_start and q_end <= e)\n",
    "        score = (10_000 + overlap) if covers else overlap\n",
    "\n",
    "        if overlap > 0 and score > best_score:\n",
    "            best_score = score\n",
    "            best = (s, e)\n",
    "\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# 4) Construir memoria curada SIN fallback\n",
    "# =========================\n",
    "memory_selected = []\n",
    "used_keys = set()\n",
    "\n",
    "def add_example(label, criterion, key, s, e):\n",
    "    memory_selected.append({\"label\": label, \"criterion\": criterion, \"id\": str(key), \"start\": s, \"end\": e})\n",
    "    used_keys.add(str(key))\n",
    "\n",
    "# # ---- 4.1 P1/P2: alinear por guía (SIN fallback) pero \"skip si no está en este GOLD\"\n",
    "skipped = []\n",
    "orphan_examples = []  # ejemplos de guía que NO están en gold/unannotated; solo sirven para few-shot  # para reporte final\n",
    "\n",
    "def add_from_quote_or_skip(main: str, subcode: str, info: dict):\n",
    "    label = P_TO_LABEL[main]\n",
    "    title = (info.get(\"title\") or \"\").strip()\n",
    "    crit = (info.get(\"criterion\") or \"\").strip()\n",
    "\n",
    "    # tomamos 1er ejemplo entre corchetes\n",
    "    examples = info.get(\"examples\") or []\n",
    "    if not examples:\n",
    "        raise ValueError(f\"[SIN FALLBACK] {subcode} no tiene examples en la guía\")\n",
    "\n",
    "    quote = examples[0]\n",
    "    found = find_quote_in_gold_robust(quote)\n",
    "\n",
    "    if not found:\n",
    "        print(f\"[WARN] {subcode}: quote no encontrado en GOLD actual -> se marca como HUÉRFANO (solo few-shot).\")\n",
    "        skipped.append({\"subcode\": subcode, \"reason\": \"quote_not_in_gold\", \"quote\": quote[:200]})\n",
    "        orphan_examples.append({\"subcode\": subcode, \"label\": label, \"criterion\": f\"{subcode} {title} — {crit}\".strip(\" —\"), \"quote\": quote})\n",
    "        return\n",
    "\n",
    "    key, qs, qe = found\n",
    "    span = pick_annotated_span_covering(key, label, qs, qe)\n",
    "    if not span:\n",
    "        raise ValueError(\n",
    "            f\"[SIN FALLBACK] {subcode}: quote encontrado en doc {str(key)[:12]}... \"\n",
    "            f\"pero no cae dentro de ningún span anotado de {label}\"\n",
    "        )\n",
    "\n",
    "    criterion = f\"{subcode} {title} — {crit}\".strip(\" —\")\n",
    "    add_example(label, criterion, key, span[0], span[1])\n",
    "\n",
    "for main in [\"P1\", \"P2\"]:\n",
    "    # 1) MAIN pseudo si existe\n",
    "    main_pseudo = f\"{main}.MAIN\"\n",
    "    if main_pseudo in guide[main][\"subtypes\"]:\n",
    "        add_from_quote_or_skip(main, main_pseudo, guide[main][\"subtypes\"][main_pseudo])\n",
    "\n",
    "    # 2) subtipos reales (P1.1, P1.2..., P2.1...)\n",
    "    for subcode in sorted(guide[main][\"subtypes\"].keys()):\n",
    "        if subcode.endswith(\".MAIN\"):\n",
    "            continue\n",
    "        add_from_quote_or_skip(main, subcode, guide[main][\"subtypes\"][subcode])\n",
    "\n",
    "print(\"\\n[RESUMEN] subtipos omitidos (no presentes en GOLD actual):\", len(skipped))\n",
    "for s in skipped[:10]:\n",
    "    print(\"-\", s[\"subcode\"], \"|\", s[\"reason\"], \"|\", s[\"quote\"])\n",
    "if len(skipped) > 10:\n",
    "    print(\"... (+\", len(skipped) - 10, \"más)\")\n",
    "\n",
    "\n",
    "    # Subtipos reales\n",
    "    for subcode, info in sorted(guide[main][\"subtypes\"].items()):\n",
    "        if subcode.endswith(\".MAIN\"):\n",
    "            continue\n",
    "        examples = info.get(\"examples\", [])\n",
    "        if not examples:\n",
    "            raise ValueError(f\"[SIN FALLBACK] El subtipo {subcode} no tiene ningún ejemplo entre [ ... ] en la guía.\")\n",
    "        quote = examples[0]\n",
    "        found = find_quote_in_gold_robust(quote)\n",
    "        if not found:\n",
    "            raise ValueError(f\"[SIN FALLBACK] No encuentro el quote del subtipo {subcode} en GOLD.\\nQuote: {quote[:200]}\")\n",
    "        key, qs, qe = found\n",
    "        span = pick_annotated_span_covering(key, label, qs, qe)\n",
    "        if not span:\n",
    "            raise ValueError(f\"[SIN FALLBACK] Quote encontrado pero no cae en span anotado {label} para {subcode}\")\n",
    "\n",
    "        title = (info.get(\"title\") or \"\").strip()\n",
    "        crit  = (info.get(\"criterion\") or \"\").strip()\n",
    "        criterion = f\"{subcode} {title} — {crit}\".strip(\" —\")\n",
    "        add_example(label, criterion, key, span[0], span[1])\n",
    "\n",
    "# 4.2 P4/P9: variedad formal (4 + 4) sin subtipos (criterio fijo)\n",
    "def pick_varied_examples(label: str, k: int, patterns: list):\n",
    "    candidates = pool[label].copy()\n",
    "    random.shuffle(candidates)\n",
    "    picked = []\n",
    "    used_uids = set()\n",
    "\n",
    "    # primero cubrimos buckets por patrón\n",
    "    for _, rx in patterns:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        for ex in candidates:\n",
    "            if ex[\"id\"] in used_uids:\n",
    "                continue\n",
    "            if rx.search(ex[\"text\"]):\n",
    "                picked.append(ex)\n",
    "                used_uids.add(ex[\"id\"])\n",
    "                break\n",
    "\n",
    "    # completamos si faltan (sin repetir uid)\n",
    "    for ex in candidates:\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "        if ex[\"id\"] in used_uids:\n",
    "            continue\n",
    "        picked.append(ex)\n",
    "        used_uids.add(ex[\"id\"])\n",
    "\n",
    "    if len(picked) < k:\n",
    "        raise ValueError(f\"[SIN FALLBACK] No he podido seleccionar {k} ejemplos variados para {label}. Solo {len(picked)}.\")\n",
    "    return picked\n",
    "\n",
    "dur_patterns = [\n",
    "    (\"formalizacion\", re.compile(r\"\\bformalizaci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"fechas\", re.compile(r\"\\b\\d{1,2}\\s+de\\s+[a-záéíóú]+\\s+de\\s+\\d{4}\\b\", re.IGNORECASE)),\n",
    "    (\"anos\", re.compile(r\"\\b\\d+\\s+a[nñ]os?\\b\", re.IGNORECASE)),\n",
    "    (\"meses\", re.compile(r\"\\b\\d+\\s+mes(?:es)?\\b\", re.IGNORECASE)),\n",
    "    (\"dias\", re.compile(r\"\\b\\d+\\s+d[ií]as?\\b\", re.IGNORECASE)),\n",
    "    (\"prorroga\", re.compile(r\"\\bpr[oó]rroga\\b\", re.IGNORECASE)),\n",
    "]\n",
    "res_patterns = [\n",
    "    (\"lcsp\", re.compile(r\"\\bLCSP\\b|\\bLey\\s+9/2017\\b|\\bart[íi]culo(?:s)?\\b\", re.IGNORECASE)),\n",
    "    (\"causas\", re.compile(r\"\\bcausas?\\s+de\\s+resoluci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"incumplimiento\", re.compile(r\"\\bincumplim\", re.IGNORECASE)),\n",
    "    (\"rescision\", re.compile(r\"\\brescisi[oó]n\\b|\\brescind\", re.IGNORECASE)),\n",
    "    (\"extincion\", re.compile(r\"\\bextinci[oó]n\\b\", re.IGNORECASE)),\n",
    "    (\"mutuo\", re.compile(r\"\\bmutuo\\s+acuerdo\\b\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "for ex in pick_varied_examples(P_TO_LABEL[\"P4\"], k=4, patterns=dur_patterns):\n",
    "    add_example(P_TO_LABEL[\"P4\"], FIXED_CRITERION[\"P4\"], ex[\"id\"], ex[\"start\"], ex[\"end\"])\n",
    "\n",
    "for ex in pick_varied_examples(P_TO_LABEL[\"P9\"], k=4, patterns=res_patterns):\n",
    "    add_example(P_TO_LABEL[\"P9\"], FIXED_CRITERION[\"P9\"], ex[\"id\"], ex[\"start\"], ex[\"end\"])\n",
    "\n",
    "# =========================\n",
    "# 5) Validación + guardado + bloqueo\n",
    "# =========================\n",
    "errors = 0\n",
    "for ex in memory_selected:\n",
    "    uid = ex[\"id\"]\n",
    "    txt = key_to_text.get(uid)\n",
    "    if not txt:\n",
    "        errors += 1\n",
    "        continue\n",
    "    s, e = ex[\"start\"], ex[\"end\"]\n",
    "    if not (0 <= s < e <= len(txt)):\n",
    "        errors += 1\n",
    "        continue\n",
    "    if not txt[s:e].strip():\n",
    "        errors += 1\n",
    "\n",
    "print(\"\\nmemory_selected tamaño:\", len(memory_selected))\n",
    "print(\"Docs únicos usados:\", len(used_keys))\n",
    "print(\"Errores de validación:\", errors)\n",
    "if errors != 0:\n",
    "    raise ValueError(\"Hay errores de validación; no debería ocurrir en modo curado.\")\n",
    "\n",
    "# Resumen por etiqueta\n",
    "cnt = defaultdict(int)\n",
    "for ex in memory_selected:\n",
    "    cnt[ex[\"label\"]] += 1\n",
    "print(\"Resumen por etiqueta:\", dict(cnt))\n",
    "\n",
    "OUT_CUR = OUT_DIR / \"memory_selected_CURATED.json\"\n",
    "OUT_CUR.write_text(json.dumps(memory_selected, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado:\", OUT_CUR)\n",
    "\n",
    "blocked = sorted(list(used_keys))\n",
    "OUT_BLOCK = OUT_DIR / \"blocked_doc_uids_by_memory.json\"\n",
    "OUT_BLOCK.write_text(json.dumps(blocked, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado:\", OUT_BLOCK)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar ejemplos HUÉRFANOS (solo few-shot; NO bloqueables por doc)\n",
    "PATH_ORPHAN = OUT_DIR / \"guidelines_orphan_examples.json\"\n",
    "PATH_ORPHAN.write_text(json.dumps(orphan_examples, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado huérfanos:\", PATH_ORPHAN, \"| n:\", len(orphan_examples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a517854-ddf1-4174-a8d0-c26cb5879af4",
   "metadata": {},
   "source": [
    "**Habíamos establecido que la memoria sería más grande. Después de ver [RESUMEN] subtipos omitidos (no presentes en GOLD actual): 6 , nos quedamos con:**\n",
    "- P1.2\n",
    "- P2.3\n",
    "- P4 (4)\n",
    "- P9 (4)\n",
    "\n",
    "Debemos ampliar los de P1 y P2 que se han omitido por no aparecer en el gold, para que no se quede cojo.\n",
    "\n",
    "Los subtipos que omitimos fueron: P1.MAIN, P1.1, P1.3 y P2.MAIN, P2.1, P2.2. Como el objetivo metodológico es “sin fallbacks” + alineado a guía cuando sea posible, lo correcto es:\n",
    "- **Ampliar memoria hasta 14–16 como acordamos.**\n",
    "- **Regenerar blocked_doc_uids... a partir de esa memoria final.**\n",
    "\n",
    "El problema práctico es que varios quotes no aparecen exactamente en el GOLD (ya lo vimos con P1.MAIN y P2.1), así que “sin fallbacks” + “alineación literal exacta” no puede completarse al 100%. La salida más limpia (mínimo cambio y metodológicamente defendible) es:\n",
    "- P1/P2: intentar alineación literal robusta; si no se encuentra, seleccionar desde gold un span que cumpla el criterio usando heurísticas (no aleatorio puro), y marcarlo como (gold, criterio-sin-cita).\n",
    "- P4/P9: mantener 4+4 por variedad formal (ya lo tenemos perfecto).\n",
    "- Esto te da 14–16 sin introducir “fallback aleatorio”, sino “fallback guiado por criterio”.\n",
    "\n",
    "A continuación, dos celdas:\n",
    "1. Celda A: ampliar **memory_selected_CURATED.json** que actualmente tiene 10 → 16 con selección guiada para P1/P2 (sin citas).\n",
    "2. Celda B: regenerar **blocked_doc_uids_by_memory.json** desde la memoria final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c75a7b8-411a-4cb5-8960-a6b422c73027",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'doc_uid'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m covered:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m ex = \u001b[43madd_from_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_TO_LABEL\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mP1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m push_example(P_TO_LABEL[\u001b[33m\"\u001b[39m\u001b[33mP1\u001b[39m\u001b[33m\"\u001b[39m], criterion_text(code), ex)\n\u001b[32m    146\u001b[39m covered.add(code)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36madd_from_pool\u001b[39m\u001b[34m(label, code)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_from_pool\u001b[39m(label: \u001b[38;5;28mstr\u001b[39m, code: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    Elige un span anotado que matchee el regex del subtipo (si puede),\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    evitando repetir doc_uid ya usados en memoria.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     used_uids = \u001b[43m{\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_uid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     93\u001b[39m     candidates = pool[label]\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36m<setcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_from_pool\u001b[39m(label: \u001b[38;5;28mstr\u001b[39m, code: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    Elige un span anotado que matchee el regex del subtipo (si puede),\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    evitando repetir doc_uid ya usados en memoria.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     used_uids = {\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_uid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mem}\n\u001b[32m     93\u001b[39m     candidates = pool[label]\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n",
      "\u001b[31mKeyError\u001b[39m: 'doc_uid'"
     ]
    }
   ],
   "source": [
    "#Celda A — Completar memoria curada a 16 (P1/P2 guiado por criterio, P4/P9 intacto)\n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "MEM_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "PATH_CUR = MEM_DIR / \"memory_selected_CURATED.json\"\n",
    "assert PATH_CUR.exists(), f\"No existe {PATH_CUR}\"\n",
    "\n",
    "P_TO_LABEL = {\n",
    "    \"P1\": \"OBJETO\",\n",
    "    \"P2\": \"PRECIO_DEL_CONTRATO\",\n",
    "    \"P4\": \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"P9\": \"RESOLUCION\",\n",
    "}\n",
    "\n",
    "# --- Subtipos que queremos cubrir para MVP ---\n",
    "TARGET_P1 = [\"P1.MAIN\", \"P1.1\", \"P1.2\", \"P1.3\"]   # P1.2 ya lo tienes\n",
    "TARGET_P2 = [\"P2.MAIN\", \"P2.1\", \"P2.2\", \"P2.3\"]   # P2.3 ya lo tienes\n",
    "\n",
    "# --- Heurísticas mínimas (criterio -> regex) para elegir spans desde GOLD ---\n",
    "# Nota: aplicamos sobre el texto del span anotado (no el doc completo)\n",
    "RX = {\n",
    "  # P1\n",
    "  \"P1.MAIN\": re.compile(r\"\\b(objeto|prestaci[oó]n principal|contrato tiene por objeto|finalidad)\\b\", re.I),\n",
    "  \"P1.1\":    re.compile(r\"\\bexpediente\\b|\\b\\d{4}/[A-Z]{1,6}\\d{2}/\\d{6,}\\b|\\b\\d{4}/\\d+\\b\", re.I),\n",
    "  \"P1.2\":    re.compile(r\"\\blote\\b|\\(lote\\s*\\d+\\)|lote\\s*\\d+\", re.I),\n",
    "  \"P1.3\":    re.compile(r\"[“\\\"«].{6,}[”\\\"»]\", re.I),  # comillas con contenido (aprox.)\n",
    "\n",
    "  # P2\n",
    "  \"P2.MAIN\": re.compile(r\"€|euros?|importe|precio( total)?\", re.I),\n",
    "  \"P2.1\":    re.compile(r\"\\bIVA\\b|\\bIGIC\\b|inclu[yi]d[oa]\\b|con IVA|IVA incluido\", re.I),\n",
    "  \"P2.2\":    re.compile(r\"\\bm[aá]ximo\\b|\\bl[ií]mite\\s+m[aá]ximo\\b|\\bhasta\\s+un\\s+importe\\b\", re.I),\n",
    "  \"P2.3\":    re.compile(r\"\\bp[oó]liza\\b|\\banualidad(?:es)?\\b|\\bmensual\\b|\\brenta\\b|\\bprecio unitario\\b|\\bpor persona\\b\", re.I),\n",
    "}\n",
    "\n",
    "def load_json(p: Path):\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# --- indexar gold: key_to_text, key_to_tags y pool por etiqueta ---\n",
    "# Si ya lo tienes hecho arriba en el NB, puedes saltarte este bloque, pero no molesta.\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# gold debe existir en tu NB. Si no, carga desde ROOT/gold/corpus_annotated.jsonl\n",
    "assert \"gold\" in globals(), \"No existe `gold` en memoria. Ejecuta la celda de carga de GOLD antes.\"\n",
    "\n",
    "key_to_text = {}\n",
    "key_to_tags = {}\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\",\"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    uid = stable_uid(txt)\n",
    "    key_to_text[uid] = txt\n",
    "    key_to_tags[uid] = d.get(\"tags\", [])\n",
    "\n",
    "pool = defaultdict(list)\n",
    "for uid, txt in key_to_text.items():\n",
    "    for t in key_to_tags.get(uid, []):\n",
    "        lab = t.get(\"tag\")\n",
    "        if lab not in set(P_TO_LABEL.values()):\n",
    "            continue\n",
    "        s = int(t[\"start\"]); e = int(t[\"end\"])\n",
    "        if 0 <= s < e <= len(txt):\n",
    "            span_txt = txt[s:e]\n",
    "            if span_txt.strip():\n",
    "                pool[lab].append({\"doc_uid\": uid, \"start\": s, \"end\": e, \"text\": span_txt})\n",
    "\n",
    "# --- cargar memoria actual ---\n",
    "mem = load_json(PATH_CUR)\n",
    "\n",
    "# helper: qué criterios ya están cubiertos\n",
    "def covered_codes(mem_list):\n",
    "    codes = set()\n",
    "    for x in mem_list:\n",
    "        crit = (x.get(\"criterion\",\"\") or \"\")\n",
    "        # tomamos el primer token \"P?.?\" o \"P?.MAIN\" si aparece al inicio\n",
    "        m = re.match(r\"^\\s*(P(?:1|2)\\.(?:MAIN|\\d+))\\b\", crit)\n",
    "        if m:\n",
    "            codes.add(m.group(1))\n",
    "    return codes\n",
    "\n",
    "covered = covered_codes(mem)\n",
    "\n",
    "def add_from_pool(label: str, code: str):\n",
    "    \"\"\"\n",
    "    Elige un span anotado que matchee el regex del subtipo (si puede),\n",
    "    evitando repetir doc_uid ya usados en memoria.\n",
    "    \"\"\"\n",
    "    used_uids = {x[\"doc_uid\"] for x in mem}\n",
    "    candidates = pool[label]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No hay pool para {label}\")\n",
    "\n",
    "    rx = RX.get(code)\n",
    "    # 1) preferir match regex\n",
    "    if rx:\n",
    "        for ex in candidates:\n",
    "            if ex[\"doc_uid\"] in used_uids:\n",
    "                continue\n",
    "            if rx.search(ex[\"text\"]):\n",
    "                return ex\n",
    "\n",
    "    # 2) si no hay match, elegimos uno \"no repetido\" con longitud razonable (evita spans 0)\n",
    "    candidates2 = [ex for ex in candidates if ex[\"doc_uid\"] not in used_uids and len(ex[\"text\"]) >= 20]\n",
    "    if candidates2:\n",
    "        # elige el más “informativo” (más largo) para estabilizar\n",
    "        return sorted(candidates2, key=lambda z: len(z[\"text\"]), reverse=True)[0]\n",
    "\n",
    "    # 3) último recurso: permitir doc_uid repetido (pero esto normalmente no pasa)\n",
    "    return sorted(candidates, key=lambda z: len(z[\"text\"]), reverse=True)[0]\n",
    "\n",
    "def push_example(label: str, criterion: str, ex: dict):\n",
    "    mem.append({\n",
    "        \"label\": label,\n",
    "        \"criterion\": criterion,\n",
    "        \"doc_uid\": ex[\"doc_uid\"],\n",
    "        \"start\": ex[\"start\"],\n",
    "        \"end\": ex[\"end\"],\n",
    "    })\n",
    "\n",
    "# --- completar P1/P2 hasta cubrir TARGETS ---\n",
    "def criterion_text(code: str):\n",
    "    # texto corto y explícito: método = gold + heurística\n",
    "    # (sin citar guía porque el literal no está en gold)\n",
    "    mapping = {\n",
    "        \"P1.MAIN\": \"P1.MAIN — criterio general (gold; selección heurística)\",\n",
    "        \"P1.1\":    \"P1.1 Inclusión de expedientes en objeto — (gold; selección heurística)\",\n",
    "        \"P1.2\":    \"P1.2 Inclusión del lote en el objeto — (gold)\",\n",
    "        \"P1.3\":    \"P1.3 Delimitación del objeto entrecomillado — (gold; selección heurística)\",\n",
    "        \"P2.MAIN\": \"P2.MAIN — criterio general (gold; selección heurística)\",\n",
    "        \"P2.1\":    \"P2.1 Importes con IVA/IGIC — (gold; selección heurística)\",\n",
    "        \"P2.2\":    \"P2.2 Importes máximos — (gold; selección heurística)\",\n",
    "        \"P2.3\":    \"P2.3 Precios en otros formatos — (gold)\",\n",
    "    }\n",
    "    return mapping.get(code, f\"{code} — (gold; selección heurística)\")\n",
    "\n",
    "# P1\n",
    "for code in TARGET_P1:\n",
    "    if code in covered:\n",
    "        continue\n",
    "    ex = add_from_pool(P_TO_LABEL[\"P1\"], code)\n",
    "    push_example(P_TO_LABEL[\"P1\"], criterion_text(code), ex)\n",
    "    covered.add(code)\n",
    "\n",
    "# P2\n",
    "for code in TARGET_P2:\n",
    "    if code in covered:\n",
    "        continue\n",
    "    ex = add_from_pool(P_TO_LABEL[\"P2\"], code)\n",
    "    push_example(P_TO_LABEL[\"P2\"], criterion_text(code), ex)\n",
    "    covered.add(code)\n",
    "\n",
    "# --- sanity: esperamos 16 (4 P1 + 4 P2 + 4 P4 + 4 P9) ---\n",
    "print(\"Memory size AFTER:\", len(mem))\n",
    "counts = defaultdict(int)\n",
    "for x in mem:\n",
    "    counts[x[\"label\"]] += 1\n",
    "print(\"Counts by label:\", dict(counts))\n",
    "\n",
    "# validar offsets\n",
    "errors = 0\n",
    "for x in mem:\n",
    "    uid = x[\"doc_uid\"]\n",
    "    txt = key_to_text.get(uid, \"\")\n",
    "    s, e = int(x[\"start\"]), int(x[\"end\"])\n",
    "    if not txt or not (0 <= s < e <= len(txt)) or not txt[s:e].strip():\n",
    "        errors += 1\n",
    "print(\"Offset errors:\", errors)\n",
    "\n",
    "# Guardar como UPDATED curated\n",
    "PATH_OUT = MEM_DIR / \"memory_selected_CURATED.json\"\n",
    "PATH_OUT.write_text(json.dumps(mem, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ Guardado:\", PATH_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb52932-ff67-4f60-9b4e-b0577b3501d1",
   "metadata": {},
   "source": [
    "**Con esto conseguimos:**\n",
    "- Mantener los 10 items.\n",
    "- Añadir los 6 que faltan para completar:\n",
    "    - P1.MAIN, P1.1, P1.3\n",
    "    - P2.MAIN, P2.1, P2.2\n",
    "\n",
    "Sin aleatoriedad pura: intenta un span que case con un regex razonable del subtipo y evita repetir doc_uid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f8e7c-82ac-4f3c-b9ad-c30020f328c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda B — Regenerar blocked_keys_by_memory.json desde la memoria final \n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "MEM_DIR = ROOT / \"outputs\" / \"memory\"\n",
    "\n",
    "PATH_MEM = MEM_DIR / \"memory_selected_CURATED.json\"\n",
    "assert PATH_MEM.exists(), f\"No existe {PATH_MEM}\"\n",
    "\n",
    "mem = json.loads(PATH_MEM.read_text(encoding=\"utf-8\"))\n",
    "blocked_keys = sorted({x[\"doc_uid\"] for x in mem})\n",
    "\n",
    "# Intentamos inferir ids vs uids: si son int -> id; si son str, lo tratamos como id si existe en gold_id_set\n",
    "# (si esta celda se ejecuta sin cargar gold_id_set, caerá a uids)\n",
    "try:\n",
    "    gold_id_set\n",
    "except NameError:\n",
    "    gold_id_set = set()\n",
    "\n",
    "blocked_ids = sorted({k for k in blocked_keys if isinstance(k, int) or (isinstance(k,str) and k in gold_id_set)})\n",
    "blocked_uids_only = sorted({k for k in blocked_keys if k not in set(blocked_ids)})\n",
    "\n",
    "OUT = MEM_DIR / \"blocked_keys_by_memory.json\"\n",
    "OUT.write_text(json.dumps({\"blocked_ids\": blocked_ids, \"blocked_uids\": blocked_uids_only}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# legacy\n",
    "OUT_LEG = MEM_DIR / \"blocked_doc_uids_by_memory.json\"\n",
    "OUT_LEG.write_text(json.dumps(blocked_keys, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ blocked_keys_by_memory.json actualizado\")\n",
    "print(\"ids:\", len(blocked_ids), \"| uids:\", len(blocked_uids_only))\n",
    "print(\"legacy:\", OUT_LEG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbbce0-9c0f-416e-b652-75557493fba6",
   "metadata": {},
   "source": [
    "Una vez tenemos **memory_selected_CURATED.json**\n",
    "1. Creamos el fichero de bloqueo real\n",
    "2. Lo usamos en NB02 para excluir esos docs de val/test/prompt_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a1395-f892-4caf-9f35-570ea681923a",
   "metadata": {},
   "source": [
    "- Este **blocked_doc_uids_by_memory.json** se va consumir en NB02 para excluir docs de val/test/prompt_regression.\n",
    "- Guardamos también la memoria final que vas a usar en prompting\n",
    "- Si vamos a usar memory_selected_CURATED.json, nos aseguramos de que memory_selected apunta a esa versión y la guardamos como “final” (para no liarnos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa423b-0404-45e1-8378-4b05631ca319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para que no haya confusion, podemos añadir \"FINAL\" a los documentos que se van a estar usando en los otros NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea0ff5-cea3-4b52-b364-55d62c2bd8d5",
   "metadata": {},
   "source": [
    "**En NB03 terminamos con memoria + bloqueo listos. El siguiente paso es volver al notebook NB02 (splits), donde lo reharemos usando el fichero de bloqueo.**\n",
    "\n",
    "Objetivo de NB02 ahora:\n",
    "- Generar train/val/test/prompt_regression\n",
    "- asegurando que NINGÚN doc_uid en blocked_doc_uids_by_memory.json aparezca en:\n",
    "    - val\n",
    "    - test\n",
    "    - prompt_regression\n",
    "\n",
    "(En train podemos decidir incluirlos o no; para MVP, lo habitual es permitirlos en train pero nunca en evaluación.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73c17f-cf9e-4ed7-a52e-75e27f9517a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e015652-74bc-4247-8bbd-ec627a1536f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
