{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250eac0b-b55c-4b5c-a931-5a8cf166c9ec",
   "metadata": {},
   "source": [
    "## **NOTEBOOK 04-Experimentos 1/2/3 (prompting) y predicciones JSON estricto**\n",
    "1. Cargar prompt_regression.jsonl (para depurar prompts rápido)\n",
    "2. Definir los 3 prompts:\n",
    "    - Exp1: solo nombres de etiquetas (sin criterios)\n",
    "    - Exp2: criterios + memoria (tu memory_selected_FINAL)\n",
    "    - Exp3: criterios + memoria + few-shot adicional\n",
    "3. Generar predicciones JSON estricto (start/end/label) por documento\n",
    "4. Guardar:\n",
    "    - outputs/predictions/exp1.jsonl\n",
    "    - outputs/predictions/exp2.jsonl\n",
    "    - outputs/predictions/exp3.jsonl\n",
    "\n",
    "A continuación, haremos:\n",
    "- Cargar memoria final (memory_selected_FINAL.json) y el bloqueo anti-leakage.\n",
    "- Ejecutar Experimento 1/2/3 (prompting) y guardar predicciones en JSONL.\n",
    "- Validar salida con Pydantic (JSON estricto + offsets coherentes).\n",
    "- Incluir una forma automática de seleccionar few-shot extra (Exp3) sin selección manual y sin leakage.\n",
    "\n",
    "Nota operativa: NB04 asume que vamos a predecir sobre el gold (porque es el único con ground truth) y que NB05 evaluará contra gold_mvp. Los splits train/val/test/prompt_regression que hicimos en NB02 siguen siendo útiles para futuro fine-tuning y para testear prompts con texto no anotado, pero la evaluación real exige gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe7e105-4721-424a-8879-4563e962adc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "assert ROOT.exists()\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3cc950-9008-4ff6-9e2e-15f986fcf1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comprobamos kernel y que user site es False\n",
    "import site\n",
    "site.ENABLE_USER_SITE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ea7fb8-d933-4544-a549-5be6288e5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONNOUSERSITE = None\n",
      "ENABLE_USER_SITE = True\n",
      "USER_SITE = /home/jovyan/.local/lib/python3.11/site-packages\n"
     ]
    }
   ],
   "source": [
    "import os, site\n",
    "print(\"PYTHONNOUSERSITE =\", os.environ.get(\"PYTHONNOUSERSITE\"))\n",
    "print(\"ENABLE_USER_SITE =\", site.ENABLE_USER_SITE)\n",
    "print(\"USER_SITE =\", site.getusersitepackages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a96cc5-a0e5-4ffb-a89b-01c83dd0a600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "✅ Kernel correcto: inesagent_gpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"PY:\", sys.executable)\n",
    "assert \"/inesagent_gpu/bin/python\" in sys.executable, \\\n",
    "    \"No estás en el kernel inesagent_gpu\"\n",
    "\n",
    "print(\"✅ Kernel correcto: inesagent_gpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6e2509-1148-48a8-915e-c6985bc56ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "ROOT OK: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "#celda bootstrap para que no se rompa nunca el environment \n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "\n",
    "# Asegura que el kernel es el correcto\n",
    "print(\"Python:\", sys.executable)\n",
    "assert \"/home/jovyan/.conda/envs/inesagent_gpu\" in sys.executable, \"Kernel incorrecto (no inesagent_gpu)\"\n",
    "\n",
    "# Cache HF persistente\n",
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/jovyan/.cache/huggingface/hub\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/.cache/huggingface/hub\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/jovyan/.cache/huggingface/datasets\"\n",
    "\n",
    "print(\"ROOT OK:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedf92a8-24bc-4c6f-bdd9-aa6e71a88db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n"
     ]
    }
   ],
   "source": [
    "#Como cambiamos de local a server virtual, tenemos que cambiar los paths de todo \n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ROOT del proyecto en el servidor\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8266a9-0f4d-4cfb-b3ee-d45e7b4d6e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "True\n",
      "HOME: /home/jovyan\n"
     ]
    }
   ],
   "source": [
    "#comprobamos que el entorno es persistente (antes era efimero y se borraba al reiniciar server) \n",
    "import sys, os, pathlib\n",
    "print(sys.executable)\n",
    "print(pathlib.Path(sys.executable).exists())\n",
    "print(\"HOME:\", pathlib.Path.home())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fcbad1-d89f-4f56-9c5a-12da0d2e4dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n",
      "/home/jovyan/inesagent/gold/corpus_annotated.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/memory/memory_selected_FINAL.json -> True\n",
      "/home/jovyan/inesagent/outputs/splits/gold_val.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/gold_test.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/prompt_regression.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/train.jsonl -> True (train.jsonl puede ser opcional en NB04)\n"
     ]
    }
   ],
   "source": [
    "# Empezamos con el prompting. Imports y configuración de rutas (SERVER)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, re, hashlib, random\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "# ROOT del proyecto en el servidor\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "# --- Rutas del proyecto ---\n",
    "# Gold completo (si lo usas para resolver spans o construir índices)\n",
    "# OJO: asegúrate del nombre real del archivo (jsonl vs json)\n",
    "PATH_GOLD = ROOT / \"gold\" / \"corpus_annotated.jsonl\"   # cambia a .json si tu gold está en .json\n",
    "\n",
    "# Memoria (elige el fichero que realmente tienes)\n",
    "# Si tu fichero se llama memory_selected_CURATED.json, cámbialo aquí\n",
    "PATH_MEMORY = ROOT / \"outputs\" / \"memory\" / \"memory_selected_FINAL.json\"\n",
    "\n",
    "# Doc_uids bloqueados por leakage (si lo estás usando)\n",
    "PATH_BLOCKED = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "# Splits (según tu carpeta)\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "PATH_VAL  = SPLITS_DIR / \"gold_val.jsonl\"\n",
    "PATH_TEST = SPLITS_DIR / \"gold_test.jsonl\"\n",
    "PATH_PR   = SPLITS_DIR / \"prompt_regression.jsonl\"\n",
    "\n",
    "# Train: OJO con si es json o jsonl\n",
    "PATH_TRAIN = SPLITS_DIR / \"train.jsonl\"\n",
    "\n",
    "# Salida predicciones\n",
    "OUT_PRED = ROOT / \"outputs\" / \"predictions\"\n",
    "OUT_PRED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seed reproducible\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Etiquetas MVP\n",
    "MVP_LABELS = [\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "\n",
    "# --- Sanity checks (muy recomendables) ---\n",
    "for p in [PATH_GOLD, PATH_MEMORY, PATH_VAL, PATH_TEST, PATH_PR]:\n",
    "    print(str(p), \"->\", p.exists())\n",
    "\n",
    "print(str(PATH_TRAIN), \"->\", PATH_TRAIN.exists(), \"(train.jsonl puede ser opcional en NB04)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ad1c3b-c5e0-498f-8bc9-58bb6a9f2c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n",
      "/home/jovyan/inesagent/gold/corpus_annotated.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/memory/memory_selected_FINAL.json -> True\n",
      "/home/jovyan/inesagent/outputs/splits/gold_val.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/gold_test.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/prompt_regression.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/train.jsonl -> True (train puede ser opcional en NB04)\n",
      "UTILS cargadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# NB04 — UTILS + PATHS\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, re, hashlib, random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# ROOT del proyecto (SERVER)\n",
    "# ------------------------------------------------------------------\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PATHS del proyecto\n",
    "# ------------------------------------------------------------------\n",
    "PATH_GOLD = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "\n",
    "PATH_MEMORY = ROOT / \"outputs\" / \"memory\" / \"memory_selected_FINAL.json\"\n",
    "PATH_BLOCKED = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "PATH_VAL  = SPLITS_DIR / \"gold_val.jsonl\"\n",
    "PATH_TEST = SPLITS_DIR / \"gold_test.jsonl\"\n",
    "PATH_PR   = SPLITS_DIR / \"prompt_regression.jsonl\"\n",
    "PATH_TRAIN = SPLITS_DIR / \"train.jsonl\"  \n",
    "\n",
    "OUT_PRED = ROOT / \"outputs\" / \"predictions\"\n",
    "OUT_PRED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuración global\n",
    "# ------------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "MVP_LABELS = [\n",
    "    \"OBJETO\",\n",
    "    \"PRECIO_DEL_CONTRATO\",\n",
    "    \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"RESOLUCION\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Funciones utilitarias\n",
    "# ------------------------------------------------------------------\n",
    "def is_jsonl(path: Path) -> bool:\n",
    "    return path.suffix.lower() == \".jsonl\"\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SYSTEM prompt (neutro, estable)\n",
    "# ------------------------------------------------------------------\n",
    "SYSTEM = \"\"\"Eres un modelo de ANOTACIÓN de texto. Tu tarea es marcar spans (fragmentos) dentro de un texto.\n",
    "\n",
    "Salida obligatoria: JSON estricto con:\n",
    "- doc_uid: string\n",
    "- spans: lista de objetos {tag, start, end}\n",
    "\n",
    "Reglas:\n",
    "- start/end son offsets de caracteres 0-based sobre el texto EXACTO.\n",
    "- end es exclusivo.\n",
    "- El substring text[start:end] debe existir exactamente.\n",
    "- No inventes texto ni offsets.\n",
    "- Puedes devolver múltiples spans.\n",
    "\n",
    "Etiquetas permitidas (solo estas 4):\n",
    "OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION\n",
    "\n",
    "Devuelve SOLO JSON, sin explicaciones.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Sanity check de paths\n",
    "# ------------------------------------------------------------------\n",
    "for p in [PATH_GOLD, PATH_MEMORY, PATH_VAL, PATH_TEST, PATH_PR]:\n",
    "    print(str(p), \"->\", p.exists())\n",
    "\n",
    "print(str(PATH_TRAIN), \"->\", PATH_TRAIN.exists(), \"(train puede ser opcional en NB04)\")\n",
    "\n",
    "print(\"UTILS cargadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11bc390d-11e1-43a5-ac5d-82543582360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have tokenizer: False\n",
      "have model: False\n",
      "have run_llm: False\n",
      "have SYSTEM: True\n"
     ]
    }
   ],
   "source": [
    "#para asegurarnos de que sabemos de donde viene cualquier error (saldria \"False\") \n",
    "print(\"have tokenizer:\", \"tokenizer\" in globals())\n",
    "print(\"have model:\", \"model\" in globals())\n",
    "print(\"have run_llm:\", \"run_llm\" in globals())\n",
    "print(\"have SYSTEM:\", \"SYSTEM\" in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847c3ada-bcfb-4381-9805-34398144f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_val: 34\n",
      "gold_test: 34\n",
      "prompt_regression: 10\n",
      "train: 279\n"
     ]
    }
   ],
   "source": [
    "#Loaders (JSON / JSONL) + doc_uid estable\n",
    "def load_jsonl(path: Path) -> List[dict]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "gold_val = load_jsonl(PATH_VAL)\n",
    "gold_test = load_jsonl(PATH_TEST)\n",
    "prompt_regression = load_jsonl(PATH_PR)\n",
    "\n",
    "print(\"gold_val:\", len(gold_val))\n",
    "print(\"gold_test:\", len(gold_test))\n",
    "print(\"prompt_regression:\", len(prompt_regression))\n",
    "\n",
    "# train.jsonl solo si lo necesitas\n",
    "if PATH_TRAIN.exists():\n",
    "    train = load_jsonl(PATH_TRAIN)\n",
    "    print(\"train:\", len(train) if isinstance(train, list) else type(train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccdfd953-30b4-42f5-85de-fcd18b7c0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos is_jsonl (no es una funcion built-in)\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def is_jsonl(path: Path) -> bool:\n",
    "    return path.suffix.lower() == \".jsonl\"\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1230a03-9448-4781-b158-4db76885baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold docs: 373\n",
      "Memoria ejemplos: 8\n",
      "Blocked doc_uids: 16\n"
     ]
    }
   ],
   "source": [
    "#Cargamos gold + memoria + blocked\n",
    "# Gold\n",
    "if not PATH_GOLD.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_GOLD}\")\n",
    "gold = load_json(PATH_GOLD) if not is_jsonl(PATH_GOLD) else load_jsonl(PATH_GOLD)\n",
    "\n",
    "# Memoria final\n",
    "if not PATH_MEMORY.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_MEMORY}\")\n",
    "memory_selected = load_json(PATH_MEMORY)\n",
    "\n",
    "# Blocked uids por memoria (anti-leakage)\n",
    "if not PATH_BLOCKED.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_BLOCKED}\")\n",
    "blocked = set(load_json(PATH_BLOCKED))\n",
    "\n",
    "print(\"Gold docs:\", len(gold))\n",
    "print(\"Memoria ejemplos:\", len(memory_selected))\n",
    "print(\"Blocked doc_uids:\", len(blocked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be79a7e-fce6-4555-be13-fcc092d8d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos stable_uid para que pueda leer el resto de docs sin problema \n",
    "import hashlib\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    \"\"\"\n",
    "    UID estable y determinista a partir del texto completo del documento.\n",
    "    Usamos SHA1 para compatibilidad con el resto del pipeline.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a45832e-9998-4e37-a796-690e5356bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold MVP docs: 373\n"
     ]
    }
   ],
   "source": [
    "#Construimos gold_mvp y filtramos a 4 etiquetas \n",
    "def filter_tags_mvp(tags: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for t in tags:\n",
    "        if t.get(\"tag\") in MVP_LABELS:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "gold_mvp = []\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    uid = stable_uid(txt)\n",
    "    tags = filter_tags_mvp(d.get(\"tags\", []))\n",
    "    if not tags:\n",
    "        continue\n",
    "    gold_mvp.append({\n",
    "        \"doc_id\": d.get(\"id\"),\n",
    "        \"doc_uid\": uid,\n",
    "        \"text\": txt,\n",
    "        \"tags\": tags\n",
    "    })\n",
    "\n",
    "print(\"Gold MVP docs:\", len(gold_mvp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3e5884-f874-4015-b4af-830a2aed8540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval pool: 357\n",
      "gold_val: 35 gold_test: 35 gold_train_pool: 287\n"
     ]
    }
   ],
   "source": [
    "#Split de evaluación dentro del gold (val/test) sin leakage\n",
    "#Esto crea gold_val y gold_test evitando documentos bloqueados (usados en memoria/few-shot)\n",
    "\n",
    "eval_pool = [d for d in gold_mvp if d[\"doc_uid\"] not in blocked]\n",
    "random.shuffle(eval_pool)\n",
    "\n",
    "test_n = max(30, int(0.10 * len(eval_pool)))   # mínimo 30 si hay suficiente\n",
    "val_n  = max(30, int(0.10 * len(eval_pool)))\n",
    "\n",
    "gold_test = eval_pool[:test_n]\n",
    "gold_val  = eval_pool[test_n:test_n+val_n]\n",
    "gold_train_pool = eval_pool[test_n+val_n:]  # pool para few-shot extra, etc.\n",
    "\n",
    "print(\"Eval pool:\", len(eval_pool))\n",
    "print(\"gold_val:\", len(gold_val), \"gold_test:\", len(gold_test), \"gold_train_pool:\", len(gold_train_pool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "668d505b-4738-4c6d-ab5f-d930ec58f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic>=2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (2.12.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2) (0.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#instalamos pydantic, que usaremos en la siguiente celda \n",
    "import sys\n",
    "!{sys.executable} -m pip install -U \"pydantic>=2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffcd037-bbc7-4151-b0e3-15a93224242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON = /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "site-packages = ['/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages']\n",
      "user-site = /home/jovyan/.local/lib/python3.11/site-packages\n",
      "pydantic = 2.12.5 /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/pydantic/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#comprobamos que está\n",
    "import sys, site\n",
    "print(\"PYTHON =\", sys.executable)\n",
    "print(\"site-packages =\", site.getsitepackages())\n",
    "print(\"user-site =\", site.getusersitepackages())\n",
    "\n",
    "import pydantic\n",
    "print(\"pydantic =\", pydantic.__version__, pydantic.__file__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458d7f4b-bc29-43e0-b08e-99d685ab2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pydantic: esquema de salida estricto (spans)\n",
    "\n",
    "from typing import List, Optional, Literal, Dict, Union, Annotated, Any\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "Label = Literal[\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "\n",
    "class SpanPred(BaseModel):\n",
    "    tag: Label\n",
    "    start: int = Field(ge=0)\n",
    "    end: int = Field(ge=0)\n",
    "\n",
    "    @field_validator(\"end\")\n",
    "    @classmethod\n",
    "    def end_must_be_gt_start(cls, v, info):\n",
    "        start = info.data.get(\"start\")\n",
    "        if start is not None and v <= start:\n",
    "            raise ValueError(\"end must be > start\")\n",
    "        return v\n",
    "\n",
    "class DocPred(BaseModel):\n",
    "    doc_uid: str\n",
    "    spans: List[SpanPred]\n",
    "\n",
    "def validate_pred(doc_uid: str, text: str, pred_json: Dict[str, Any]) -> DocPred:\n",
    "    obj = DocPred(**pred_json)\n",
    "    if obj.doc_uid != doc_uid:\n",
    "        raise ValueError(\"doc_uid mismatch\")\n",
    "\n",
    "    # offsets in range + span not empty\n",
    "    for sp in obj.spans:\n",
    "        if sp.end > len(text):\n",
    "            raise ValueError(\"span end out of range\")\n",
    "        if not text[sp.start:sp.end].strip():\n",
    "            raise ValueError(\"empty span text\")\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ff8c3ab-0408-4329-9b96-7eef0a15826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: El presente contrato tendrá un plazo de vigencia desde el 1 de enero de 2024 (o desde la fecha de formalización del contrato si fuera posterior a ésta) y hasta el 31 de diciembre de 2024.\n",
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: DURACIÓN DEL CONTRATO. El Contrato tendrá una duración de VEINTICUATRO (24) MESES, contados a partir del día 1 de enero de 2024.\n",
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: El plazo de vigencia es de 2 años desde la formalización del contrato, no pudiéndose prorrogar.\n",
      "- LABEL: DURACION_TOTAL_DEL_CONTRATO\n",
      "  CRITERION: P4 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: El plazo de prestación del servicio será de 12 meses desde el 01/03/2024 hasta 28/02/2025 de acuerdo con el art. 153 de la Ley 9/2017 de Contratos del Sector Público.\n",
      "- LABEL: RESOLUCION\n",
      "  CRITERION: P9 – criterio único (subtipos solo ejemplificativos)\n",
      "  EXAMPLE_SPAN: RESOLUCIÓN DEL CONTRATO Serán causas de re ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Render de memoria (para Exp2/Exp3)\n",
    "\n",
    "# Índice para recuperar texto por doc_uid (solo de gold)\n",
    "uid_to_text = {d[\"doc_uid\"]: d[\"text\"] for d in gold_mvp}\n",
    "\n",
    "def render_memory_blocks(memory: List[Dict[str, Any]]) -> str:\n",
    "    blocks = []\n",
    "    for ex in memory:\n",
    "        uid = ex[\"doc_uid\"]\n",
    "        txt = uid_to_text.get(uid, \"\")\n",
    "        s, e = ex[\"start\"], ex[\"end\"]\n",
    "        span_txt = txt[s:e].replace(\"\\n\", \" \").strip()\n",
    "        blocks.append(\n",
    "            f\"- LABEL: {ex['label']}\\n\"\n",
    "            f\"  CRITERION: {ex['criterion']}\\n\"\n",
    "            f\"  EXAMPLE_SPAN: {span_txt}\"\n",
    "        )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "memory_text = render_memory_blocks(memory_selected)\n",
    "print(memory_text[:1200], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a19e35-1edd-4698-93a6-8f90c861d8e2",
   "metadata": {},
   "source": [
    "**Para la selección automática de few-shot extra (Exp3) de forma no manual, debemos saber:**\n",
    "\n",
    "Política (automática, reproducible):\n",
    "- Seleccionar ejemplos desde gold_train_pool (no bloqueados, no val/test).\n",
    "- Evitar doc_uids ya usados en memoria.\n",
    "- Para P1/P2: 1 ejemplo extra por etiqueta (diversidad lexical simple).\n",
    "- Para P4: 2 ejemplos extra (uno con “formalización”, otro con fechas).\n",
    "- Para P9: 2 ejemplos extra (uno con “LCSP/artículos”, otro con “incumplimiento/causas”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f7ca220-c7fc-43d2-a1c7-4ad1216a0672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewshot_extra: 6\n",
      "OBJETO 5ff3583e80 SUMINISTRO DE DOS FURGONES (UN VEHÍCULO DE COMBUSTIÓN ASISTENCIAL CARROZADO COMO AMBULANCIA PARA EL SERVICIO DE PROTECCIÓN CIVIL Y UN FURGÓN\n",
      "PRECIO_DEL_CONTRATO 5ff3583e80 128.491,47 euros, IVA incluido\n",
      "DURACION_TOTAL_DEL_CONTRATO 98bf9c3389 El plazo de duración del contrato es de 24 meses. El objeto del contrato consta de 2 revisiones, la primera se realizará, tras la formalizac\n",
      "DURACION_TOTAL_DEL_CONTRATO 6bb61ba065 El plazo de ejecución de la prestación objeto de este contrato será desde el 01 de diciembre de 2023, o desde la firma del contrato si ésta \n",
      "RESOLUCION 98bf9c3389 Serán causas de resolución de este contrato las establecidas en los artículos 211 y 313 de la LCSP.\n",
      "RESOLUCION 98bf9c3389 Serán causas de resolución de este contrato las establecidas en los artículos 211 y 313 de la LCSP.\n"
     ]
    }
   ],
   "source": [
    "#Selección automática de few-shot extra (Exp3) (no manual)\n",
    "memory_uids = {ex[\"doc_uid\"] for ex in memory_selected}\n",
    "\n",
    "def spans_for_label(doc, label):\n",
    "    txt = doc[\"text\"]\n",
    "    for t in doc[\"tags\"]:\n",
    "        if t[\"tag\"] == label:\n",
    "            s, e = int(t[\"start\"]), int(t[\"end\"])\n",
    "            yield (s, e, txt[s:e])\n",
    "\n",
    "def pick_first_match(label: str, regex: re.Pattern, k: int) -> List[Dict[str, Any]]:\n",
    "    picked = []\n",
    "    used = set()\n",
    "    for d in gold_train_pool:\n",
    "        if d[\"doc_uid\"] in memory_uids:\n",
    "            continue\n",
    "        if d[\"doc_uid\"] in used:\n",
    "            continue\n",
    "        for s, e, span_txt in spans_for_label(d, label):\n",
    "            if regex.search(span_txt):\n",
    "                picked.append({\n",
    "                    \"label\": label,\n",
    "                    \"criterion\": f\"FEWSHOT_EXTRA ({label})\",\n",
    "                    \"doc_uid\": d[\"doc_uid\"],\n",
    "                    \"start\": s,\n",
    "                    \"end\": e,\n",
    "                })\n",
    "                used.add(d[\"doc_uid\"])\n",
    "                break\n",
    "        if len(picked) >= k:\n",
    "            break\n",
    "    return picked\n",
    "\n",
    "fewshot_extra = []\n",
    "# OBJETO: 1 extra (p.ej. contiene \"SERVICIO\" / \"SUMINISTRO\" típicos)\n",
    "fewshot_extra += pick_first_match(\"OBJETO\", re.compile(r\"\\bSERVICIO\\b|\\bSUMINISTRO\\b|\\bOBRAS\\b\", re.IGNORECASE), k=1)\n",
    "# PRECIO: 1 extra (IVA / euros)\n",
    "fewshot_extra += pick_first_match(\"PRECIO_DEL_CONTRATO\", re.compile(r\"€|euros|IVA\", re.IGNORECASE), k=1)\n",
    "# DURACION: 2 extras (formalización + fechas)\n",
    "fewshot_extra += pick_first_match(\"DURACION_TOTAL_DEL_CONTRATO\", re.compile(r\"formalizaci[oó]n\", re.IGNORECASE), k=1)\n",
    "fewshot_extra += pick_first_match(\"DURACION_TOTAL_DEL_CONTRATO\", re.compile(r\"\\b\\d{1,2}\\s+de\\s+[a-záéíóú]+\\s+de\\s+\\d{4}\\b\", re.IGNORECASE), k=1)\n",
    "# RESOLUCION: 2 extras (LCSP/artículos + incumplimiento/causas)\n",
    "fewshot_extra += pick_first_match(\"RESOLUCION\", re.compile(r\"\\bLCSP\\b|Ley\\s+9/2017|art[ií]cul\", re.IGNORECASE), k=1)\n",
    "fewshot_extra += pick_first_match(\"RESOLUCION\", re.compile(r\"incumplim|causas?\\s+de\\s+resoluci\", re.IGNORECASE), k=1)\n",
    "\n",
    "print(\"fewshot_extra:\", len(fewshot_extra))\n",
    "for ex in fewshot_extra:\n",
    "    txt = uid_to_text[ex[\"doc_uid\"]]\n",
    "    print(ex[\"label\"], ex[\"doc_uid\"][:10], txt[ex[\"start\"]:ex[\"end\"]][:140].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9982010-3f2f-421b-b52e-d3b7ffa9f706",
   "metadata": {},
   "source": [
    "**Guardado opcional**\n",
    "\n",
    "Cómo se usará en Exp3: memory_selected + fewshot_extra (y se bloquean también sus doc_uids si luego queremos máxima limpieza)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7212bf71-3bdb-4360-98de-b272aa23541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Guardado opcional\n",
    "(PATH_GOLD.parent.parent / \"outputs\" / \"memory\" / \"fewshot_extra.json\").write_text(\n",
    "    json.dumps(fewshot_extra, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e390a21-71e3-4654-864f-11f9dd65010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Templates de prompts (Exp1/Exp2/Exp3)\n",
    "SYSTEM = (\n",
    "    \"Eres un anotador jurídico experto en contratos del sector público en España. \"\n",
    "    \"Tu tarea es identificar fragmentos textuales (segmentos/oraciones/cláusulas) que cumplen una función jurídica \"\n",
    "    \"y devolver sus offsets start/end con una etiqueta. \"\n",
    "    \"No extraigas palabras sueltas: selecciona el fragmento mínimo suficiente que expresa la función.\"\n",
    ")\n",
    "\n",
    "LABEL_DEFS_MIN = (\n",
    "    \"Etiquetas disponibles:\\n\"\n",
    "    \"- OBJETO: prestación principal del contrato.\\n\"\n",
    "    \"- PRECIO_DEL_CONTRATO: cuantía del contrato (con o sin IVA/IGIC según corresponda).\\n\"\n",
    "    \"- DURACION_TOTAL_DEL_CONTRATO: vigencia/plazo total y condiciones de inicio del plazo.\\n\"\n",
    "    \"- RESOLUCION: causas y régimen de resolución/extinción del contrato.\\n\"\n",
    ")\n",
    "\n",
    "OUTPUT_CONSTRAINTS = (\n",
    "    \"Devuelve SOLO un JSON válido con esta forma:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"doc_uid\": \"<doc_uid>\",\\n'\n",
    "    '  \"spans\": [ {\"tag\":\"OBJETO\",\"start\":0,\"end\":10}, ... ]\\n'\n",
    "    \"}\\n\"\n",
    "    \"Reglas:\\n\"\n",
    "    \"- start/end son offsets de caracteres sobre el texto original.\\n\"\n",
    "    \"- end es exclusivo.\\n\"\n",
    "    \"- No inventes offsets.\\n\"\n",
    "    \"- No incluyas texto fuera del JSON.\\n\"\n",
    ")\n",
    "\n",
    "def make_user_prompt_exp1(doc_uid: str, text: str) -> str:\n",
    "    return (\n",
    "        LABEL_DEFS_MIN + \"\\n\"\n",
    "        + OUTPUT_CONSTRAINTS + \"\\n\"\n",
    "        + f'DOC_UID: {doc_uid}\\n'\n",
    "        + \"TEXTO:\\n\"\n",
    "        + text\n",
    "    )\n",
    "\n",
    "def make_user_prompt_exp2(doc_uid: str, text: str, memory_str: str) -> str:\n",
    "    return (\n",
    "        LABEL_DEFS_MIN + \"\\n\"\n",
    "        + \"Criterios y ejemplos (memoria):\\n\"\n",
    "        + memory_str + \"\\n\\n\"\n",
    "        + OUTPUT_CONSTRAINTS + \"\\n\"\n",
    "        + f'DOC_UID: {doc_uid}\\n'\n",
    "        + \"TEXTO:\\n\"\n",
    "        + text\n",
    "    )\n",
    "\n",
    "def make_user_prompt_exp3(doc_uid: str, text: str, memory_str: str, fewshot_str: str) -> str:\n",
    "    return (\n",
    "        LABEL_DEFS_MIN + \"\\n\"\n",
    "        + \"Criterios y ejemplos (memoria):\\n\"\n",
    "        + memory_str + \"\\n\\n\"\n",
    "        + \"Few-shot adicional:\\n\"\n",
    "        + fewshot_str + \"\\n\\n\"\n",
    "        + OUTPUT_CONSTRAINTS + \"\\n\"\n",
    "        + f'DOC_UID: {doc_uid}\\n'\n",
    "        + \"TEXTO:\\n\"\n",
    "        + text\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee6c99-cb66-4296-bf26-9b87105197d8",
   "metadata": {},
   "source": [
    "## Cargamos el modelo (GPU)\n",
    "\n",
    "Para nuestra instancia con vGPU 20GB (INESData), lo más práctico es Transformers + 4-bit. vLLM va muy bien, pero requiere más setup; para un NB, Transformers suele ser más directo.\n",
    "\n",
    "Elegimos entre: **Mistral 7B Instruct** o **Llama 3.1 8B Instruct**.\n",
    "\n",
    "Modelo recomendado para tus experimentos (España, jurídico, español)\n",
    "\n",
    "Para prompting en español con 20GB VRAM:\n",
    "- Meta-Llama-3.1-8B-Instruct en 4-bit. Suele ser más robusto “cero/few-shot” y consistente en instrucciones largas (opción recomendada)\n",
    "- Mistral-7B-Instruct-v0.3 en 4-bit. Muy eficiente y suele rendir bien con prompts estructurados (alternativa muy válida)\n",
    "\n",
    "Qué elegir ahora: yo elegiría Llama 3.1 8B Instruct si tienes acceso al checkpoint. Si no, usa Mistral 7B Instruct v0.3.\n",
    "\n",
    "Por ahora, elegimos **Modelo: Meta-Llama-3.1-8B-Instruct (4-bit)**\n",
    "\n",
    "Motivo: mejor comportamiento que Mistral 7B con:\n",
    "- instrucciones largas,\n",
    "- criterios explícitos,\n",
    "- memoria estructurada,\n",
    "- español jurídico.\n",
    "\n",
    "**!! Como experimento, probaremos los 2 a ver cuál da mejores resultados**\n",
    "Solo habría que cambiar:\n",
    "- *MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"*\n",
    "- por *MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061908bd-8c91-43c6-bc1b-1bc60c56d3b5",
   "metadata": {},
   "source": [
    "## Instalación transformers y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49ba83c6-ca26-4943-9f44-f323fc874e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "USER_SITE: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "PATH has ~/.local/bin?: False\n",
      "\n",
      "=== transformers ===\n",
      " Name: transformers\n",
      "Version: 4.46.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by:\n",
      "\n",
      "=== huggingface-hub ===\n",
      " Name: huggingface_hub\n",
      "Version: 0.36.1\n",
      "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
      "Home-page: https://github.com/huggingface/huggingface_hub\n",
      "Author: Hugging Face, Inc.\n",
      "Author-email: julien@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, hf-xet, packaging, pyyaml, requests, tqdm, typing-extensions\n",
      "Required-by: accelerate, tokenizers, transformers\n",
      "\n",
      "=== accelerate ===\n",
      " Name: accelerate\n",
      "Version: 1.12.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      "Requires: huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by:\n",
      "\n",
      "=== bitsandbytes ===\n",
      " Name: bitsandbytes\n",
      "Version: 0.49.1\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Author: \n",
      "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
      "License-Expression: MIT\n",
      "Location: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      "Requires: numpy, packaging, torch\n",
      "Required-by:\n",
      "\n",
      "=== torch ===\n",
      " Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, bitsandbytes\n"
     ]
    }
   ],
   "source": [
    "import sys, site, os, subprocess, textwrap\n",
    "\n",
    "print(\"PYTHON:\", sys.executable)\n",
    "print(\"USER_SITE:\", site.getusersitepackages())\n",
    "print(\"PATH has ~/.local/bin?:\", os.path.expanduser(\"~/.local/bin\") in os.environ.get(\"PATH\",\"\"))\n",
    "\n",
    "def pip_show(pkg):\n",
    "    r = subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", pkg], capture_output=True, text=True)\n",
    "    return r.stdout.strip() if r.returncode == 0 else \"(not installed)\"\n",
    "\n",
    "for p in [\"transformers\",\"huggingface-hub\",\"accelerate\",\"bitsandbytes\",\"torch\"]:\n",
    "    print(\"\\n===\", p, \"===\\n\", pip_show(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88071632-b3aa-4b04-8a2d-2fa5eb001806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: /home/jovyan/.conda/envs/inesagent_gpu/bin/python -m pip install --user -U huggingface_hub>=0.30,<1.0 transformers>=4.45,<4.47 accelerate>=0.34,<2.0 tokenizers>=0.20,<0.22 safetensors>=0.4 sentencepiece protobuf<5 jsonschema pydantic>=2.5 tqdm regex numpy bitsandbytes>=0.43\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.30 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.36.1)\n",
      "Requirement already satisfied: transformers<4.47,>=4.45 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: accelerate<2.0,>=0.34 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.20 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.20.3)\n",
      "Collecting tokenizers<0.22,>=0.20\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.2.1)\n",
      "Collecting protobuf<5\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting jsonschema\n",
      "  Downloading jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pydantic>=2.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (2.12.5)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (4.67.2)\n",
      "Requirement already satisfied: regex in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (2026.1.15)\n",
      "Requirement already satisfied: numpy in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (2.4.2)\n",
      "Requirement already satisfied: bitsandbytes>=0.43 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (4.15.0)\n",
      "Requirement already satisfied: psutil in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate<2.0,>=0.34) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate<2.0,>=0.34) (2.5.1)\n",
      "Collecting attrs>=22.2.0 (from jsonschema)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema)\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema)\n",
      "  Downloading rpds_py-0.30.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2.5) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from pydantic>=2.5) (0.4.2)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate<2.0,>=0.34)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate<2.0,>=0.34) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate<2.0,>=0.34) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<2.0,>=0.34) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate<2.0,>=0.34) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (2026.1.4)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.30.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n",
      "Installing collected packages: sympy, rpds-py, protobuf, attrs, referencing, jsonschema-specifications, jsonschema\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/7\u001b[0m [sympy]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [jsonschema]7\u001b[0m [jsonschema]specifications]\n",
      "\u001b[1A\u001b[2KSuccessfully installed attrs-25.4.0 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 protobuf-4.25.8 referencing-0.37.0 rpds-py-0.30.0 sympy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script jsonschema is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Instalación/upgrade terminado.\n",
      "👉 IMPORTANTE: Reinicia el kernel AHORA (Kernel -> Restart) y vuelve a ejecutar las celdas.\n"
     ]
    }
   ],
   "source": [
    "#boostrap estable, instala/actualiza todo en el py del kernel \n",
    "import sys, subprocess\n",
    "\n",
    "pkgs = [\n",
    "    # núcleo HF (versiones compatibles)\n",
    "    \"huggingface_hub>=0.30,<1.0\",\n",
    "    \"transformers>=4.45,<4.47\",\n",
    "    \"accelerate>=0.34,<2.0\",\n",
    "    \"tokenizers>=0.20,<0.22\",\n",
    "    \"safetensors>=0.4\",\n",
    "\n",
    "    # utilidades frecuentes\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf<5\",  # evita roturas típicas con algunas wheels\n",
    "    \"jsonschema\",\n",
    "    \"pydantic>=2.5\",\n",
    "    \"tqdm\",\n",
    "    \"regex\",\n",
    "    \"numpy\",\n",
    "\n",
    "    # cuantización 4-bit (si tu runtime lo soporta)\n",
    "    \"bitsandbytes>=0.43\",\n",
    "]\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"-U\"] + pkgs\n",
    "print(\"RUN:\", \" \".join(cmd))\n",
    "subprocess.check_call(cmd)\n",
    "\n",
    "print(\"\\n✅ Instalación/upgrade terminado.\")\n",
    "print(\"👉 IMPORTANTE: Reinicia el kernel AHORA (Kernel -> Restart) y vuelve a ejecutar las celdas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca65a32c-7bca-432b-a00e-ab0ac7d0689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.46.3\n",
      "huggingface_hub: 0.36.1\n",
      "accelerate: 1.12.0\n",
      "✅ BitsAndBytesConfig OK\n"
     ]
    }
   ],
   "source": [
    "#verificacion despues de reiniciar el kernel \n",
    "import transformers, huggingface_hub, accelerate\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "print(\"✅ BitsAndBytesConfig OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86e4ce93-72db-44b6-852e-d5d59202a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (915.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.6/915.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-1.3.7-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2023.6.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx<1,>=0.23.0 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m240.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.65.0)\n",
      "Collecting typer-slim (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m316.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Collecting cuda-bindings==12.9.4 (from torch>=2.0.0->accelerate)\n",
      "  Downloading cuda_bindings-12.9.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvshmem-cu12==3.4.5 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.6.0 (from torch>=2.0.0->accelerate)\n",
      "  Downloading triton-3.6.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.7.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2023.5.7)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.1.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.0)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, typer-slim, triton, sympy, shellingham, safetensors, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, h11, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, httpcore, cuda-bindings, nvidia-cusolver-cu12, httpx, torch, huggingface_hub, accelerate\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts hf and tiny-agents are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optimum 1.16.1 requires transformers[sentencepiece]>=4.26.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.12.0 cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-1.3.7 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 safetensors-0.7.0 shellingham-1.5.4 sympy-1.14.0 torch-2.10.0 triton-3.6.0 typer-slim-0.21.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e738e9-ba41-45e0-8d2b-f3aad25c9eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.12.0 /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/accelerate/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#comprobamos que los paquetes se estén instalando en inesagent_gpu \n",
    "import accelerate\n",
    "print(\"accelerate:\", accelerate.__version__, accelerate.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61537941-18cb-40c9-9eb3-53589efb241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<3,>=2.3 in /home/jovyan/.local/lib/python3.11/site-packages (from bitsandbytes) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2023.6.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/jovyan/.local/lib/python3.11/site-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (2.1.3)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23b3934f-b21e-4e55-8114-4d37623c76af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad8508e1-0066-49ed-bcca-93b874dddc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/jovyan/.local/lib/python3.11/site-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer-slim in /home/jovyan/.local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jovyan/.local/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/jovyan/.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jovyan/.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /home/jovyan/.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->transformers) (8.1.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.7.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jovyan/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jovyan/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[33m  WARNING: The script transformers is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.22.2 transformers-5.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Instalamos bitsandbites (libreria para optimizar modelos de IA) (solo 1 vez) y reiniciamos Kernel\n",
    "#separo los install para que no pete (con !pip install -U transformers accelerate bitsandbytes sentencepiece de golpe se muere)\n",
    "!pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d511d5e-623c-42b9-965e-a7a7ffa0a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.46.3\n",
      "huggingface_hub: 0.36.1\n",
      "accelerate: 1.12.0\n",
      "bitsandbytes: 0.49.1\n"
     ]
    }
   ],
   "source": [
    "#Verifico versiones\n",
    "import transformers, huggingface_hub, accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"bitsandbytes:\", bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c44bd4f-95bf-4482-a18a-43b3c11b3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.33\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub<1.0,>=0.34\n",
      "  Downloading huggingface_hub-0.36.1-py3-none-any.whl (566 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers==4.45.2)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.45.2)\n",
      "  Downloading numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging>=20.0 (from transformers==4.45.2)\n",
      "  Downloading packaging-26.0-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m267.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1 (from transformers==4.45.2)\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.45.2)\n",
      "  Downloading regex-2026.1.15-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.2)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m361.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers==4.45.2)\n",
      "  Downloading tqdm-4.67.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m263.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting psutil (from accelerate>=0.33)\n",
      "  Downloading psutil-7.2.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=2.0.0 (from accelerate>=0.33)\n",
      "  Downloading torch-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (915.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.6/915.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface_hub<1.0,>=0.34)\n",
      "  Downloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub<1.0,>=0.34)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface_hub<1.0,>=0.34)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m348.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.5.1 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m248.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuda-bindings==12.9.4 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading cuda_bindings-12.9.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvshmem-cu12==3.4.5 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.6.0 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading triton-3.6.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.45.2)\n",
      "  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m161.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests->transformers==4.45.2)\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m373.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->transformers==4.45.2)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers==4.45.2)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, triton, tqdm, sympy, sentencepiece, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, cuda-pathfinder, charset_normalizer, certifi, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, cuda-bindings, nvidia-cusolver-cu12, huggingface_hub, torch, tokenizers, transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.6.0\n",
      "    Uninstalling triton-3.6.0:\n",
      "      Successfully uninstalled triton-3.6.0\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tqdm is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.2.1\n",
      "    Uninstalling sentencepiece-0.2.1:\n",
      "      Successfully uninstalled sentencepiece-0.2.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-nvshmem-cu12\n",
      "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
      "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
      "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.13.1.3\n",
      "    Uninstalling nvidia-cufile-cu12-1.13.1.3:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: cuda-pathfinder\n",
      "    Found existing installation: cuda-pathfinder 1.3.3\n",
      "    Uninstalling cuda-pathfinder-1.3.3:\n",
      "      Successfully uninstalled cuda-pathfinder-1.3.3\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: cuda-bindings\n",
      "    Found existing installation: cuda-bindings 12.9.4\n",
      "    Uninstalling cuda-bindings-12.9.4:\n",
      "      Successfully uninstalled cuda-bindings-12.9.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface_hub 1.3.7\n",
      "    Uninstalling huggingface_hub-1.3.7:\n",
      "      Successfully uninstalled huggingface_hub-1.3.7\n",
      "\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.10.0\n",
      "    Uninstalling torch-2.10.0:\n",
      "      Successfully uninstalled torch-2.10.0\n",
      "\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.2\n",
      "    Uninstalling tokenizers-0.22.2:\n",
      "      Successfully uninstalled tokenizers-0.22.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.0.0\n",
      "    Uninstalling transformers-5.0.0:\n",
      "      Successfully uninstalled transformers-5.0.0\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.49.1\n",
      "    Uninstalling bitsandbytes-0.49.1:\n",
      "      Successfully uninstalled bitsandbytes-0.49.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.12.0\n",
      "    Uninstalling accelerate-1.12.0:\n",
      "      Successfully uninstalled accelerate-1.12.0\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.15.0 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
      "google-auth 2.21.0 requires urllib3<2.0, but you have urllib3 2.6.3 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "scipy 1.11.1 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.4.2 which is incompatible.\n",
      "ipympl 0.9.3 requires ipython<9, but you have ipython 9.10.0 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 accelerate-1.12.0 bitsandbytes-0.49.1 certifi-2026.1.4 charset_normalizer-3.4.4 cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 filelock-3.20.3 fsspec-2026.1.0 hf-xet-1.2.0 huggingface_hub-0.36.1 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 packaging-26.0 psutil-7.2.2 pyyaml-6.0.3 regex-2026.1.15 requests-2.32.5 safetensors-0.7.0 sentencepiece-0.2.1 sympy-1.14.0 tokenizers-0.20.3 torch-2.10.0 tqdm-4.67.2 transformers-4.45.2 triton-3.6.0 typing-extensions-4.15.0 urllib3-2.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#reparo transformers (pip) y reinicio kernel para evitar errores \"Could not find LlamaForCausalLM\" (mezcla de paquetes)\n",
    "#reduzco transformers a 4.45.2 (más estable para Llama 3)\n",
    "!pip install -U --no-cache-dir --force-reinstall \\\n",
    "  \"transformers==4.45.2\" \\\n",
    "  \"accelerate>=0.33\" \\\n",
    "  \"huggingface_hub>=0.34,<1.0\" \\\n",
    "  \"safetensors>=0.4\" \\\n",
    "  \"sentencepiece\" \\\n",
    "  \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a9763a0-3e27-4a70-8626-c4f5468cc212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "#despues de reinstalar torch CUDA con mamba (piso el torch de pip por tener otra version).  \n",
    "#despues en terminal fuerzo a eliminar el torch de pip\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "116c74e8-69a4-4883-8bd5-2f077e47e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.46.3\n",
      "huggingface_hub: 0.36.1\n",
      "accelerate: 1.12.0\n",
      "bitsandbytes: 0.49.1\n"
     ]
    }
   ],
   "source": [
    "#Verifico versiones\n",
    "import transformers, huggingface_hub, accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"bitsandbytes:\", bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f3ee3ae-fec7-41b8-8336-101d578f499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT OK: /home/jovyan/inesagent\n",
      "config: True\n",
      "gold: True\n",
      "outputs: True\n"
     ]
    }
   ],
   "source": [
    "#después de instalar pytorch y el resto de paquetes, descomprimir el zip de carpetas de inesagent verificamos que esté todo:\n",
    "from pathlib import Path\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), ROOT\n",
    "print(\"ROOT OK:\", ROOT)\n",
    "\n",
    "print(\"config:\", (ROOT/\"config\").exists())\n",
    "print(\"gold:\", (ROOT/\"gold\").exists())\n",
    "print(\"outputs:\", (ROOT/\"outputs\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f004f019-e06b-4069-9204-7de377a3632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n",
      "gpu: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n",
      "VRAM (GB): 42.29\n"
     ]
    }
   ],
   "source": [
    "#sanitycheck GPU y VRAM \n",
    "import torch\n",
    "print(\"cuda:\", torch.cuda.is_available())\n",
    "print(\"gpu:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(\"VRAM (GB):\", round(props.total_memory/1e9, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e175fbd1-c71a-456a-8df9-2c5d629efbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: accelerate>=0.33 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.49.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.34 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.36.1)\n",
      "Requirement already satisfied: safetensors in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: requests in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (4.67.2)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers==4.45.2) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.34) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.34) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.34) (4.15.0)\n",
      "Requirement already satisfied: psutil in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate>=0.33) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate>=0.33) (2.5.1)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate>=0.33)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.33) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.33) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.33) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.33) (3.0.3)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed sympy-1.14.0 transformers-4.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#aseguramos dependencias \n",
    "import sys\n",
    "!{sys.executable} -m pip install -U --no-cache-dir \\\n",
    "  \"transformers==4.45.2\" \"accelerate>=0.33\" \"bitsandbytes\" \\\n",
    "  \"huggingface_hub>=0.34,<1.0\" \"safetensors\" \"sentencepiece\" \"requests\" \"tqdm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6031215b-9bbb-4d8e-90f9-611670ad5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF login + acceso OK\n"
     ]
    }
   ],
   "source": [
    "#Hacemos login Huggingface con nuestro token (inesagent-token) \n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "login(token=\"hf_REDACTED\")  #mi token\n",
    "\n",
    "# test gated\n",
    "hf_hub_download(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"config.json\")\n",
    "print(\"HF login + acceso OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60531afa-916c-4cdb-98f5-d3e88484852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared\n"
     ]
    }
   ],
   "source": [
    "#limpiamos VRAM antes de cargar para evitar fragmentación\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "print(\"GPU cache cleared\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c149ea-ee78-4ef6-8332-03484cea2013",
   "metadata": {},
   "source": [
    "## Cargamos el modelo \n",
    "- MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "- MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07d6cf10-03e7-4673-86f7-526431faac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "transformers: 4.46.3\n",
      "transformers file: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#comprobar que estamos usando transformers desde env/kernal, y no user-site. Limpio local y reinstalo dentro del env \n",
    "import sys, transformers\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"transformers file:\", transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd9cf602-d9d9-40ab-8312-3e461f169c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "ENABLE_USER_SITE: True\n",
      "USER_SITE: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "sys.path (primeros 8):\n",
      " - /home/jovyan/.conda/envs/inesagent_gpu/lib/python311.zip\n",
      " - /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11\n",
      " - /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/lib-dynload\n",
      " - \n",
      " - /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages\n",
      " - /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/setuptools/_vendor\n",
      " - /tmp/tmpysprx0hf\n"
     ]
    }
   ],
   "source": [
    "import sys, site\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"ENABLE_USER_SITE:\", site.ENABLE_USER_SITE)\n",
    "print(\"USER_SITE:\", site.getusersitepackages())\n",
    "print(\"sys.path (primeros 8):\")\n",
    "for p in sys.path[:8]:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e2be828-30c8-4446-8720-f84410e9879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_SITE: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "OK: user-site fuera de sys.path y PYTHONNOUSERSITE=1\n"
     ]
    }
   ],
   "source": [
    "#Desactivar user-site dentro del kernel (parche inmediato)\n",
    "import sys, site, os\n",
    "\n",
    "user_site = site.getusersitepackages()\n",
    "print(\"USER_SITE:\", user_site)\n",
    "\n",
    "# 1) quita user-site del sys.path si está\n",
    "sys.path = [p for p in sys.path if p != user_site]\n",
    "\n",
    "# 2) y además fija la variable para procesos hijos (pip, etc.)\n",
    "os.environ[\"PYTHONNOUSERSITE\"] = \"1\"\n",
    "\n",
    "print(\"OK: user-site fuera de sys.path y PYTHONNOUSERSITE=1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06d92e73-30ba-4d7f-a881-7d946cf08044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tokenizers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping huggingface-hub as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping safetensors as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping sentencepiece as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#“Purge” de los paquetes en ~/.local (para que no vuelvan) \n",
    "import sys, os\n",
    "os.environ[\"PYTHONNOUSERSITE\"] = \"1\"\n",
    "\n",
    "# Desinstala de user-site (si existían ahí)\n",
    "!python -m pip uninstall -y transformers tokenizers huggingface-hub safetensors accelerate bitsandbytes peft sentencepiece\n",
    "\n",
    "# Limpia caches (opcional pero recomendable)\n",
    "!rm -rf /home/jovyan/.cache/huggingface /home/jovyan/.cache/torch /home/jovyan/.cache/pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a8e86a0-6724-4403-a0e1-e5d5c201eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n"
     ]
    }
   ],
   "source": [
    "#veriricamos que seguimos dentro del entorno\n",
    "import sys\n",
    "print(\"PYTHON:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "690cd08e-1ac6-4d9a-91c6-a97621c7458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (26.0)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (80.10.2)\n",
      "Requirement already satisfied: wheel in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.46.3)\n",
      "Requirement already satisfied: packaging in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (26.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub<1.0,>=0.30 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.36.1)\n",
      "Requirement already satisfied: transformers<4.47,>=4.46 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: tokenizers in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.20.3)\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: safetensors in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: accelerate in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.49.1)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (4.67.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.30) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers<4.47,>=4.46) (2.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from transformers<4.47,>=4.46) (2026.1.15)\n",
      "Requirement already satisfied: psutil in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.30) (2026.1.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#instalamos dependencias dentro del env (correcto y seguro) \n",
    "!PIP_USER=false PYTHONNOUSERSITE=1 {sys.executable} -m pip install --no-user --no-warn-script-location \\\n",
    "    -U pip setuptools wheel packaging\n",
    "\n",
    "!PIP_USER=false PYTHONNOUSERSITE=1 {sys.executable} -m pip install --no-user --no-warn-script-location \\\n",
    "    -U \"huggingface_hub>=0.30,<1.0\" \\\n",
    "       \"transformers>=4.46,<4.47\" \\\n",
    "       tokenizers safetensors accelerate bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2630dad-8c93-44df-a686-29d70b88550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.46.3\n",
      "Transformers path: /home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#despues de instalar, ejecutamos para comprobar que ya no hay nada en .local\n",
    "import transformers, sys\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers path:\", transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74cd8c73-7a06-4902-8d0e-2a62027c3b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-site: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "User-site not found.\n"
     ]
    }
   ],
   "source": [
    "#sigue saliendo .local, por lon que vamos a borrar manualmente todo lo instalado ahi para que transformers no se aloje ahi\n",
    "import shutil, site, os\n",
    "user_site = site.USER_SITE\n",
    "print(\"User-site:\", user_site)\n",
    "\n",
    "if os.path.exists(user_site):\n",
    "    shutil.rmtree(user_site)\n",
    "    print(\"User-site deleted.\")\n",
    "else:\n",
    "    print(\"User-site not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee4937c9-cfff-4766-b0fc-d2bbeac32151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF login + acceso OK\n"
     ]
    }
   ],
   "source": [
    "#Hacemos login Huggingface con nuestro token (inesagent-token) \n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "login(token=\"hf_REDACTED\")  #mi token\n",
    "\n",
    "# test gated\n",
    "hf_hub_download(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"config.json\")\n",
    "print(\"HF login + acceso OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f9f31d9-cf39-4672-bf88-90656347d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.1\n",
      "transformers: 4.46.3\n",
      "cuda available: True\n",
      "gpu: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n"
     ]
    }
   ],
   "source": [
    "#checks rapidos\n",
    "import torch, transformers\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d9b4e25-8662-4b69-90a3-471b714cfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:56<00:00, 29.11s/it]\n",
      "/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "#pipeline en GPU + control de longitud (lo importante)\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device_map=\"auto\",                      # <-- usa GPU automáticamente\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},  # fp16 suele ir bien en vGPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "out = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=64,          # <-- evita el error de max_length\n",
    "    do_sample=False,            # determinista (útil para anotación/IE)\n",
    "    return_full_text=False,     # solo respuesta (opcional)\n",
    ")\n",
    "\n",
    "print(out[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e107f9c-fb92-4754-9073-336376725be9",
   "metadata": {},
   "source": [
    "## Los modelos “Instruct” suelen esperar un formato de chat (plantilla)\n",
    "\n",
    "pipeline puede manejar messages, pero si quieres control total (y reproducibilidad), usa `apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62abc1dd-757e-4678-a712-d807d3f9e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "prompt = tok.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919181b-62ca-4dc2-9f4e-5c8bc8ade781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19205aef-d51d-4319-aa73-083c3ceebd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True | device: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6981e223-493d71bc63ff01003a384538;33ee3625-22c0-4593-968a-cf372731f65e)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:403\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1655\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1652\u001b[39m ):\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:420\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    417\u001b[39m     message = (\n\u001b[32m    418\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-6981e223-493d71bc63ff01003a384538;33ee3625-22c0-4593-968a-cf372731f65e)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m use_cuda = torch.cuda.is_available()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCUDA:\u001b[39m\u001b[33m\"\u001b[39m, use_cuda, \u001b[33m\"\u001b[39m\u001b[33m| device:\u001b[39m\u001b[33m\"\u001b[39m, torch.cuda.get_device_name(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_cuda \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Intento 4-bit (bitsandbytes)\u001b[39;00m\n\u001b[32m     12\u001b[39m model = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:877\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    875\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m    876\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m877\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1017\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1015\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1019\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/configuration_utils.py:574\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/configuration_utils.py:633\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    648\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/inesagent_gpu/lib/python3.11/site-packages/transformers/utils/hub.py:421\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    420\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    422\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    427\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    429\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    430\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6981e223-493d71bc63ff01003a384538;33ee3625-22c0-4593-968a-cf372731f65e)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"CUDA:\", use_cuda, \"| device:\", torch.cuda.get_device_name(0) if use_cuda else \"CPU\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "# Intento 4-bit (bitsandbytes)\n",
    "model = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0},  # fuerza GPU:0\n",
    "        )\n",
    "        print(\"✅ Modelo cargado en 4-bit (GPU)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ No se pudo cargar 4-bit (bitsandbytes). Motivo:\", repr(e))\n",
    "        print(\"➡️ Fallback a FP16 en GPU...\")\n",
    "\n",
    "if model is None:\n",
    "    # Fallback: FP16 (GPU) o FP32 (CPU)\n",
    "    dtype = torch.float16 if use_cuda else torch.float32\n",
    "    device_map = {\"\": 0} if use_cuda else None\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    print(\"✅ Modelo cargado en\", \"FP16 GPU\" if use_cuda else \"FP32 CPU\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998ab05-316d-46b3-9513-cba83c76afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1b07b-d307-462f-8f7f-4821ce5eb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths del proyecto ---\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "\n",
    "PATH_INPUT_JSONL = ROOT / \"outputs\" / \"splits\" / \"gold_test.jsonl\"   # o prompt_regression.jsonl\n",
    "PATH_MEMORY = ROOT / \"outputs\" / \"memory\" / \"memory_selected_CURATED.json\"\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"exp4\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PATH_OUT_JSONL = OUT_DIR / \"preds.jsonl\"\n",
    "\n",
    "# --- Labels MVP ---\n",
    "MVP_LABELS = [\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "\n",
    "# --- GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ GPU:\", torch.cuda.get_device_name(0))\n",
    "    DEVICE = 0\n",
    "    DTYPE = torch.float16\n",
    "else:\n",
    "    print(\"⚠️ No GPU, usarás CPU (lento).\")\n",
    "    DEVICE = -1\n",
    "    DTYPE = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb8c98-43cb-4a09-bfc4-01728c2db83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 2: elegimos el  modelo \n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0} if DEVICE == 0 else None,\n",
    ")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    ")\n",
    "print(\"OK model:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad945f4-3ff8-44bb-91f1-cc4a41c0066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos modelo en 4-bit sin offload, forzando GPU. Esto evitará el error de dispatch CPU/disk \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},   # <- fuerza TODO a GPU:0, evita offload automático\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Modelo cargado OK en GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803993c9-caac-49c2-9fb3-0386cd939a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprobamos que el modelo quedó totalmente en GPU\n",
    "import torch\n",
    "print(\"cuda:\", torch.cuda.is_available())\n",
    "print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "print(\"mem free/total:\", torch.cuda.mem_get_info())\n",
    "print(\"model device:\", model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e59582-22ee-4fef-abfa-b991012aa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificamos que no hay offload y todo está en GPU \n",
    "# Si todo está en GPU, no debería haber parámetros en cpu\n",
    "on_cpu = 0\n",
    "total = 0\n",
    "for p in model.parameters():\n",
    "    total += 1\n",
    "    if p.device.type == \"cpu\":\n",
    "        on_cpu += 1\n",
    "print(\"params on CPU:\", on_cpu, \"/\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f3e32-be06-4d45-b9c8-edad687a9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fijamos PAD/EOS y evitamos warnings de attention_mask y padding (no utilizamos) \n",
    "# Asegurar tokens especiales para generación estable\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"eos_token_id:\", tokenizer.eos_token_id)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185317b-ba24-4a04-8b0e-be32187705fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generación con attention_mask + pad_token_id. Ejecutamos run_llm \n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_llm(system: str, user: str, max_new_tokens: int = 256) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,  # <- ya definido\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Solo devolvemos lo generado (sin el prompt)\n",
    "    gen_ids = output_ids[0, input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7dbc8-de14-48a5-a161-82bc072468fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_llm(\"Eres un asistente.\", 'Devuélveme SOLO este JSON: {\"doc_uid\":\"test\",\"spans\":[]}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eada188-bcd3-4e0f-96c2-7a0580149eba",
   "metadata": {},
   "source": [
    "**¿Por qué hacemos eso?**\n",
    "- Los modelos chat (Llama 3.1 Instruct) usan plantillas de conversación.\n",
    "- Para que generate() sea determinista y sin padding raro, conviene: definir pad_token\n",
    "- pasar attention_mask y decodificar solo la parte generada (no el prompt completo)\n",
    "\n",
    "Eso es exactamente lo que hace la versión corregida.\n",
    "\n",
    "Siguiente paso\n",
    "\n",
    "Al confirmar que el test ya saca solo JSON limpio ({\"doc_uid\":\"test\",\"spans\":[]}), pasamos a extract_first_json + Pydantic.\n",
    "Con esta nueva run_llm, el parser fallará mucho menos porque el modelo devolverá menos “basura” alrededor del JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855a2b7-e0a5-423f-aa42-185bbb954817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parseo JSON robusto \n",
    "import json\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_root_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extrae el objeto JSON raíz (completo) del output.\n",
    "    Implementa balanceo de llaves para soportar anidamiento.\n",
    "    Devuelve el primer objeto parseable que contenga doc_uid y spans.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Output no es string\")\n",
    "\n",
    "    candidates = []\n",
    "    start_positions = [i for i, ch in enumerate(text) if ch == \"{\"]\n",
    "\n",
    "    for start in start_positions:\n",
    "        depth = 0\n",
    "        for end in range(start, len(text)):\n",
    "            ch = text[end]\n",
    "            if ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    chunk = text[start:end+1]\n",
    "                    # intentamos parsear\n",
    "                    try:\n",
    "                        obj = json.loads(chunk)\n",
    "                        # preferimos el root esperado\n",
    "                        if isinstance(obj, dict) and (\"doc_uid\" in obj) and (\"spans\" in obj):\n",
    "                            return obj\n",
    "                        candidates.append(obj)\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    break\n",
    "\n",
    "    # fallback: si alguno parseó pero no tenía keys, devolvemos el primero dict parseable\n",
    "    for obj in candidates:\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "\n",
    "    raise ValueError(\"No se pudo extraer JSON raíz válido con doc_uid y spans.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50932dce-59c0-4121-9424-5de9a816eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacion de etiquetas \n",
    "from typing import List, Literal, Optional\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "MVP_LABELS = {\n",
    "    \"OBJETO\",\n",
    "    \"PRECIO_DEL_CONTRATO\",\n",
    "    \"DURACION_TOTAL_DEL_CONTRATO\",\n",
    "    \"RESOLUCION\",\n",
    "}\n",
    "\n",
    "class SpanPred(BaseModel):\n",
    "    tag: Literal[\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "    start: int = Field(ge=0)\n",
    "    end: int = Field(ge=0)\n",
    "\n",
    "    @field_validator(\"end\")\n",
    "    @classmethod\n",
    "    def end_after_start(cls, v, info):\n",
    "        start = info.data.get(\"start\")\n",
    "        if start is not None and v < start:\n",
    "            raise ValueError(\"end must be >= start\")\n",
    "        return v\n",
    "\n",
    "class PredDoc(BaseModel):\n",
    "    doc_uid: str\n",
    "    spans: List[SpanPred] = Field(default_factory=list)\n",
    "\n",
    "def validate_pred(doc_uid: str, text: str, pred_json: dict) -> PredDoc:\n",
    "    \"\"\"\n",
    "    - valida esquema\n",
    "    - valida límites start/end dentro del texto\n",
    "    - valida que substring no sea vacío/solo espacios\n",
    "    - filtra etiquetas fuera del MVP\n",
    "    \"\"\"\n",
    "    pred = PredDoc.model_validate(pred_json)\n",
    "\n",
    "    fixed_spans = []\n",
    "    for s in pred.spans:\n",
    "        if s.tag not in MVP_LABELS:\n",
    "            continue\n",
    "        if s.start < 0 or s.end > len(text):\n",
    "            continue\n",
    "        substr = text[s.start:s.end]\n",
    "        if not substr.strip():\n",
    "            continue\n",
    "        fixed_spans.append(s)\n",
    "\n",
    "    pred.spans = fixed_spans\n",
    "    # Forzamos doc_uid del contexto (por si el modelo la cambia)\n",
    "    pred.doc_uid = doc_uid\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721817ad-5bbb-46bb-88f1-d802d0cd307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "gold_val = load_jsonl(SPLITS_DIR / \"gold_val.jsonl\")\n",
    "gold_test = load_jsonl(SPLITS_DIR / \"gold_test.jsonl\")  # opcional ahora\n",
    "\n",
    "print(\"gold_val:\", len(gold_val))\n",
    "print(\"gold_test:\", len(gold_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b016b69-884a-4fdd-a4df-f974a97fa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #para evitar errores de SYSTEM not defined\n",
    "SYSTEM = \"\"\"Eres un modelo de ANOTACIÓN de texto. Tu tarea es marcar spans (fragmentos) dentro de un texto.\n",
    "\n",
    "Salida obligatoria: JSON estricto con:\n",
    "- doc_uid: string\n",
    "- spans: lista de objetos {tag, start, end}\n",
    "\n",
    "Reglas de offsets:\n",
    "- start/end son offsets de caracteres 0-based sobre el texto EXACTO.\n",
    "- end es exclusivo.\n",
    "- El substring text[start:end] debe existir exactamente.\n",
    "- No inventes texto ni offsets.\n",
    "- Puedes devolver múltiples spans.\n",
    "- Devuelve como máximo 8 spans en total.\n",
    "- Si hay más candidatos, elige los más claros y completos.\n",
    "\n",
    "Etiquetas permitidas (solo estas 4):\n",
    "OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION\n",
    "\n",
    "Devuelve SOLO JSON, sin explicaciones ni texto adicional.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e4583-018e-4865-9c35-ec2050e10cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper de prediccion seguro \n",
    "def safe_predict_one(doc_uid: str, text: str, user_prompt: str):\n",
    "    assert \"SYSTEM\" in globals(), \"SYSTEM no está definido (ejecuta la celda de SYSTEM)\"\n",
    "    raw = run_llm(SYSTEM, user_prompt, max_new_tokens=900)\n",
    "    pred_json = extract_first_json(raw)\n",
    "    pred_obj = validate_pred(doc_uid, text, pred_json)\n",
    "    return pred_obj.model_dump(), raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400948ed-70cf-43e6-9639-55345ed8fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = gold_val[0]\n",
    "raw = run_llm(\n",
    "    SYSTEM,\n",
    "    f'Devuelve SOLO este JSON: {{\"doc_uid\":\"{d0[\"doc_uid\"]}\",\"spans\":[]}}',\n",
    "    max_new_tokens=200\n",
    ")\n",
    "\n",
    "print(\"RAW LEN:\", len(raw))\n",
    "print(\"RAW REPR:\", repr(raw[:500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be82c-7584-476a-9e4c-060766a5690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_predict_one(doc_uid: str, text: str, user_prompt: str):\n",
    "    raw = run_llm(SYSTEM, user_prompt, max_new_tokens=900)\n",
    "\n",
    "    # Manejo de rechazo/no-JSON\n",
    "    if \"{\" not in raw:\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, raw\n",
    "  \n",
    "    pred_json = extract_root_json(raw)\n",
    "    pred_obj = validate_pred(doc_uid, text, pred_json)\n",
    "    return pred_obj.model_dump(), raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613ae35-67b1-4b03-8eee-a857c3e2217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_SANITY = \"Eres un asistente que devuelve exactamente el JSON solicitado. Devuelve SOLO JSON, sin texto adicional.\"\n",
    "\n",
    "d0 = gold_val[0]\n",
    "raw = run_llm(\n",
    "    SYSTEM_SANITY,\n",
    "    f'Devuelve EXACTAMENTE este JSON: {{\"doc_uid\":\"{d0[\"doc_uid\"]}\",\"spans\":[]}}',\n",
    "    max_new_tokens=60\n",
    ")\n",
    "\n",
    "print(\"RAW:\", raw)\n",
    "print(extract_first_json(raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39336c7c-df0d-4544-86c9-29700cb4feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"Eres un modelo de ANOTACIÓN de texto. Tu tarea es marcar spans (fragmentos) dentro de un texto.\n",
    "\n",
    "Salida obligatoria: JSON estricto con:\n",
    "- doc_uid: string\n",
    "- spans: lista de objetos {tag, start, end}\n",
    "\n",
    "Reglas de offsets:\n",
    "- start/end son offsets de caracteres 0-based sobre el texto EXACTO.\n",
    "- end es exclusivo.\n",
    "- El substring text[start:end] debe existir exactamente.\n",
    "- No inventes texto ni offsets.\n",
    "- Puedes devolver múltiples spans.\n",
    "- Devuelve como máximo 8 spans en total.\n",
    "- Si hay más candidatos, elige los más claros y completos.\n",
    "\n",
    "\n",
    "Etiquetas permitidas (solo estas 4):\n",
    "OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION\n",
    "\n",
    "Devuelve SOLO JSON, sin explicaciones ni texto adicional.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb793e-7485-4eb3-8315-cb79254b31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SYSTEM[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c577e-23c8-4879-a85a-967d1fed42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check rápido con un doc real (si tenemos gold_val)\n",
    "d0 = gold_val[0]\n",
    "pred0, raw0 = safe_predict_one(\n",
    "    d0[\"doc_uid\"],\n",
    "    d0[\"text\"],\n",
    "    f'Devuelve SOLO este JSON: {{\"doc_uid\":\"{d0[\"doc_uid\"]}\",\"spans\":[]}}'\n",
    ")\n",
    "pred0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b1f76-579b-466e-8198-692d01886757",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = gold_val[0]\n",
    "pred0, raw0 = safe_predict_one(\n",
    "    d0[\"doc_uid\"],\n",
    "    d0[\"text\"],\n",
    "    f'Devuelve EXACTAMENTE este JSON: {{\"doc_uid\":\"{d0[\"doc_uid\"]}\",\"spans\":[]}}'\n",
    ")\n",
    "pred0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaaae79-3876-41ee-9f7f-36ad7a1b9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dejamos safe_predict_one robusto ante no-JSON (imprescindible para batch) \n",
    "def safe_predict_one(doc_uid: str, text: str, user_prompt: str):\n",
    "    raw = run_llm(SYSTEM, user_prompt, max_new_tokens=900)\n",
    "\n",
    "    # Si no devuelve JSON, no rompemos el experimento\n",
    "    if \"{\" not in raw:\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, raw\n",
    "\n",
    "    pred_json = extract_first_json(raw)\n",
    "    pred_obj = validate_pred(doc_uid, text, pred_json)\n",
    "    return pred_obj.model_dump(), raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670abbef-71b3-4e39-9261-c61b27e38e2d",
   "metadata": {},
   "source": [
    "## Construimos memory_str y lanzamos Exp1/Exp2/Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e7bfa-eb7a-4cee-a327-1b63a42573c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#al resetear kernel quizá haya celdas que no han persistido. Nos asgeuramos re-ejecutando: \n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11852d50-919f-4e7e-97d4-68d50e6c3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_selected = load_json(PATH_MEMORY)\n",
    "print(\"memory_selected:\", len(memory_selected))\n",
    "print(\"keys:\", memory_selected[0].keys())\n",
    "print(\"example:\", memory_selected[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6928e47-9b0a-46db-b12e-7e19c368b19f",
   "metadata": {},
   "source": [
    "**_memory_selected_ tiene offsets, pero no trae el texto del span. Así que el siguiente paso es:**\n",
    "- Cargar gold (corpus anotado completo)\n",
    "- Construir un índice doc_uid -> text\n",
    "- “Adjuntar” text = gold_text[start:end] a cada ejemplo de memoria\n",
    "- Renderizar memory_str para Exp2/Exp3\n",
    "- (Opcional) crear fewshot_extra automático para Exp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df732a0-a192-4d58-a14a-255b8957e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos gold completo+indexamos por doc_uid \n",
    "# --- Cargar gold completo (para poder extraer el texto de cada span de memoria) ---\n",
    "if not PATH_GOLD.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro {PATH_GOLD}\")\n",
    "\n",
    "gold_raw = load_jsonl(PATH_GOLD) if is_jsonl(PATH_GOLD) else load_json(PATH_GOLD)\n",
    "print(\"gold_raw docs:\", len(gold_raw))\n",
    "\n",
    "# Index doc_uid -> text\n",
    "gold_text_by_uid = {}\n",
    "missing_uid = 0\n",
    "\n",
    "for d in gold_raw:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    uid = stable_uid(txt)\n",
    "    # si hay colisiones (muy raro), mantenemos el primero\n",
    "    gold_text_by_uid.setdefault(uid, txt)\n",
    "\n",
    "print(\"gold_text_by_uid:\", len(gold_text_by_uid))\n",
    "\n",
    "# Sanity: ¿todos los doc_uid de memoria existen en gold?\n",
    "mem_uids = {ex[\"doc_uid\"] for ex in memory_selected}\n",
    "missing = [u for u in mem_uids if u not in gold_text_by_uid]\n",
    "print(\"memory doc_uids:\", len(mem_uids))\n",
    "print(\"missing in gold_text_by_uid:\", len(missing))\n",
    "if missing:\n",
    "    print(\"Ejemplo missing:\", missing[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc9af0-31ae-474c-8b1d-5744c966f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjuntamos texto a memoria+validamos offsets \n",
    "def attach_span_text(memory_selected, gold_text_by_uid):\n",
    "    out = []\n",
    "    bad = 0\n",
    "\n",
    "    for ex in memory_selected:\n",
    "        uid = ex[\"doc_uid\"]\n",
    "        text = gold_text_by_uid.get(uid)\n",
    "        if text is None:\n",
    "            bad += 1\n",
    "            continue\n",
    "\n",
    "        start, end = ex[\"start\"], ex[\"end\"]\n",
    "        if not (0 <= start <= end <= len(text)):\n",
    "            bad += 1\n",
    "            continue\n",
    "\n",
    "        span_txt = text[start:end]\n",
    "        if not span_txt.strip():\n",
    "            bad += 1\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            **ex,\n",
    "            \"text\": span_txt\n",
    "        })\n",
    "\n",
    "    return out, bad\n",
    "\n",
    "memory_selected_with_text, bad_mem = attach_span_text(memory_selected, gold_text_by_uid)\n",
    "\n",
    "print(\"memory_selected_with_text:\", len(memory_selected_with_text))\n",
    "print(\"bad memory examples:\", bad_mem)\n",
    "print(\"Ejemplo con text:\", memory_selected_with_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a7a08-6547-48c2-828a-e7082d55fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderizamos memory_str (para prompts Exp2/Exp3 \n",
    "def render_memory_blocks(examples):\n",
    "    # formato compacto y estable para prompting\n",
    "    lines = []\n",
    "    for ex in examples:\n",
    "        lines.append(\n",
    "            f'- label: {ex[\"label\"]}\\n'\n",
    "            f'  criterion: {ex[\"criterion\"]}\\n'\n",
    "            f'  example: \"{ex[\"text\"]}\"'\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "memory_str = render_memory_blocks(memory_selected_with_text)\n",
    "print(\"memory_str chars:\", len(memory_str))\n",
    "print(memory_str[:600])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b780e3b-2f7f-4a5c-8458-e0ec5fca05b7",
   "metadata": {},
   "source": [
    "**(Opcional pero recomendado) — fewshot_extra automático para Exp3 (4 ejemplos, 1 por etiqueta)**\n",
    "- Esto crea 4 ejemplos extra distintos de los 16 de memoria, sacados del gold, para dar “variedad formal” en Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddf51c-07a9-4c2a-b4f1-7a88b843a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un pool de spans MVP desde gold_raw (como hiciste antes en NB03)\n",
    "def filter_tags_mvp(tags):\n",
    "    return [t for t in tags if t.get(\"tag\") in MVP_LABELS]\n",
    "\n",
    "gold_spans_pool = []\n",
    "for d in gold_raw:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    uid = stable_uid(txt)\n",
    "    tags = filter_tags_mvp(d.get(\"tags\", []))\n",
    "    for t in tags:\n",
    "        start, end = t[\"start\"], t[\"end\"]\n",
    "        if 0 <= start <= end <= len(txt):\n",
    "            stxt = txt[start:end]\n",
    "            if stxt.strip():\n",
    "                gold_spans_pool.append({\n",
    "                    \"label\": t[\"tag\"],\n",
    "                    \"criterion\": \"FEWSHOT_EXTRA (variedad formal)\",\n",
    "                    \"doc_uid\": uid,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"text\": stxt\n",
    "                })\n",
    "\n",
    "print(\"Pool spans MVP:\", len(gold_spans_pool))\n",
    "\n",
    "# Excluimos doc_uids ya usados en memoria para evitar leakage dentro del prompt\n",
    "mem_doc_uids = {ex[\"doc_uid\"] for ex in memory_selected_with_text}\n",
    "pool_no_memdocs = [x for x in gold_spans_pool if x[\"doc_uid\"] not in mem_doc_uids]\n",
    "\n",
    "# Selección 1 por etiqueta\n",
    "fewshot_extra = []\n",
    "for lab in MVP_LABELS:\n",
    "    cand = [x for x in pool_no_memdocs if x[\"label\"] == lab]\n",
    "    # elegimos el primero (o aleatorio reproducible)\n",
    "    if cand:\n",
    "        fewshot_extra.append(cand[0])\n",
    "\n",
    "print(\"fewshot_extra:\", len(fewshot_extra))\n",
    "for ex in fewshot_extra:\n",
    "    print(ex[\"label\"], ex[\"doc_uid\"][:12], ex[\"text\"][:100])\n",
    "\n",
    "fewshot_str = render_memory_blocks(fewshot_extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecceb77-736d-4243-bd79-6f807106cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajustamos los prompts Exp1/Exp2/Exp3 (ya con memory_str y fewshot_str) \n",
    "LABEL_DEFS_MIN = \"\"\"\n",
    "Etiquetas:\n",
    "- OBJETO: prestación principal / objeto del contrato, título o descripción concisa.\n",
    "- PRECIO_DEL_CONTRATO: cuantía/precio del contrato (importe total, anualidad, con o sin IVA si aplica).\n",
    "- DURACION_TOTAL_DEL_CONTRATO: periodo total de vigencia/ejecución, plazos, fechas de inicio/fin.\n",
    "- RESOLUCION: causas/condiciones de resolución/extinción del contrato.\n",
    "\"\"\"\n",
    "\n",
    "def make_user_prompt_exp1(doc_uid, text):\n",
    "    text_win = make_text_window(text)   #ventanas de texto (Acortar)\n",
    "    return f\"... TEXTO: {text_win} ...\"\n",
    "\n",
    "\n",
    "TEXTO (doc_uid={doc_uid}):\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Devuelve SOLO JSON con spans.\n",
    "\"\"\"\n",
    "\n",
    "def make_user_prompt_exp2(doc_uid: str, text: str) -> str:\n",
    "    return f\"\"\"{LABEL_DEFS_MIN}\n",
    "\n",
    "Guía (criterios + ejemplos):\n",
    "{memory_str}\n",
    "\n",
    "TEXTO (doc_uid={doc_uid}):\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Devuelve SOLO JSON con spans.\n",
    "\"\"\"\n",
    "\n",
    "def make_user_prompt_exp3(doc_uid: str, text: str) -> str:\n",
    "   return f\"\"\"{LABEL_DEFS_MIN}\n",
    "\n",
    "Guía (criterios + ejemplos):\n",
    "{memory_str}\n",
    "\n",
    "Few-shot extra:\n",
    "{fewshot_str}\n",
    "\n",
    "TEXTO (doc_uid={doc_uid}):\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Devuelve SOLO JSON con spans.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e0b2f-0780-4865-9ac8-521ac4b436b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnostico inmediato_ ver el raw que rompió \n",
    "def predict_for_docs(docs, exp_name: str, make_prompt_fn):\n",
    "    out_rows = []\n",
    "    bad = 0\n",
    "    for i, d in enumerate(docs):\n",
    "        doc_uid = d[\"doc_uid\"]\n",
    "        text = d[\"text\"]\n",
    "        user_prompt = make_prompt_fn(doc_uid, text)\n",
    "\n",
    "        try:\n",
    "            pred, raw = safe_predict_one(doc_uid, text, user_prompt)\n",
    "        except Exception as e:\n",
    "            print(\"\\n\" + \"=\"*120)\n",
    "            print(\"ERROR en\", exp_name, \"doc\", i, \"doc_uid\", doc_uid)\n",
    "            print(\"Exception:\", repr(e))\n",
    "            print(\"RAW (primeros 1200 chars):\")\n",
    "            raw_dbg = run_llm(SYSTEM, user_prompt, max_new_tokens=900)\n",
    "            print(raw_dbg[:1200])\n",
    "            print(\"=\"*120 + \"\\n\")\n",
    "            raise\n",
    "\n",
    "        if isinstance(pred, dict) and pred.get(\"_error\"):\n",
    "            bad += 1\n",
    "        out_rows.append(pred)\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"{exp_name}: {i+1}/{len(docs)} | errores: {bad}\")\n",
    "    print(exp_name, \"DONE. total:\", len(docs), \"errores:\", bad)\n",
    "    return out_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2de21-97f5-42ae-9999-06e75e4f2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos safe_predict_one aún mas tolerante (no romper batch) \n",
    "def safe_predict_one(doc_uid: str, text: str, user_prompt: str):\n",
    "    raw = run_llm(SYSTEM, user_prompt, max_new_tokens=150)\n",
    "\n",
    "    if \"{\" not in raw:\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, raw\n",
    "\n",
    "    try:\n",
    "        pred_json = extract_first_json(raw)\n",
    "    except Exception as e:\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"json_parse_error\", \"_raw\": raw, \"_exception\": repr(e)}, raw\n",
    "\n",
    "    try:\n",
    "        pred_obj = validate_pred(doc_uid, text, pred_json)\n",
    "        return pred_obj.model_dump(), raw\n",
    "    except Exception as e:\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"validation_error\", \"_raw\": raw, \"_pred_json\": pred_json, \"_exception\": repr(e)}, raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be5617-ad50-4f98-84ef-c8e153a03488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_autoclose_json(raw: str) -> str:\n",
    "    # Si empieza por \"{\" pero no termina en \"}\", intentamos cerrar de forma conservadora\n",
    "    s = raw.strip()\n",
    "    if not s.startswith(\"{\"):\n",
    "        return raw\n",
    "    # cierre simple si falta\n",
    "    if s.count(\"{\") > s.count(\"}\"):\n",
    "        s = s + (\"\\n}\" * (s.count(\"{\") - s.count(\"}\")))\n",
    "    if s.count(\"[\") > s.count(\"]\"):\n",
    "        s = s + (\"]\" * (s.count(\"[\") - s.count(\"]\")))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc9248-7b19-449f-9e78-3e0bb1e2be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = try_autoclose_json(raw)\n",
    "pred_json = extract_first_json(raw2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea47aaf-2ebf-4ab2-8350-3e1450744846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnóstico mínimo: imprime 2 raws de error (solo una vez)\n",
    "# Debug: ver qué devuelve cuando NO hay JSON\n",
    "for i in range(3):\n",
    "    d = gold_val[i]\n",
    "    doc_uid = d[\"doc_uid\"]\n",
    "    text = d[\"text\"]\n",
    "    user_prompt = make_user_prompt_exp1(doc_uid, text)\n",
    "    raw = run_llm(SYSTEM, user_prompt, max_new_tokens=400)\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"DOC\", i, \"doc_uid\", doc_uid)\n",
    "    print(\"RAW REPR:\", repr(raw[:400]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52be6c-85f8-4c03-8d5b-7fcd93603304",
   "metadata": {},
   "source": [
    "**Este output indica que sí hay formato**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cb797-fc93-466a-8699-dfd2513c6e47",
   "metadata": {},
   "source": [
    "**Arreglo fuerte: cambia Exp1 para forzar JSON (como el sanity)**\n",
    "\n",
    "El Exp1 actual probablemente es demasiado “explicativo”. Hazlo ultra-rígido y añade un ejemplo mínimo dentro del prompt (no es few-shot de contenido, solo de formato).\n",
    "\n",
    "Reemplaza completo make_user_prompt_exp1 por esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b3f46-dd83-4338-bdb6-9f4ded659446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_prompt_exp1(doc_uid: str, text: str) -> str:\n",
    "    return f\"\"\"\n",
    "TAREA:\n",
    "Anota spans del texto para estas etiquetas:\n",
    "OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION.\n",
    "\n",
    "{FORMAT_RULES}\n",
    "\n",
    "TEXTO (doc_uid={doc_uid}):\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Devuelve SOLO este JSON (rellenando spans si procede):\n",
    "{{\"doc_uid\":\"{doc_uid}\",\"spans\":[]}}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53152a-e35a-4b1d-be46-d5aae9e72c08",
   "metadata": {},
   "source": [
    "**Ajuste en run_llm: fuerza “eos” y evita respuestas largas**\n",
    "\n",
    "A veces el modelo genera “texto” si no ve el final. Podemos ayudar:\n",
    "- mantén do_sample=False\n",
    "- subimos max_new_tokens (1500), antes teníamos 600-900 y dio error\n",
    "- (opcional) añade repetition_penalty=1.05 para evitar divagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf099b3-4223-4e73-9823-df4515316bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para distinguir de dónde viene el error, probamos con 3 tipos: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd919cd-4602-4471-86be-604383e31883",
   "metadata": {},
   "source": [
    "## Ejecutamos Exp1/Exp2/Exp3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc569be-d2ca-49c3-ab53-1495ad6d4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_RULES = \"\"\"\n",
    "REGLAS OBLIGATORIAS:\n",
    "- Devuelve SOLO JSON válido.\n",
    "- Usa comillas dobles en claves y strings.\n",
    "- No devuelvas texto fuera del JSON.\n",
    "- Devuelve como máximo 12 spans en total.\n",
    "- Solo anota spans que correspondan claramente a las etiquetas.\n",
    "- No “partas” el texto en trozos consecutivos (NO segmentación artificial).\n",
    "- Si no encuentras evidencia clara para una etiqueta, no la inventes.\n",
    "- Los offsets deben corresponder exactamente al substring del texto.\n",
    "- Devuelve como máximo 8 spans en total.\n",
    "- Si hay más candidatos, elige los más claros y completos.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1ce30-1c21-4c94-b92b-2d2f1426c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejecutamos Exp1/Exp2/Exp3 sobre gold_val y guardamos \n",
    "def save_jsonl(rows, path: Path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def predict_for_docs(docs, exp_name: str, make_prompt_fn):\n",
    "    out_rows = []\n",
    "    bad = 0\n",
    "    for i, d in enumerate(docs):\n",
    "        doc_uid = d[\"doc_uid\"]\n",
    "        text = d[\"text\"]\n",
    "        user_prompt = make_prompt_fn(doc_uid, text)\n",
    "\n",
    "        pred, raw = safe_predict_one(doc_uid, text, user_prompt)\n",
    "        if isinstance(pred, dict) and pred.get(\"_error\"):\n",
    "            bad += 1\n",
    "        out_rows.append(pred)\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"{exp_name}: {i+1}/{len(docs)} | errores_no_json: {bad}\")\n",
    "    print(exp_name, \"DONE. total:\", len(docs), \"errores_no_json:\", bad)\n",
    "    return out_rows\n",
    "\n",
    "pred_exp1_val = predict_for_docs(gold_val, \"exp1_val\", make_user_prompt_exp1)\n",
    "pred_exp2_val = predict_for_docs(gold_val, \"exp2_val\", make_user_prompt_exp2)\n",
    "pred_exp3_val = predict_for_docs(gold_val, \"exp3_val\", make_user_prompt_exp3)\n",
    "\n",
    "save_jsonl(pred_exp1_val, OUT_PRED / \"exp1_val.jsonl\")\n",
    "save_jsonl(pred_exp2_val, OUT_PRED / \"exp2_val.jsonl\")\n",
    "save_jsonl(pred_exp3_val, OUT_PRED / \"exp3_val.jsonl\")\n",
    "\n",
    "print(\"Guardado en:\", OUT_PRED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335a196-9892-4bcd-b4a4-4d16267b590d",
   "metadata": {},
   "source": [
    "Esto indica algo muy concreto: casi todos los outputs están entrando por la rama “no JSON / no {” (tu contador lo llama errores_no_json). O sea: el modelo está devolviendo texto sin llaves {...} para la mayoría de documentos.\n",
    "\n",
    "Esto suele pasar cuando el prompt “real” (Exp1) está activando rechazo / respuesta conversacional, mientras que el sanity check “Devuelve EXACTAMENTE…” sí funciona.\n",
    "\n",
    "Vamos a arreglarlo con dos medidas:\n",
    "- Cambiar el prompt de Exp1 para que sea tan “cerrado” como el sanity check, y\n",
    "- Registrar el raw de los primeros errores para ver el patrón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16daacc-2ed4-4f03-8bc7-bc4635150d4d",
   "metadata": {},
   "source": [
    "Lo que te está pasando ahora es casi seguro una de estas dos cosas:\n",
    "\n",
    "1. Tu contador “no_json” está mal (está contando cualquier _error, no solo non_json_output), o\n",
    "2. En batch, para muchos docs el modelo vuelve a responder tipo “No puedo ayudarte…” (rechazo) o devuelve algo vacío, y eso sí dispara non_json_output.\n",
    "\n",
    "Vamos a salir de dudas en 2 minutos con una celda de instrumentación que:\n",
    "- cuenta errores por tipo,\n",
    "- imprime el primer raw que cae en non_json_output,\n",
    "- y guarda ese raw a disco para inspección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05eb89-8d31-460c-bc11-4d73cf8cc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instrumentación: contador real + primer ejemplo de fallo (solo debug) \n",
    "from collections import Counter\n",
    "\n",
    "def predict_for_docs_debug(docs, exp_name: str, make_prompt_fn, max_new_tokens=1200):\n",
    "    out_rows = []\n",
    "    err_counter = Counter()\n",
    "    first_nonjson = None\n",
    "    first_parse = None\n",
    "    first_val = None\n",
    "\n",
    "    for i, d in enumerate(docs):\n",
    "        doc_uid = d[\"doc_uid\"]\n",
    "        text = d[\"text\"]\n",
    "        user_prompt = make_prompt_fn(doc_uid, text)\n",
    "\n",
    "        pred, raw = safe_predict_one(doc_uid, text, user_prompt)\n",
    "\n",
    "        if isinstance(pred, dict) and pred.get(\"_error\"):\n",
    "            err_counter[pred[\"_error\"]] += 1\n",
    "\n",
    "            if pred[\"_error\"] == \"non_json_output\" and first_nonjson is None:\n",
    "                first_nonjson = (i, doc_uid, raw)\n",
    "            if pred[\"_error\"] == \"json_parse_error\" and first_parse is None:\n",
    "                first_parse = (i, doc_uid, raw)\n",
    "            if pred[\"_error\"] == \"validation_error\" and first_val is None:\n",
    "                first_val = (i, doc_uid, raw)\n",
    "\n",
    "        out_rows.append(pred)\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"{exp_name}: {i+1}/{len(docs)} | errors:\", dict(err_counter))\n",
    "\n",
    "    print(exp_name, \"DONE. errors:\", dict(err_counter))\n",
    "\n",
    "    # Imprime 1 ejemplo de cada tipo (si existe)\n",
    "    def show(label, item):\n",
    "        if item is None:\n",
    "            print(f\"\\nNo hubo {label}.\")\n",
    "            return\n",
    "        i, uid, raw = item\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(label, \"| doc idx:\", i, \"| doc_uid:\", uid)\n",
    "        print(\"RAW LEN:\", len(raw))\n",
    "        print(\"RAW (primeros 800 chars):\")\n",
    "        print(raw[:800])\n",
    "\n",
    "    show(\"FIRST non_json_output\", first_nonjson)\n",
    "    show(\"FIRST json_parse_error\", first_parse)\n",
    "    show(\"FIRST validation_error\", first_val)\n",
    "\n",
    "    return out_rows\n",
    "\n",
    "# Ejecuta SOLO exp1 con debug primero\n",
    "pred_exp1_val = predict_for_docs_debug(gold_val, \"exp1_val\", make_user_prompt_exp1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5f5b4-d94b-4170-b7b1-c85319ed60a1",
   "metadata": {},
   "source": [
    "## 'validation_error'\n",
    "El problema viene de offsets inventados (lo arreglaremos obligando a copiar el substring y/o bajando ambición en el system)\n",
    "\n",
    "validation_error porque tu extract_first_json() está extrayendo el JSON equivocado: en vez de devolver el objeto raíz {\"doc_uid\":..., \"spans\":[...]}, está devolviendo un objeto interno tipo {\"tag\":\"PRECIO_DEL_CONTRATO\",\"start\":114,\"end\":141}. Por eso Pydantic dice que falta doc_uid.\n",
    "\n",
    " 34 líneas: son 34 porque gold_val.jsonl tiene 34 documentos. Eso corresponde al split de validación: en NB02 hiciste val_n = int(0.10 * len(rest)) y tras filtrar leakage te quedaron 34 docs en validación (y otros 34 en test).\n",
    " \n",
    "Siguientes pasos:\n",
    "1. Arreglo crítico: reemplazar extract_first_json para que SIEMPRE devuelva el objeto raíz\n",
    "- Tu regex anterior re.findall(r\"\\{[\\s\\S]*?\\}\", ...) no puede manejar JSON anidado; se corta en la primera } que ve y te devuelve objetos internos.\n",
    "- Usa este extractor por balanceo de llaves (stack) y además prioriza el objeto que contiene \"doc_uid\" y \"spans\".\n",
    "\n",
    "2. Arreglo secundario: tus _raw están truncados (muchos no cierran)\n",
    "- En tu log, varios _raw acaban en ... \"end\": o se corta a mitad. Eso dará json_parse_error (no validation_error) cuando el extractor sea correcto.\n",
    "- Para reducir truncado:\n",
    "    - Limita el número de spans.\n",
    "    - Sube max_new_tokens.\n",
    "    - Recorta texto de entrada (porque estás pasando contratos largos).\n",
    "\n",
    "3. Limitar spans en el prompt\n",
    "\n",
    "Añade esto a todos tus prompts:\n",
    "- Devuelve como máximo 8 spans en total.\n",
    "- Si hay más candidatos, elige los más claros y completos.\n",
    "\n",
    "4. Recortar el texto (clave)\n",
    "- Para MVP de prompting, NO metas el contrato completo si mide miles de chars. Haz ventana con heurística:\n",
    "- Para EXP1: coge primeros 2500 chars + últimos 2500 chars (muchas resoluciones van al final).\n",
    "- O bien busca keywords para extraer fragmentos (OBJETO/Precio/Duración/Resolución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c00e0-7558-429a-af4d-1b056501235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recortamos texto para probar Exp1\n",
    "def make_text_window(text: str, head: int = 2500, tail: int = 2500) -> str:\n",
    "    if len(text) <= head + tail + 200:\n",
    "        return text\n",
    "    return text[:head] + \"\\n...\\n\" + text[-tail:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ac503-40e2-4d14-94a4-e32f607fd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmamos el arreglo con 1 doc \n",
    "d0 = gold_val[0]\n",
    "raw = run_llm(SYSTEM, make_user_prompt_exp1(d0[\"doc_uid\"], d0[\"text\"]), max_new_tokens=1500)\n",
    "pred_json = extract_root_json(raw)\n",
    "pred_json.keys(), len(pred_json.get(\"spans\", []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b510e9-8ff9-437c-9a6c-10cb0b4fb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pred(d0[\"doc_uid\"], d0[\"text\"], pred_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c51add-c00b-4444-b1bc-f19913492fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# NB4 — Carga segura del modelo\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# --------- Sanity checks ---------\n",
    "print(\"Python:\", torch.__file__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"❌ CUDA no disponible. Este NB requiere GPU.\")\n",
    "\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# --------- Modelo ---------\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# --------- Config 4-bit (estable) ---------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# --------- Tokenizer ---------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# Llama necesita padding explícito\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------- Modelo ---------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.float16,\n",
    "    device_map={\"\": 0},  # fuerza TODO a GPU\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Modelo cargado correctamente en GPU (4-bit)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f4b09-b92b-4266-9487-3d0d0ae8f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Modelo cargado:\", model.__class__.__name__)\n",
    "print(\"Dispositivo primer parámetro:\", next(model.parameters()).device)\n",
    "print(\"dtype primer parámetro:\", next(model.parameters()).dtype)\n",
    "print(\"Memoria GPU (GB):\",\n",
    "      round(torch.cuda.memory_allocated()/1024**3, 2), \"alloc |\",\n",
    "      round(torch.cuda.memory_reserved()/1024**3, 2), \"reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3276b-6f62-4951-ab8e-f96ba6c5b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checklist mínimo antes de inferencia\n",
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"bf16:\", torch.cuda.is_bf16_supported())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63c256-5aee-4587-a7c2-531c9654b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_chat(model, tokenizer, system: str, user: str,\n",
    "                  max_new_tokens: int = 512,\n",
    "                  temperature: float = 0.0,\n",
    "                  top_p: float = 0.9):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # evita warnings y edge cases\n",
    "    )\n",
    "\n",
    "    # sampling solo si temperature > 0\n",
    "    if temperature and temperature > 0:\n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ceec9-657c-4b05-8be6-2c25dca85d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta el prompt del proyecto (system + user)\n",
    "#System (rol + conducta)\n",
    "SYSTEM = \"\"\"Eres un anotador jurídico experto en contratos del sector público en España.\n",
    "Tu tarea es detectar spans exactos en el texto para estas etiquetas:\n",
    "- OBJETO\n",
    "- PRECIO_DEL_CONTRATO\n",
    "- DURACION_TOTAL_DEL_CONTRATO\n",
    "- RESOLUCION\n",
    "\n",
    "Devuelve SIEMPRE JSON válido y nada más. No inventes texto. No cambies el texto original.\n",
    "Los offsets start/end son índices de caracteres sobre el texto original (Python slicing, end exclusivo).\n",
    "Si no encuentras una evidencia clara para una etiqueta, no la incluyas en la lista spans. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635f8b6-0d59-4c73-9c06-9ff698c827a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inyectar la memoria (few-shot) desde tu JSON curado\n",
    "#Tu memoria buena ahora es outputs/memory/memory_selected_CURATED.json\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "MEM_PATH = ROOT / \"outputs\" / \"memory\" / \"memory_selected_CURATED.json\"\n",
    "GOLD_PATH = ROOT / \"gold\" / \"corpus_annotated.jsonl\"  # ajusta si usas json\n",
    "\n",
    "memory_selected = json.loads(MEM_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"Memoria ejemplos:\", len(memory_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dc6f7-9e99-4b41-987d-f1aca88ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora necesitamos convertir doc_uid+offsets a texto (para meter el ejemplo literal en el prompt). Para eso abrimos el gold y construimos un uid_to_text.\n",
    "import hashlib, json\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "uid_to_text = {}\n",
    "# si es jsonl:\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        txt = d.get(\"text\",\"\")\n",
    "        if txt:\n",
    "            uid_to_text[stable_uid(txt)] = txt\n",
    "\n",
    "def render_memory_block(memory_selected, uid_to_text, k=None):\n",
    "    items = memory_selected if k is None else memory_selected[:k]\n",
    "    blocks = []\n",
    "    for ex in items:\n",
    "        txt = uid_to_text[ex[\"doc_uid\"]]\n",
    "        span = txt[ex[\"start\"]:ex[\"end\"]].replace(\"\\n\", \" \")\n",
    "        blocks.append(\n",
    "            f\"- label: {ex['label']}\\n\"\n",
    "            f\"  criterio: {ex['criterion']}\\n\"\n",
    "            f\"  ejemplo: {span}\"\n",
    "        )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "MEMORY_BLOCK = render_memory_block(memory_selected, uid_to_text)\n",
    "print(MEMORY_BLOCK[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3d3dd-d86f-425c-90bf-d5257f0f11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User prompt final (instrucciones + memoria + texto a etiquetar + formato)\n",
    "import json\n",
    "\n",
    "JSON_SCHEMA_HINT = \"\"\"Devuelve un JSON con esta forma:\n",
    "{\n",
    "  \"spans\": [\n",
    "    {\"label\": \"...\", \"start\": 0, \"end\": 10}\n",
    "  ]\n",
    "}\n",
    "Reglas:\n",
    "- start/end deben delimitar substring NO vacío\n",
    "- end es exclusivo\n",
    "- no duplicar spans idénticos\n",
    "- si no hay spans, devuelve {\"spans\": []}\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(text: str) -> str:\n",
    "    return f\"\"\"MEMORIA (ejemplos correctos):\n",
    "{MEMORY_BLOCK}\n",
    "\n",
    "TAREA:\n",
    "Anota en el TEXTO los spans para las 4 etiquetas: OBJETO, PRECIO_DEL_CONTRATO, DURACION_TOTAL_DEL_CONTRATO, RESOLUCION.\n",
    "\n",
    "{JSON_SCHEMA_HINT}\n",
    "\n",
    "TEXTO:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d05901-e5aa-4dbc-8a33-4d8ce533b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba rápida con 1 documento\n",
    "# coge un doc cualquiera del gold o de tu split de evaluación\n",
    "sample_text = next(iter(uid_to_text.values()))\n",
    "user_prompt = build_user_prompt(sample_text)\n",
    "\n",
    "resp = generate_chat(model, tokenizer, SYSTEM, user_prompt, max_new_tokens=600, temperature=0.0)\n",
    "print(resp[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241ae3d-aae2-4417-b176-b2c2b5d4af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post-proceso mínimo para limpiar spans inválidos\n",
    "import json\n",
    "\n",
    "def clean_spans(pred: dict, text: str):\n",
    "    spans = pred.get(\"spans\", [])\n",
    "    cleaned = []\n",
    "    for s in spans:\n",
    "        lab = s.get(\"label\")\n",
    "        start = int(s.get(\"start\", -1))\n",
    "        end = int(s.get(\"end\", -1))\n",
    "\n",
    "        # descarta vacíos / fuera de rango\n",
    "        if start < 0 or end < 0 or start >= end:\n",
    "            continue\n",
    "        if end > len(text):\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\"label\": lab, \"start\": start, \"end\": end})\n",
    "    return {\"spans\": cleaned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8421fd-30fa-49bd-995d-e195bee29d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = generate_chat(model, tokenizer, SYSTEM, user_prompt, max_new_tokens=600, temperature=0.0)\n",
    "pred = json.loads(raw)\n",
    "pred = clean_spans(pred, sample_text)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e651c-70cc-473e-852a-cd1d86548e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plain(model, tokenizer, prompt: str, max_new_tokens=512, temperature=0.0, top_p=0.9):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "    if temperature and temperature > 0:\n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
    "\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def build_plain_prompt(system: str, user: str) -> str:\n",
    "    return f\"SYSTEM:\\n{system}\\n\\nUSER:\\n{user}\\n\\nASSISTANT:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c41fc-163d-483a-962e-0d2538c8e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_plain_prompt(SYSTEM, user_prompt)\n",
    "resp = generate_plain(model, tokenizer, prompt, max_new_tokens=600, temperature=0.0)\n",
    "print(resp[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecf02a-6b76-4713-869c-d7dfdfb24f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 1 — Imports + rutas + carga prompt_regression\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "ROOT = Path(\"/home/jovyan/inesagent\")\n",
    "PATH_PROMPT_REG = ROOT / \"outputs\" / \"splits\" / \"prompt_regression.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"predictions\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert PATH_PROMPT_REG.exists(), f\"No existe {PATH_PROMPT_REG}\"\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "prompt_reg_docs = load_jsonl(PATH_PROMPT_REG)\n",
    "print(\"prompt_regression docs:\", len(prompt_reg_docs))\n",
    "print(\"keys ejemplo:\", list(prompt_reg_docs[0].keys()))\n",
    "print(\"len(text) ejemplo:\", len(prompt_reg_docs[0][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ceef29-4356-427e-88fa-6f607d548440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 2 — Carga memoria final + bloqueo anti-leakage\n",
    "PATH_MEMORY = ROOT / \"outputs\" / \"memory\" / \"memory_selected_FINAL.json\"\n",
    "PATH_BLOCKED = ROOT / \"outputs\" / \"memory\" / \"blocked_doc_uids_by_memory.json\"\n",
    "\n",
    "memory = json.loads(PATH_MEMORY.read_text(encoding=\"utf-8\"))\n",
    "blocked_uids = set(json.loads(PATH_BLOCKED.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "print(\"memory examples:\", len(memory))\n",
    "print(\"blocked uids:\", len(blocked_uids))\n",
    "print(\"memory[0] keys:\", list(memory[0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0fb06-136c-4b32-b74f-31a31f74c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 2.1 (antes de pydantic) Haz el parseo “tolerante” antes de Pydantic (convierte spans vacíos → se eliminan)\n",
    "#Antes hacíamos Pred.model_validate_json(raw) directamente → si el modelo devuelve basura, muere.\n",
    "#Ahora: mostramos el RAW siempre (para depurar),parseamos JSON,limpiamos spans inválidos (0,0, quote vacío),y Pydantic valida lo que queda\n",
    "def _extract_json_object(s: str) -> str:\n",
    "    # coge el último {...} del final\n",
    "    m = re.search(r\"\\{.*\\}\\s*$\", s, flags=re.DOTALL)\n",
    "    return m.group(0) if m else s\n",
    "\n",
    "def _sanitize_pred_dict(obj: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Normaliza casos típicos de “plantilla vacía”\n",
    "    spans = obj.get(\"spans\", [])\n",
    "    if not isinstance(spans, list):\n",
    "        return {\"spans\": []}\n",
    "\n",
    "    cleaned = []\n",
    "    for sp in spans:\n",
    "        if not isinstance(sp, dict):\n",
    "            continue\n",
    "        start = sp.get(\"start\", None)\n",
    "        end = sp.get(\"end\", None)\n",
    "        quote = sp.get(\"quote\", None)\n",
    "        label = sp.get(\"label\", None)\n",
    "\n",
    "        # Descarta spans vacíos / mal formados\n",
    "        if label not in LABELS:\n",
    "            continue\n",
    "        if not isinstance(start, int) or not isinstance(end, int):\n",
    "            continue\n",
    "        if end <= start:\n",
    "            continue\n",
    "        if not isinstance(quote, str) or not quote.strip():\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\"label\": label, \"start\": start, \"end\": end, \"quote\": quote})\n",
    "\n",
    "    return {\"spans\": cleaned}\n",
    "\n",
    "def run_one_exp(text: str, exp: int):\n",
    "    if exp == 1:\n",
    "        user = build_user_exp1(text)\n",
    "    elif exp == 2:\n",
    "        user = build_user_exp2(text, memory_block)\n",
    "    elif exp == 3:\n",
    "        user = build_user_exp3(text, memory_block, fewshot_extra)\n",
    "    else:\n",
    "        raise ValueError(\"exp debe ser 1,2,3\")\n",
    "\n",
    "    raw = generate_chat(model, tokenizer, SYSTEM_BASE, user, max_new_tokens=600, temperature=0.0)\n",
    "    raw_json = _extract_json_object(raw)\n",
    "\n",
    "    print(\"RAW (primeros 1200 chars):\\n\", raw[:1200], \"\\n\")\n",
    "\n",
    "    # 1) parse JSON “a mano”\n",
    "    obj = json.loads(raw_json)\n",
    "    obj = _sanitize_pred_dict(obj)\n",
    "\n",
    "    # 2) valida con Pydantic\n",
    "    pred = Pred.model_validate(obj)\n",
    "    pred2 = strict_verify(pred, text)\n",
    "\n",
    "    print(\"Spans validados:\", len(pred2.spans))\n",
    "    for s in pred2.spans:\n",
    "        print(\"\\n\", s.label, s.start, s.end)\n",
    "        print(pretty_span(text, s.start, s.end, window=80))\n",
    "\n",
    "    return pred2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af81da-2ac4-4593-bbf2-15408ee184a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 3 — Pydantic: JSON estricto + offsets coherentes\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "from typing import Literal\n",
    "\n",
    "Label = Literal[\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "\n",
    "class Span(BaseModel):\n",
    "    label: Label\n",
    "    start: int = Field(ge=0)\n",
    "    end: int = Field(ge=0)\n",
    "    quote: str = Field(min_length=1)  # substring literal para verificación\n",
    "\n",
    "    @field_validator(\"end\")\n",
    "    @classmethod\n",
    "    def end_after_start(cls, v, info):\n",
    "        start = info.data.get(\"start\")\n",
    "        if start is not None and v <= start:\n",
    "            raise ValueError(\"end must be > start\")\n",
    "        return v\n",
    "\n",
    "class Pred(BaseModel):\n",
    "    spans: List[Span]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739863d-1549-4e14-a80e-9135ed8b6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 4 — Helpers: normalización + verificación de offsets contra quote\n",
    "def strict_verify(pred: Pred, text: str) -> Pred:\n",
    "    ok_spans = []\n",
    "    for s in pred.spans:\n",
    "        if s.end > len(text):\n",
    "            continue\n",
    "        extracted = text[s.start:s.end]\n",
    "        if extracted != s.quote:\n",
    "            # Si no coincide EXACTO, descartamos (o lanza error si quieres)\n",
    "            continue\n",
    "        ok_spans.append(s)\n",
    "    return Pred(spans=ok_spans)\n",
    "\n",
    "def pretty_span(text: str, start: int, end: int, window: int = 80) -> str:\n",
    "    a = max(0, start - window)\n",
    "    b = min(len(text), end + window)\n",
    "    ctx = text[a:b].replace(\"\\n\",\" \")\n",
    "    mark = text[start:end].replace(\"\\n\",\" \")\n",
    "    return f\"...{ctx}...\\n>>> [{start}:{end}] {mark}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecdfc7-986d-4496-a913-354d93e2ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 5 — Construcción de prompts Exp1/Exp2/Exp3\n",
    "#Cambio crítico: pedimos quote además de start/end, y exigimos:\n",
    "#quote debe ser exactamente text[start:end]\n",
    "#start/end deben ser offsets del texto original que se da\n",
    "LABELS = [\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "\n",
    "SYSTEM_BASE = \"\"\"Eres un anotador jurídico experto en contratos del sector público en España.\n",
    "Tu tarea es identificar fragmentos textuales (segmentos/oraciones/cláusulas) que cumplen una función jurídica y devolver sus offsets start/end con una etiqueta.\n",
    "Devuelves SOLO JSON válido.\n",
    "Reglas:\n",
    "- NO escribas explicaciones ni código.\n",
    "Salida EXACTA: {\"spans\":[...]}.\n",
    "- Si no encuentras ninguna etiqueta, devuelve {\"spans\": []}.\n",
    "- PROHIBIDO devolver spans vacíos: nunca uses start=0,end=0 ni quote=\"\". Si no hay evidencia, NO incluyas ese span.\n",
    "- Cada span debe incluir: label, start, end, quote.\n",
    "- quote debe ser EXACTAMENTE el substring literal del texto: quote == text[start:end].\n",
    "- start/end son offsets 0-indexados sobre el texto ORIGINAL.\n",
    "\"\"\"\n",
    "\n",
    "def build_user_exp1(text: str) -> str:\n",
    "    return f\"\"\"Tarea: detecta spans para estas etiquetas: {\", \".join(LABELS)}.\n",
    "\n",
    "Devuelve JSON:\n",
    "{{\n",
    "  \"spans\": [{{\"label\":\"...\", \"start\":0, \"end\":0, \"quote\":\"...\"}}]\n",
    "}}\n",
    "\n",
    "TEXTO:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def memory_to_prompt(memory: List[Dict[str,Any]], uid_to_text: Dict[str,str], max_per_label: Optional[int]=None) -> str:\n",
    "    # convierte memory_selected_FINAL en mini-few-shot con quote real\n",
    "    by_label = {l: [] for l in LABELS}\n",
    "    for ex in memory:\n",
    "        by_label[ex[\"label\"]].append(ex)\n",
    "\n",
    "    parts = []\n",
    "    for lab in LABELS:\n",
    "        exs = by_label.get(lab, [])\n",
    "        if max_per_label:\n",
    "            exs = exs[:max_per_label]\n",
    "        for ex in exs:\n",
    "            txt = uid_to_text.get(ex[\"doc_uid\"])\n",
    "            if not txt:\n",
    "                continue\n",
    "            quote = txt[ex[\"start\"]:ex[\"end\"]].replace(\"\\n\",\" \")\n",
    "            parts.append(\n",
    "                f\"\"\"Ejemplo ({lab})\n",
    "Criterio: {ex[\"criterion\"]}\n",
    "Texto: \\\"\\\"\\\"{txt}\\\"\\\"\\\"\n",
    "Respuesta: {{\"spans\":[{{\"label\":\"{lab}\",\"start\":{ex[\"start\"]},\"end\":{ex[\"end\"]},\"quote\":\"{quote}\"}}]}}\n",
    "\"\"\"\n",
    "            )\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def build_user_exp2(text: str, memory_block: str) -> str:\n",
    "    return f\"\"\"Tarea: detecta spans para estas etiquetas: {\", \".join(LABELS)}.\n",
    "\n",
    "Aquí tienes ejemplos (memoria) con criterios y offsets correctos:\n",
    "{memory_block}\n",
    "\n",
    "Ahora anota el siguiente texto.\n",
    "\n",
    "Devuelve JSON:\n",
    "{{\n",
    "  \"spans\": [{{\"label\":\"...\", \"start\":0, \"end\":0, \"quote\":\"...\"}}]\n",
    "}}\n",
    "\n",
    "TEXTO:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def build_user_exp3(text: str, memory_block: str, fewshot_extra: str) -> str:\n",
    "    return f\"\"\"Tarea: detecta spans para estas etiquetas: {\", \".join(LABELS)}.\n",
    "\n",
    "Memoria (curada):\n",
    "{memory_block}\n",
    "\n",
    "Few-shot adicional:\n",
    "{fewshot_extra}\n",
    "\n",
    "Devuelve JSON:\n",
    "{{\n",
    "  \"spans\": [{{\"label\":\"...\", \"start\":0, \"end\":0, \"quote\":\"...\"}}]\n",
    "}}\n",
    "\n",
    "TEXTO:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585305f-65cf-45ce-8fc8-dcad1218d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 6 — Preparar uid_to_text para construir memoria prompt\n",
    "import hashlib\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "uid_to_text = {}\n",
    "for d in prompt_reg_docs:\n",
    "    txt = d.get(\"text\",\"\")\n",
    "    if txt:\n",
    "        uid_to_text[stable_uid(txt)] = txt\n",
    "\n",
    "# OJO: tu memory_selected_FINAL.json usa doc_uid de GOLD, no de prompt_regression.\n",
    "# Para poder construir quotes, necesitamos cargar el GOLD también (mínimo texto por uid).\n",
    "PATH_GOLD_JSON  = ROOT / \"gold\" / \"corpus_annotated.json\"\n",
    "PATH_GOLD_JSONL = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "\n",
    "def load_gold():\n",
    "    if PATH_GOLD_JSON.exists():\n",
    "        return json.loads(PATH_GOLD_JSON.read_text(encoding=\"utf-8\"))\n",
    "    if PATH_GOLD_JSONL.exists():\n",
    "        return load_jsonl(PATH_GOLD_JSONL)\n",
    "    raise FileNotFoundError(\"No encuentro corpus_annotated.json ni .jsonl\")\n",
    "\n",
    "gold = load_gold()\n",
    "gold_uid_to_text = {}\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\",\"\")\n",
    "    if txt:\n",
    "        gold_uid_to_text[stable_uid(txt)] = txt\n",
    "\n",
    "print(\"gold docs:\", len(gold_uid_to_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c694132-4314-4100-92e1-b7a7784aab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 7 — Construir bloque de memoria para Exp2/Exp3\n",
    "memory_block = memory_to_prompt(memory, gold_uid_to_text, max_per_label=None)\n",
    "print(\"Memory block chars:\", len(memory_block))\n",
    "print(memory_block[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142328e-80ee-4d69-be24-37e895ae53e1",
   "metadata": {},
   "source": [
    "## Para Few-shot extra para Exp3 (automático, sin leakage)\n",
    "\n",
    "Idea: coger docs de prompt_regression que NO estén bloqueados y crear 1 ejemplo artificial por etiqueta usando spans GOLD…\n",
    "Pero NB04 está corriendo sobre prompt_regression, que NO tiene tags. Para hacerlo “sin selección manual y sin leakage”, lo correcto es:\n",
    "- construir few-shot extra desde GOLD (porque ahí sí hay tags),\n",
    "- eligiendo docs no bloqueados, y\n",
    "- tomando 1 span por etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14f7c4-15ce-4e57-a771-b6c1cbf0a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 8 — Few-shot extra para Exp3 (automático, sin leakage)\n",
    "def build_fewshot_extra_from_gold(gold: List[Dict[str,Any]], blocked_uids: set, k_per_label: int = 1):\n",
    "    MVP = set(LABELS)\n",
    "    picks = {lab: [] for lab in MVP}\n",
    "\n",
    "    for d in gold:\n",
    "        txt = d.get(\"text\",\"\")\n",
    "        if not txt:\n",
    "            continue\n",
    "        uid = stable_uid(txt)\n",
    "        if uid in blocked_uids:\n",
    "            continue\n",
    "\n",
    "        for t in d.get(\"tags\", []):\n",
    "            lab = t.get(\"tag\")\n",
    "            if lab not in MVP:\n",
    "                continue\n",
    "            if len(picks[lab]) >= k_per_label:\n",
    "                continue\n",
    "            s = int(t[\"start\"]); e = int(t[\"end\"])\n",
    "            if 0 <= s < e <= len(txt):\n",
    "                quote = txt[s:e].replace(\"\\n\",\" \")\n",
    "                picks[lab].append((txt, lab, s, e, quote))\n",
    "\n",
    "        if all(len(picks[lab]) >= k_per_label for lab in MVP):\n",
    "            break\n",
    "\n",
    "    # Render en prompt\n",
    "    blocks = []\n",
    "    for lab in LABELS:\n",
    "        for (txt, lab, s, e, quote) in picks[lab]:\n",
    "            blocks.append(\n",
    "                f\"\"\"Texto: \\\"\\\"\\\"{txt}\\\"\\\"\\\"\n",
    "Respuesta: {{\"spans\":[{{\"label\":\"{lab}\",\"start\":{s},\"end\":{e},\"quote\":\"{quote}\"}}]}}\n",
    "\"\"\"\n",
    "            )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "fewshot_extra = build_fewshot_extra_from_gold(gold, blocked_uids, k_per_label=1)\n",
    "print(\"Few-shot extra chars:\", len(fewshot_extra))\n",
    "print(fewshot_extra[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7d9eb-881e-4423-bd06-5455fa38e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 9 — generate_chat() (la versión sin warnings)\n",
    "import torch\n",
    "\n",
    "def generate_chat(model, tokenizer, system: str, user: str,\n",
    "                  max_new_tokens: int = 512,\n",
    "                  temperature: float = 0.0,\n",
    "                  top_p: float = 0.9):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    if temperature and temperature > 0:\n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3058a7f-577e-4ce9-82a2-bb3030f18150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 10 — Ejecutar 1 doc (supervisión real)\n",
    "def run_one_exp(text: str, exp: int):\n",
    "    if exp == 1:\n",
    "        user = build_user_exp1(text)\n",
    "    elif exp == 2:\n",
    "        user = build_user_exp2(text, memory_block)\n",
    "    elif exp == 3:\n",
    "        user = build_user_exp3(text, memory_block, fewshot_extra)\n",
    "    else:\n",
    "        raise ValueError(\"exp debe ser 1,2,3\")\n",
    "\n",
    "    raw = generate_chat(model, tokenizer, SYSTEM_BASE, user, max_new_tokens=600, temperature=0.0)\n",
    "    # intenta extraer JSON bruto (por si añade basura)\n",
    "    m = re.search(r\"\\{.*\\}\\s*$\", raw, flags=re.DOTALL)\n",
    "    if m:\n",
    "        raw = m.group(0)\n",
    "\n",
    "    pred = Pred.model_validate_json(raw)\n",
    "    pred2 = strict_verify(pred, text)  # descarta spans cuyo quote no coincide\n",
    "\n",
    "    print(\"RAW JSON:\", raw[:800], \"...\\n\")\n",
    "    print(\"Spans validados:\", len(pred2.spans))\n",
    "    for s in pred2.spans:\n",
    "        print(\"\\n\", s.label, s.start, s.end)\n",
    "        print(pretty_span(text, s.start, s.end, window=80))\n",
    "    return pred2\n",
    "\n",
    "sample = prompt_reg_docs[0][\"text\"]\n",
    "_ = run_one_exp(sample, exp=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dc1f5-dfa2-485c-ace0-7ed8bf69243f",
   "metadata": {},
   "source": [
    "## Importante: Exp1 te va a dar muchísimos “spans: []” (es normal)\n",
    "\n",
    "Exp1 (solo nombres de etiquetas) con un Llama 3.1 8B sin criterios suele:\n",
    "- o inventar offsets,\n",
    "- o devolverte vacío,\n",
    "- o devolverte spans enormes.\n",
    "\n",
    "Así que para depurar offsets “serios”, empieza por Exp2 (memoria+criterios), y luego Exp3.\n",
    "\n",
    "`if exp == 1: user = build_user_exp1(text)`\n",
    "      \n",
    "`elif exp == 2: user = build_user_exp2(text, memory_block)`\n",
    "       \n",
    "`elif exp == 3: user = build_user_exp3(text, memory_block, fewshot_extra)`\n",
    "\n",
    "- Exp1 suele ser flojo: etiquetas sin criterios → más alucinación.\n",
    "- Exp2 debe mejorar: criterios + memoria.\n",
    "- Exp3 añade estabilidad: few-shot extra desde GOLD no bloqueado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62145c9-fb5b-4ce6-aa8d-702839acda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probamos\n",
    "sample = prompt_reg_docs[0][\"text\"]\n",
    "_ = run_one_exp(sample, exp=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f883c85-def0-4ab8-8239-f6cf1bebcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 11 — Loop por documentos y guardado JSONL exp1/exp2/exp3. guardamos 3 ficheros jsonl en outputs/predictions\n",
    "#Cada línea: {doc_uid, id(opcional), text_len, pred_spans:[...]}\n",
    "def doc_uid(text: str) -> str:\n",
    "    return stable_uid(text)\n",
    "\n",
    "def predict_doc(d: Dict[str,Any], exp: int) -> Dict[str,Any]:\n",
    "    text = d[\"text\"]\n",
    "    uid = doc_uid(text)\n",
    "\n",
    "    if uid in blocked_uids:\n",
    "        # NB04: para prompt_regression normalmente NO necesitas bloquear,\n",
    "        # pero si quieres evitar ver textos “vistos” en memoria, lo dejamos:\n",
    "        return {\"doc_uid\": uid, \"skipped\": True, \"reason\": \"blocked_uid\"}\n",
    "\n",
    "    if exp == 1:\n",
    "        user = build_user_exp1(text)\n",
    "    elif exp == 2:\n",
    "        user = build_user_exp2(text, memory_block)\n",
    "    elif exp == 3:\n",
    "        user = build_user_exp3(text, memory_block, fewshot_extra)\n",
    "    else:\n",
    "        raise ValueError(\"exp debe ser 1,2,3\")\n",
    "\n",
    "    raw = generate_chat(model, tokenizer, SYSTEM_BASE, user, max_new_tokens=600, temperature=0.0)\n",
    "\n",
    "    # extraer JSON si el modelo mete texto extra\n",
    "    m = re.search(r\"\\{.*\\}\\s*$\", raw, flags=re.DOTALL)\n",
    "    if m:\n",
    "        raw = m.group(0)\n",
    "\n",
    "    pred = Pred.model_validate_json(raw)\n",
    "    pred = strict_verify(pred, text)\n",
    "\n",
    "    return {\n",
    "        \"doc_uid\": uid,\n",
    "        \"id\": d.get(\"id\"),\n",
    "        \"text_len\": len(text),\n",
    "        \"spans\": [s.model_dump() for s in pred.spans],\n",
    "    }\n",
    "\n",
    "def run_experiment(docs: List[Dict[str,Any]], exp: int, out_path: Path, limit: Optional[int]=None):\n",
    "    n = 0\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in docs[: (limit or len(docs))]:\n",
    "            try:\n",
    "                rec = predict_doc(d, exp)\n",
    "            except ValidationError as e:\n",
    "                rec = {\"doc_uid\": doc_uid(d[\"text\"]), \"error\": \"pydantic_validation\", \"detail\": str(e)}\n",
    "            except Exception as e:\n",
    "                rec = {\"doc_uid\": doc_uid(d[\"text\"]), \"error\": \"runtime\", \"detail\": str(e)}\n",
    "\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n += 1\n",
    "\n",
    "    print(f\"Exp{exp} guardado:\", out_path, \"| docs:\", n)\n",
    "\n",
    "OUT1 = OUT_DIR / \"exp1.jsonl\"\n",
    "OUT2 = OUT_DIR / \"exp2.jsonl\"\n",
    "OUT3 = OUT_DIR / \"exp3.jsonl\"\n",
    "\n",
    "# primero prueba con 5 para depurar\n",
    "run_experiment(prompt_reg_docs, exp=1, out_path=OUT1, limit=5)\n",
    "run_experiment(prompt_reg_docs, exp=2, out_path=OUT2, limit=5)\n",
    "run_experiment(prompt_reg_docs, exp=3, out_path=OUT3, limit=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7109076-4fe9-4990-9ebd-ec5636f8965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda 12 — “Supervisor” rápido: inspeccionar predicciones guardadas\n",
    "def read_jsonl(path: Path, n: int = 3):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows = read_jsonl(OUT2, n=3)\n",
    "for r in rows:\n",
    "    print(\"\\nDOC\", r.get(\"doc_uid\"), \"spans:\", len(r.get(\"spans\",[])), \"skipped:\", r.get(\"skipped\", False), \"error:\", r.get(\"error\"))\n",
    "    for s in r.get(\"spans\", [])[:4]:\n",
    "        print(\" \", s[\"label\"], s[\"start\"], s[\"end\"], \"| quote[:60]=\", s[\"quote\"][:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51bb71-283d-405b-a4d0-2e74bb211067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
