{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d570ed8",
   "metadata": {},
   "source": [
    "# NB04 — Experimentos 1/2/3 (prompting) con salida JSON estricta\n",
    "\n",
    "Este notebook está “limpio” y organizado para:\n",
    "\n",
    "- cargar datos (gold + memoria) sin variables fantasma\n",
    "- definir prompts Exp1/Exp2/Exp3\n",
    "- generar predicciones con un modelo local (Transformers) en GPU\n",
    "- validar salida con **Pydantic v2** + verificación estricta de offsets/quote\n",
    "- guardar resultados en JSONL\n",
    "\n",
    "> Nota: **No** hardcodees tokens de Hugging Face aquí. Si el modelo es gated, haz login en terminal con `huggingface-cli login`.\n",
    ">\n",
    "> Para que no se me pierda el entorno, se puede guardar como fichero \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f48d2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /home/jovyan/.conda/envs/inesagent_gpu/bin/python\n",
      "PIP_USER (si existe): yes\n",
      "ENABLE_USER_SITE: True\n",
      "USER_SITE: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n"
     ]
    }
   ],
   "source": [
    "# 0) Sanity del entorno (debe ser inesagent_gpu)\n",
    "import os, sys, site\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "assert \"/home/jovyan/.conda/envs/inesagent_gpu/bin/python\" in sys.executable, \"❌ No estás en el kernel inesagent_gpu\"\n",
    "\n",
    "print(\"PIP_USER (si existe):\", os.environ.get(\"PIP_USER\"))\n",
    "print(\"ENABLE_USER_SITE:\", site.ENABLE_USER_SITE)\n",
    "print(\"USER_SITE:\", site.getusersitepackages())\n",
    "\n",
    "# GPU\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Si PIP_USER='yes', pip intentará instalar con --user y fallará (por el blindaje).\n",
    "# Para instalar paquetes desde notebook, usaremos siempre:  env -u PIP_USER  + sys.executable -m pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10322776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: []\n",
      "✅ Todo instalado.\n"
     ]
    }
   ],
   "source": [
    "# 1) Instalación opcional (solo si falta algo)\n",
    "#    Ejecuta esta celda SOLO si un import falla.\n",
    "import sys, importlib.util, subprocess, textwrap, os\n",
    "\n",
    "REQUIRED = [\n",
    "    \"pydantic>=2\",\n",
    "    \"jsonschema\",\n",
    "    \"transformers>=4.45,<4.47\",\n",
    "    \"accelerate>=0.34,<2.0\",\n",
    "    \"huggingface_hub>=0.30,<1.0\",\n",
    "    \"safetensors>=0.4\",\n",
    "    \"sentencepiece\",\n",
    "    # cuantización 4-bit (si tu runtime lo soporta)\n",
    "    \"bitsandbytes>=0.43\",\n",
    "    # utilidades\n",
    "    \"tqdm\",\n",
    "]\n",
    "\n",
    "def missing_pkgs(reqs):\n",
    "    missing = []\n",
    "    for r in reqs:\n",
    "        name = r.split(\"==\")[0].split(\">=\")[0].split(\"<\")[0].strip()\n",
    "        if importlib.util.find_spec(name) is None:\n",
    "            missing.append(r)\n",
    "    return missing\n",
    "\n",
    "miss = missing_pkgs(REQUIRED)\n",
    "print(\"Missing:\", miss)\n",
    "\n",
    "if miss:\n",
    "    cmd = f'env -u PIP_USER \"{sys.executable}\" -m pip install -U --no-cache-dir ' + \" \".join([repr(x) for x in miss])\n",
    "    print(\"Running:\", cmd)\n",
    "    # Ejecutamos como shell para poder usar env -u PIP_USER\n",
    "    r = subprocess.run(cmd, shell=True, text=True)\n",
    "    if r.returncode != 0:\n",
    "        raise RuntimeError(\"❌ pip install falló. Revisa el log arriba.\")\n",
    "    print(\"✅ Instalación terminada. Reinicia Kernel si actualizaste libs base (transformers/torch).\")\n",
    "else:\n",
    "    print(\"✅ Todo instalado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00bca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Imports (una sola vez)\n",
    "from pathlib import Path\n",
    "import json, re, hashlib, random\n",
    "from typing import List, Dict, Any, Optional, Literal, Tuple\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41d1f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/jovyan/inesagent\n",
      "val_fixed keys: dict_keys(['id', 'text', 'tags', 'legacy_doc_uid'])\n",
      "/home/jovyan/inesagent/gold/corpus_annotated.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/memory/memory_selected_FINAL.json -> True\n",
      "/home/jovyan/inesagent/outputs/splits/val_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/test_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/prompt_regression_gold_FIXED.jsonl -> True\n",
      "/home/jovyan/inesagent/outputs/splits/train_gold_FIXED.jsonl -> True (train opcional)\n"
     ]
    }
   ],
   "source": [
    "# 3) Paths + utilidades de carga\n",
    "#Utilidades\n",
    "from pathlib import Path\n",
    "import json, hashlib\n",
    "\n",
    "def is_jsonl(p: Path) -> bool:\n",
    "    return p.suffix.lower() == \".jsonl\"\n",
    "\n",
    "def load_json(p: Path):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(p: Path):\n",
    "    rows = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def stable_uid(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "#Paths\n",
    "\n",
    "ROOT = Path.home() / \"inesagent\"\n",
    "assert ROOT.exists(), f\"ROOT no existe: {ROOT}\"\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "PATH_GOLD    = ROOT / \"gold\" / \"corpus_annotated.jsonl\"\n",
    "PATH_MEMORY  = ROOT / \"outputs\" / \"memory\" / \"memory_selected_FINAL.json\"\n",
    "PATH_BLOCKED = ROOT / \"outputs\" / \"memory\" / \"blocked_ids_by_memory.json\"\n",
    "\n",
    "SPLITS_DIR = ROOT / \"outputs\" / \"splits\"\n",
    "PATH_VAL   = SPLITS_DIR / \"val_gold_FIXED.jsonl\"\n",
    "PATH_TEST  = SPLITS_DIR / \"test_gold_FIXED.jsonl\"\n",
    "PATH_PR    = SPLITS_DIR / \"prompt_regression_gold_FIXED.jsonl\"\n",
    "PATH_TRAIN = SPLITS_DIR / \"train_gold_FIXED.jsonl\" # opcional\n",
    "\n",
    "val_fixed   = load_jsonl(SPLITS_DIR / \"val_gold_FIXED.jsonl\")\n",
    "test_fixed  = load_jsonl(SPLITS_DIR / \"test_gold_FIXED.jsonl\")\n",
    "train_fixed = load_jsonl(SPLITS_DIR / \"train_gold_FIXED.jsonl\")\n",
    "pr_fixed    = load_jsonl(SPLITS_DIR / \"prompt_regression_gold_FIXED.jsonl\")\n",
    "\n",
    "print(\"val_fixed keys:\", val_fixed[0].keys())\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\" / \"predictions\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "MVP_LABELS = [\"OBJETO\", \"PRECIO_DEL_CONTRATO\", \"DURACION_TOTAL_DEL_CONTRATO\", \"RESOLUCION\"]\n",
    "\n",
    "\n",
    "\n",
    "for p in [PATH_GOLD, PATH_MEMORY, PATH_VAL, PATH_TEST, PATH_PR]:\n",
    "    print(p, \"->\", p.exists())\n",
    "print(PATH_TRAIN, \"->\", PATH_TRAIN.exists(), \"(train opcional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5082790-92b7-46fd-bd36-c8d33cad29b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 279\n",
      "val: 34\n",
      "test: 34\n",
      "prompt_reg: 10\n"
     ]
    }
   ],
   "source": [
    "# 3.1) load_jsonl y load splits, comprobamos tamaños\n",
    "print(\"train:\", len(train_gold))\n",
    "print(\"val:\", len(val_gold))\n",
    "print(\"test:\", len(test_gold))\n",
    "print(\"prompt_reg:\", len(prompt_regression_gold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb61f98-332c-48ba-84cc-6ca5e04fd988",
   "metadata": {},
   "source": [
    "## Tamaño de muestras\n",
    "- train: 279 → pool principal para construir memoria / ejemplos, o para ajuste posterior.\n",
    "- val: 34 →  tamaño típico para iterar prompts y medir estabilidad sin gastar demasiado (split del corpus).\n",
    "- test: 34 → mismo tamaño que val (equilibrado) (split del corpus).\n",
    "- prompt_reg: 10 → conjunto pequeño para detectar regresiones de prompt (ideal que sea pequeño y fijo).\n",
    "\n",
    "**(!)** Hacemos chequeo para comprobar y asegurarnos de que no hay solapamiento raro como:\n",
    "- los ids no se repiten entre splits\n",
    "- y que val/test no están dentro de blocked_ids si estás usando bloqueo por leakage.\n",
    "\n",
    "**Respuesta esperada:**\n",
    "`overlap val∩test: 0\n",
    "overlap val∩train: 0\n",
    "overlap test∩train: 0\n",
    "overlap pr∩train: 0\n",
    "overlap pr∩val: 0\n",
    "overlap pr∩test: 0 (o 10)`\n",
    "\n",
    "Que `prompt_reg ∩ test = 10` significa que ✅ prompt_reg es exactamente un subconjunto de test > que sea subconjunto de test no es un error, solo implica: No debes evaluar “test” completo y prompt_reg como si fueran dos métricas independientes, porque estarías duplicando información.\n",
    "\n",
    "**Opción A (recomendada): deja prompt_reg como subconjunto de test**, pero úsalo bien:\n",
    "- `prompt_reg`: lo usas para iterar prompts (rápido).\n",
    "- `val`: lo usas para elegir el mejor prompt.\n",
    "- `test`: lo usas solo al final, una vez, con el prompt congelado.\n",
    "\n",
    "Así no hay leakage ni trampa. Opción A no tocar splits, solo metodología (prompt_reg ⊂ test). Más útil para MVP e iteración rápida\n",
    "\n",
    "**Opción B: hacer prompt_reg independiente (si quieres “pureza”)**\n",
    "- La forma más sencilla es reconstruir `prompt_reg desde train` (o desde `val`) y garantizar no solapar.\n",
    "- Como ya tienes los archivos, puedes generar un nuevo `prompt_regression_gold.jsonl` (10 docs) desde train en una celda.\n",
    "\n",
    "Opción B es más \"paper-like\" para evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7552986d-72fb-4e7a-988e-2fc47bdb89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap val∩test: 0\n",
      "overlap val∩train: 0\n",
      "overlap test∩train: 0\n",
      "overlap pr∩train: 0\n",
      "overlap pr∩val: 0\n",
      "overlap pr∩test: 0\n"
     ]
    }
   ],
   "source": [
    "# 3.2) Chequeo sobre val/test vs train\n",
    "def ids(docs): \n",
    "    return {d[\"id\"] for d in docs}\n",
    "\n",
    "ids_val = ids(val_fixed)\n",
    "ids_test = ids(test_fixed)\n",
    "ids_train = ids(train_fixed)\n",
    "ids_pr = ids(pr_fixed)\n",
    "\n",
    "print(\"overlap val∩test:\", len(ids_val & ids_test))\n",
    "print(\"overlap val∩train:\", len(ids_val & ids_train))\n",
    "print(\"overlap test∩train:\", len(ids_test & ids_train))\n",
    "print(\"overlap pr∩train:\", len(ids_pr & ids_train))\n",
    "print(\"overlap pr∩val:\", len(ids_pr & ids_val))\n",
    "print(\"overlap pr∩test:\", len(ids_pr & ids_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa1ce55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold docs: 373\n",
      "Memoria ejemplos: 8\n",
      "Blocked uids: 16\n"
     ]
    }
   ],
   "source": [
    "# 4) Cargar gold + memoria + blocked (anti-leakage)\n",
    "if not PATH_GOLD.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro gold: {PATH_GOLD}\")\n",
    "gold = load_jsonl(PATH_GOLD) if is_jsonl(PATH_GOLD) else load_json(PATH_GOLD)\n",
    "\n",
    "if not PATH_MEMORY.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro memoria: {PATH_MEMORY}\")\n",
    "memory_selected = load_json(PATH_MEMORY)\n",
    "\n",
    "if not PATH_BLOCKED.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro blocked: {PATH_BLOCKED}\")\n",
    "blocked = set(load_json(PATH_BLOCKED))\n",
    "\n",
    "print(\"Gold docs:\", len(gold))\n",
    "print(\"Memoria ejemplos:\", len(memory_selected))\n",
    "print(\"Blocked uids:\", len(blocked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f566388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold MVP docs: 373\n",
      "Eval pool: 357\n",
      "gold_val: 35 gold_test: 35 gold_train_pool: 287\n"
     ]
    }
   ],
   "source": [
    "# 5) Construir gold_mvp (solo docs que contienen alguna de las 4 etiquetas) + split val/test/train_pool\n",
    "def filter_tags_mvp(tags: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    return [t for t in tags if t.get(\"tag\") in MVP_LABELS]\n",
    "\n",
    "gold_mvp = []\n",
    "for d in gold:\n",
    "    txt = d.get(\"text\", \"\")\n",
    "    if not txt:\n",
    "        continue\n",
    "    uid = stable_uid(txt)\n",
    "    tags = filter_tags_mvp(d.get(\"tags\", []))\n",
    "    if tags:\n",
    "        gold_mvp.append({\"doc_id\": d.get(\"id\"), \"doc_uid\": uid, \"text\": txt, \"tags\": tags})\n",
    "\n",
    "print(\"Gold MVP docs:\", len(gold_mvp))\n",
    "\n",
    "# pool de evaluación sin leakage\n",
    "eval_pool = [d for d in gold_mvp if d[\"doc_uid\"] not in blocked]\n",
    "random.shuffle(eval_pool)\n",
    "\n",
    "# tamaños robustos (si el corpus es pequeño)\n",
    "test_n = min(len(eval_pool), max(30, int(0.10 * len(eval_pool))))\n",
    "val_n  = min(max(0, len(eval_pool)-test_n), max(30, int(0.10 * len(eval_pool))))\n",
    "\n",
    "gold_test = eval_pool[:test_n]\n",
    "gold_val  = eval_pool[test_n:test_n+val_n]\n",
    "gold_train_pool = eval_pool[test_n+val_n:]\n",
    "\n",
    "print(\"Eval pool:\", len(eval_pool))\n",
    "print(\"gold_val:\", len(gold_val), \"gold_test:\", len(gold_test), \"gold_train_pool:\", len(gold_train_pool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08c1b1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6) Render de memoria (para Exp2/Exp3) usando texto real del gold\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m id_to_text = \u001b[43m{\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgold_mvp\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender_memory_blocks\u001b[39m(memory: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], id_to_text: Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mstr\u001b[39m], max_per_label: \u001b[38;5;28mint\u001b[39m = \u001b[32m4\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# organiza por etiqueta y limita ejemplos por etiqueta para que el prompt no explote\u001b[39;00m\n\u001b[32m      6\u001b[39m     by_label: Dict[\u001b[38;5;28mstr\u001b[39m, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = {lab: [] \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m MVP_LABELS}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6) Render de memoria (para Exp2/Exp3) usando texto real del gold\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m id_to_text = {\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m: d[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m gold_mvp}\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender_memory_blocks\u001b[39m(memory: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], id_to_text: Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mstr\u001b[39m], max_per_label: \u001b[38;5;28mint\u001b[39m = \u001b[32m4\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# organiza por etiqueta y limita ejemplos por etiqueta para que el prompt no explote\u001b[39;00m\n\u001b[32m      6\u001b[39m     by_label: Dict[\u001b[38;5;28mstr\u001b[39m, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] = {lab: [] \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m MVP_LABELS}\n",
      "\u001b[31mKeyError\u001b[39m: 'id'"
     ]
    }
   ],
   "source": [
    "# 6) Render de memoria (para Exp2/Exp3) usando texto real del gold\n",
    "id_to_text = {d[\"id\"]: d[\"text\"] for d in gold_mvp}\n",
    "\n",
    "def render_memory_blocks(memory: List[Dict[str, Any]], id_to_text: Dict[str,str], max_per_label: int = 4) -> str:\n",
    "    # organiza por etiqueta y limita ejemplos por etiqueta para que el prompt no explote\n",
    "    by_label: Dict[str, List[Dict[str, Any]]] = {lab: [] for lab in MVP_LABELS}\n",
    "    for ex in memory:\n",
    "        if ex.get(\"label\") in by_label:\n",
    "            by_label[ex[\"label\"]].append(ex)\n",
    "\n",
    "    blocks = []\n",
    "    for lab in MVP_LABELS:\n",
    "        for ex in by_label[lab][:max_per_label]:\n",
    "            id = ex[\"doc_id\"]\n",
    "            txt = id_to_text.get(id, \"\")\n",
    "            s, e = ex[\"start\"], ex[\"end\"]\n",
    "            span_txt = txt[s:e].replace(\"\\n\", \" \").strip()\n",
    "            blocks.append(\n",
    "                f\"- LABEL: {lab}\\n\"\n",
    "                f\"  CRITERION: {ex.get('criterion','')}\\n\"\n",
    "                f\"  EXAMPLE_SPAN: {span_txt}\"\n",
    "            )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "memory_block = render_memory_blocks(memory_selected, id_to_text, max_per_label=4)\n",
    "print(memory_block[:1200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Few-shot extra automático para Exp3 (opcional): 1 ejemplo por etiqueta desde gold_train_pool\n",
    "def pick_one_example_per_label(pool: List[Dict[str,Any]], labels: List[str]) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    used_uids = set()\n",
    "    for lab in labels:\n",
    "        found = None\n",
    "        for d in pool:\n",
    "            if d[\"doc_uid\"] in used_uids:\n",
    "                continue\n",
    "            if any(t.get(\"tag\")==lab for t in d.get(\"tags\", [])):\n",
    "                found = d\n",
    "                break\n",
    "        if found:\n",
    "            used_uids.add(found[\"doc_uid\"])\n",
    "            out.append({\"label\": lab, \"doc_uid\": found[\"doc_uid\"], \"text\": found[\"text\"]})\n",
    "    return out\n",
    "\n",
    "fewshot_docs = pick_one_example_per_label(gold_train_pool, MVP_LABELS)\n",
    "\n",
    "def render_fewshot_extra(fewshot_docs: List[Dict[str,Any]]) -> str:\n",
    "    blocks = []\n",
    "    for ex in fewshot_docs:\n",
    "        # solo damos el texto y la etiqueta objetivo; el modelo debe aprender “formato” y “estilo”\n",
    "        blocks.append(\n",
    "            f\"Ejemplo extra ({ex['label']})\\n\"\n",
    "            f'Texto: \"\"\"{ex[\"text\"][:1500]}\"\"\"\\n'\n",
    "            f'Respuesta: {{\"spans\": []}}'\n",
    "\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "fewshot_extra = render_fewshot_extra(fewshot_docs)\n",
    "print(f\"fewshot_extra ejemplos: {len(fewshot_docs)}\")\n",
    "print(fewshot_extra[:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Esquema Pydantic (salida estricta) + verificación de offsets/quote\n",
    "Label = Literal[\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "\n",
    "class Span(BaseModel):\n",
    "    label: Label\n",
    "    start: int = Field(ge=0)\n",
    "    end: int = Field(ge=0)\n",
    "    quote: str = Field(min_length=1)\n",
    "\n",
    "    @field_validator(\"end\")\n",
    "    @classmethod\n",
    "    def end_after_start(cls, v, info):\n",
    "        start = info.data.get(\"start\")\n",
    "        if start is not None and v <= start:\n",
    "            raise ValueError(\"end must be > start\")\n",
    "        return v\n",
    "\n",
    "class Pred(BaseModel):\n",
    "    doc_uid: Optional[str] = None\n",
    "    spans: List[Span] = Field(default_factory=list)\n",
    "\n",
    "def strict_verify(pred: Pred, text: str) -> Pred:\n",
    "    ok = []\n",
    "    for sp in pred.spans:\n",
    "        if sp.end > len(text):\n",
    "            continue\n",
    "        if text[sp.start:sp.end] != sp.quote:\n",
    "            continue\n",
    "        ok.append(sp)\n",
    "    return Pred(doc_uid=pred.doc_uid, spans=ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca894ce9-b6de-4eca-988e-641185d29477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Parseo robusto de JSON del modelo + sanitización\n",
    "import re, json\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "def extract_balanced_json(text: str) -> str:\n",
    "    \"\"\"Devuelve el primer substring balanceado {...} que contenga 'spans'. Si no encuentra, devuelve ''.\"\"\"\n",
    "    if not isinstance(text, str) or \"{\" not in text:\n",
    "        return \"\"\n",
    "    starts = [i for i,ch in enumerate(text) if ch==\"{\"]\n",
    "    for s in starts:\n",
    "        depth = 0\n",
    "        for e in range(s, len(text)):\n",
    "            ch = text[e]\n",
    "            if ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    cand = text[s:e+1]\n",
    "                    if '\"spans\"' in cand or \"'spans'\" in cand:\n",
    "                        return cand\n",
    "    return \"\"\n",
    "\n",
    "def repair_invalid_unicode_escapes(s: str) -> str:\n",
    "    # Repara \\u que no vaya seguido de 4 hex (lo convierte en \\\\u literal para que json.loads no reviente)\n",
    "    return re.sub(r'\\\\u(?![0-9a-fA-F]{4})', r'\\\\\\\\u', s)\n",
    "\n",
    "def sanitize_pred_dict(obj: Dict[str,Any], doc_uid: str) -> Dict[str,Any]:\n",
    "    spans = obj.get(\"spans\", [])\n",
    "    if not isinstance(spans, list):\n",
    "        spans = []\n",
    "    cleaned = []\n",
    "    for sp in spans:\n",
    "        if not isinstance(sp, dict):\n",
    "            continue\n",
    "        lab = sp.get(\"label\") or sp.get(\"tag\")\n",
    "        if lab not in MVP_LABELS:\n",
    "            continue\n",
    "        try:\n",
    "            start = int(sp.get(\"start\"))\n",
    "            end = int(sp.get(\"end\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        quote = sp.get(\"quote\")\n",
    "        if not isinstance(quote, str) or not quote.strip():\n",
    "            continue\n",
    "        cleaned.append({\"label\": lab, \"start\": start, \"end\": end, \"quote\": quote})\n",
    "    return {\"doc_uid\": doc_uid, \"spans\": cleaned}\n",
    "\n",
    "def parse_and_validate(raw: str, doc_uid: str, text: str) -> Tuple[Dict[str,Any], str]:\n",
    "    \"\"\"\n",
    "    Devuelve (pred_dict, error_code).\n",
    "    Clasifica: non_json_output, json_truncated, json_parse_error, validation_error,\n",
    "               model_returned_empty, all_spans_filtered, all_spans_discarded\n",
    "    \"\"\"\n",
    "    if not isinstance(raw, str):\n",
    "        raw = str(raw)\n",
    "\n",
    "    # 1) reparar escapes unicode inválidos EN TODA la respuesta\n",
    "    raw_fixed = repair_invalid_unicode_escapes(raw)\n",
    "\n",
    "    # 2) extraer JSON balanceado\n",
    "    js = extract_balanced_json(raw_fixed)\n",
    "    if not js:\n",
    "        # Heurística para diferenciar \"no hay JSON\" vs \"parece truncado\"\n",
    "        if (\"{\".encode() and '\"spans\"' in raw_fixed and raw_fixed.count(\"{\") > raw_fixed.count(\"}\")):\n",
    "            return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"json_truncated\", \"_raw\": raw}, \"json_truncated\"\n",
    "        return {\"doc_uid\": doc_uid, \"spans\": [], \"_error\": \"non_json_output\", \"_raw\": raw}, \"non_json_output\"\n",
    "\n",
    "    # 3) parse JSON\n",
    "    try:\n",
    "        obj = json.loads(js)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"doc_uid\": doc_uid,\n",
    "            \"spans\": [],\n",
    "            \"_error\": \"json_parse_error\",\n",
    "            \"_raw\": raw,\n",
    "            \"_exception\": repr(e),\n",
    "        }, \"json_parse_error\"\n",
    "\n",
    "    # 4) contar spans \"brutos\" del modelo\n",
    "    spans_raw = obj.get(\"spans\", [])\n",
    "    n_spans_raw = len(spans_raw) if isinstance(spans_raw, list) else 0\n",
    "\n",
    "    # 5) limpiar (filtro por etiquetas MVP y campos)\n",
    "    cleaned_obj = sanitize_pred_dict(obj, doc_uid)\n",
    "    n_spans_kept = len(cleaned_obj.get(\"spans\", []))\n",
    "\n",
    "    # Si el modelo devolvió spans vacíos (o no devolvió spans)\n",
    "    if n_spans_raw == 0:\n",
    "        # Esto NO es un error de formato; es \"modelo no encontró\"\n",
    "        # Lo marcamos para que puedas medirlo aparte.\n",
    "        cleaned_obj[\"_meta\"] = {\"n_spans_raw\": n_spans_raw, \"n_spans_kept\": n_spans_kept}\n",
    "        return cleaned_obj, \"model_returned_empty\"\n",
    "\n",
    "    # Si el modelo devolvió spans pero tras limpieza se quedaron en 0 -> inventó tags, faltaban campos, etc.\n",
    "    if n_spans_raw > 0 and n_spans_kept == 0:\n",
    "        return {\n",
    "            \"doc_uid\": doc_uid,\n",
    "            \"spans\": [],\n",
    "            \"_error\": \"all_spans_filtered\",\n",
    "            \"_raw\": raw,\n",
    "            \"_meta\": {\"n_spans_raw\": n_spans_raw, \"n_spans_kept\": n_spans_kept},\n",
    "        }, \"all_spans_filtered\"\n",
    "\n",
    "    # 6) validación pydantic + strict_verify (offsets/quote)\n",
    "    try:\n",
    "        pred = Pred.model_validate(cleaned_obj)\n",
    "        pred2 = strict_verify(pred, text)\n",
    "        out = pred2.model_dump()\n",
    "\n",
    "        n_spans_final = len(out.get(\"spans\", []))\n",
    "\n",
    "        # Si había spans tras limpieza pero strict_verify los eliminó todos -> problema offsets/quote mismatch\n",
    "        if n_spans_kept > 0 and n_spans_final == 0:\n",
    "            return {\n",
    "                \"doc_uid\": doc_uid,\n",
    "                \"spans\": [],\n",
    "                \"_error\": \"all_spans_discarded\",\n",
    "                \"_raw\": raw,\n",
    "                \"_meta\": {\"n_spans_raw\": n_spans_raw, \"n_spans_kept\": n_spans_kept, \"n_spans_final\": n_spans_final},\n",
    "            }, \"all_spans_discarded\"\n",
    "\n",
    "        # OK\n",
    "        out[\"_meta\"] = {\"n_spans_raw\": n_spans_raw, \"n_spans_kept\": n_spans_kept, \"n_spans_final\": n_spans_final}\n",
    "        return out, \"\"\n",
    "\n",
    "    except ValidationError as e:\n",
    "        return {\n",
    "            \"doc_uid\": doc_uid,\n",
    "            \"spans\": [],\n",
    "            \"_error\": \"validation_error\",\n",
    "            \"_raw\": raw,\n",
    "            \"_details\": e.errors(),\n",
    "            \"_meta\": {\"n_spans_raw\": n_spans_raw, \"n_spans_kept\": n_spans_kept},\n",
    "        }, \"validation_error\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Carga del modelo (Transformers) — 4-bit si bitsandbytes está disponible\n",
    "# Si el modelo es gated (Llama), haz login en terminal:  huggingface-cli login\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"❌ CUDA no disponible. Este notebook asume GPU.\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Intento 4-bit (bnb). Si falla, cae a fp16 (más pesado).\n",
    "use_4bit = True\n",
    "bnb_config = None\n",
    "if use_4bit:\n",
    "    try:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        print(\"✅ BitsAndBytesConfig OK (4-bit)\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ No puedo configurar 4-bit. Fallback fp16. Error:\", repr(e))\n",
    "        bnb_config = None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "if bnb_config is not None:\n",
    "    model_kwargs[\"quantization_config\"] = bnb_config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Modelo cargado. device:\", model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565bf24-a584-4544-95fd-c16b4d979b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.1 Celda de diagnóstico\n",
    "print(\"tokenizer.eos_token:\", tokenizer.eos_token)\n",
    "print(\"tokenizer.eos_token_id:\", tokenizer.eos_token_id)\n",
    "print(\"tokenizer.pad_token:\", tokenizer.pad_token)\n",
    "print(\"tokenizer.pad_token_id:\", tokenizer.pad_token_id)\n",
    "\n",
    "print(\"model.config.eos_token_id:\", model.config.eos_token_id)\n",
    "print(\"model.config.pad_token_id:\", model.config.pad_token_id)\n",
    "\n",
    "# transformers usa a menudo generation_config en lugar de config\n",
    "print(\"model.generation_config.eos_token_id:\", getattr(model.generation_config, \"eos_token_id\", None))\n",
    "print(\"model.generation_config.pad_token_id:\", getattr(model.generation_config, \"pad_token_id\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a02ec5-5fff-4db1-959c-9a36e8c76bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.2 Evitar mensajes de transformers diciendo que como el token no tiene pad definido, usa EOS para padding\n",
    "# Quita warnings y define padding consistentemente\n",
    "# --- FIX robusto de EOS/PAD para quitar warnings de padding ---\n",
    "\n",
    "# 1) Determinar eos_token_id \"real\"\n",
    "eos_id = tokenizer.eos_token_id\n",
    "\n",
    "# Si tokenizer no lo tiene, intenta sacarlo del modelo\n",
    "if eos_id is None:\n",
    "    eos_id = model.config.eos_token_id\n",
    "\n",
    "# Si eos_id es lista/tupla, coge el primero\n",
    "if isinstance(eos_id, (list, tuple)):\n",
    "    eos_id = eos_id[0]\n",
    "\n",
    "# 2) Si sigue siendo None, algo está realmente mal con el tokenizer/model\n",
    "assert eos_id is not None, \"ERROR: eos_token_id es None. El tokenizer/model no tiene EOS configurado.\"\n",
    "\n",
    "# 3) Asegura PAD en tokenizer\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = eos_id\n",
    "    # opcional: token “pad” como string también\n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4) Asegura PAD/EOS en generation_config (lo que usa generate)\n",
    "model.generation_config.eos_token_id = eos_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 5) También en config, por coherencia\n",
    "model.config.eos_token_id = eos_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"OK eos_id:\", eos_id, \"| pad_id:\", tokenizer.pad_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Prompts Exp1/Exp2/Exp3 (MVP 4 etiquetas + offsets estrictos)\n",
    "LABELS = MVP_LABELS  # [\"OBJETO\",\"PRECIO_DEL_CONTRATO\",\"DURACION_TOTAL_DEL_CONTRATO\",\"RESOLUCION\"]\n",
    "\n",
    "LABELS_TXT = \", \".join(LABELS)\n",
    "\n",
    "SYSTEM_BASE = (\n",
    "    \"Eres un anotador jurídico experto en contratos del sector público en España.\\n\"\n",
    "    \"Tu tarea es EXTRAER SPANS del texto y devolver sus offsets start/end con una etiqueta.\\n\\n\"\n",
    "\n",
    "    \"ETIQUETAS PERMITIDAS (OBLIGATORIO): \" + LABELS_TXT + \".\\n\"\n",
    "    \"PROHIBIDO inventar otras etiquetas (p.ej. REUNIDOS, TITLE, ANTECEDENTES, MVP, etc.).\\n\\n\"\n",
    "\n",
    "    \"IMPORTANTE: NO escapes Unicode. No uses secuencias ni escapes como \\\\u00fa. Escribe Unicode directamente (á, é, í, ó, ú, ñ) como 'público', 'contratación', etc.\\n\"\n",
    "\n",
    "    \"FORMATO DE SALIDA (OBLIGATORIO):\\n\"\n",
    "    \"- Devuelve EXACTAMENTE 1 objeto JSON y NADA más.\\n\"\n",
    "    \"- NO uses markdown (sin ```).\\n\"\n",
    "    \"- NO añadas explicaciones ni texto fuera del JSON.\\n\"\n",
    "    \"- Esquema: {\\\"spans\\\": [{\\\"tag\\\": <LABEL>, \\\"start\\\": <int>, \\\"end\\\": <int>, \\\"quote\\\": <str>}, ...]}\\n\"\n",
    "    \"- Si no hay spans, devuelve: {\\\"spans\\\": []}.\\n\\n\"\n",
    "\n",
    "    \"REGLAS CRÍTICAS DE OFFSETS:\\n\"\n",
    "    \"- start y end son offsets de caracteres sobre el texto ORIGINAL.\\n\"\n",
    "    \"- Debe cumplirse: 0 <= start < end <= len(texto).\\n\"\n",
    "    \"- quote DEBE ser EXACTAMENTE el substring texto[start:end].\\n\"\n",
    "    \"- Si no puedes calcular offsets con seguridad, NO incluyas ese span.\\n\"\n",
    "    \"- PROHIBIDO usar start=0,end=0 con quote no vacío.\\n\\n\"\n",
    "\n",
    "    \"LÍMITES:\\n\"\n",
    "    \"- Devuelve como máximo 6 spans en total.\\n\"\n",
    "    \"- No repitas spans.\\n\\n\"\n",
    "\n",
    "    \"PROHIBIDO:\\n\"\n",
    "    \"- No incluyas los delimitadores <<<TEXT>>> o <<<END_TEXT>>> dentro de quote.\\n\"\n",
    ")\n",
    "\n",
    "def build_user_exp1(text: str) -> str:\n",
    "    return (\n",
    "        \"Extrae spans del texto SOLO para estas etiquetas: \" + LABELS_TXT + \".\\n\"\n",
    "        \"Devuelve SOLO JSON válido.\\n\\n\"\n",
    "        \"TEXTO (no incluyas los delimitadores en quote):\\n\"\n",
    "        \"<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "        \"\\nRESPONDE SOLO CON JSON:\\n\"\n",
    "        \"{\\\"spans\\\": []}\"\n",
    "    )\n",
    "\n",
    "def build_user_exp2(text: str, memory_block: str) -> str:\n",
    "    return (\n",
    "        \"Usa la MEMORIA como guía. Extrae spans SOLO para estas etiquetas: \" + LABELS_TXT + \".\\n\"\n",
    "        \"Devuelve SOLO JSON válido.\\n\\n\"\n",
    "        \"MEMORIA:\\n\"\n",
    "        f\"{memory_block}\\n\\n\"\n",
    "        \"TEXTO (no incluyas los delimitadores en quote):\\n\"\n",
    "        \"<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "        \"\\nRESPONDE SOLO CON JSON:\\n\"\n",
    "        \"{\\\"spans\\\": []}\"\n",
    "    )\n",
    "\n",
    "def build_user_exp3(text: str, memory_block: str, fewshot_extra: str) -> str:\n",
    "    return (\n",
    "        \"Usa MEMORIA + FEW-SHOT EXTRA como guía. Extrae spans SOLO para estas etiquetas: \" + LABELS_TXT + \".\\n\"\n",
    "        \"Devuelve SOLO JSON válido.\\n\\n\"\n",
    "        \"MEMORIA:\\n\"\n",
    "        f\"{memory_block}\\n\\n\"\n",
    "        \"FEW-SHOT EXTRA:\\n\"\n",
    "        f\"{fewshot_extra}\\n\\n\"\n",
    "        \"TEXTO (no incluyas los delimitadores en quote):\\n\"\n",
    "        \"<<<TEXT>>>\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        \"<<<END_TEXT>>>\\n\"\n",
    "        \"\\nRESPONDE SOLO CON JSON:\\n\"\n",
    "        \"{\\\"spans\\\": []}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Generación chat (sin warnings) + predictor seguro\n",
    "@torch.no_grad()\n",
    "def generate_chat(system: str, user: str, max_new_tokens: int = 900-1200, temperature: float = 0.0, top_p: float = 0.9) -> str:\n",
    "    messages = [{\"role\":\"system\",\"content\":system}, {\"role\":\"user\",\"content\":user}]\n",
    "    enc = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attention_mask = enc.get(\"attention_mask\")\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    temperature=None, #no necesitamos temperatura ni top_p asi que podríamos omitirlo para evitar los warnings\n",
    "    top_p=None, #no necesitamos temperatura ni top_p asi que podríamos omitirlo para evitar los warnings\n",
    "    )\n",
    "    \n",
    "    if temperature and temperature > 0: \n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=temperature, top_p=top_p))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "\n",
    "    out = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
    "    gen_ids = out[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def predict_one(doc_uid: str, text: str, exp: int) -> Dict[str,Any]:\n",
    "    if exp == 1:\n",
    "        user = build_user_exp1(text)\n",
    "    elif exp == 2:\n",
    "        user = build_user_exp2(text, memory_block)\n",
    "    elif exp == 3:\n",
    "        user = build_user_exp3(text, memory_block, fewshot_extra)\n",
    "    else:\n",
    "        raise ValueError(\"exp debe ser 1,2,3\")\n",
    "\n",
    "    raw = generate_chat(SYSTEM_BASE, user, max_new_tokens=900, temperature=0.0)\n",
    "    pred, err = parse_and_validate(raw, doc_uid, text)\n",
    "\n",
    "    # DEBUG: guarda raw si hay error O si quedó vacío\n",
    "    if err or (isinstance(pred, dict) and pred.get(\"spans\") == []):\n",
    "        pred[\"_raw\"] = raw\n",
    "        if err:\n",
    "            pred[\"_error\"] = err\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1adc0-7f07-4fd8-a675-82a3a6145ed8",
   "metadata": {},
   "source": [
    "## Instrucciones del prompt y experimentos\n",
    "\n",
    "En `predict_one`:\n",
    "- Exp1 llama: `build_user_exp1(text)` → solo “instrucciones + texto” (system + user), sin memoria ni ejemplos extra.\n",
    "- Exp2 llama: `build_user_exp2(text, memory_block)` → instrucciones + memoria + texto\n",
    "- Exp3 llama: `build_user_exp3(text, memory_block, fewshot_extra)` → instrucciones + memoria + few-shot + texto\n",
    "\n",
    "Y en todos los casos el system es SYSTEM_BASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a124e-814c-4498-907b-445547f158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.0.0. Definimos run_experiment + save_jsonl + rutas de salida (esto se ejecuta una vez), después se ejecutan los experimentos\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def run_experiment(docs: List[Dict[str,Any]], exp: int, name: str, n_limit: Optional[int]=None) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    counter = Counter()\n",
    "    docs2 = docs if n_limit is None else docs[:n_limit]\n",
    "    for i, d in enumerate(docs2, start=1):\n",
    "        pred = predict_one(d[\"doc_uid\"], d[\"text\"], exp=exp)\n",
    "        if pred.get(\"_error\"):\n",
    "            counter[pred[\"_error\"]] += 1\n",
    "        out.append(pred)\n",
    "        if i % 5 == 0:\n",
    "            print(f\"{name}: {i}/{len(docs2)} | errors:\", dict(counter))\n",
    "    print(\"DONE\", name, \"| total:\", len(out), \"| errors:\", dict(counter))\n",
    "    return out\n",
    "\n",
    "def _jsonable(x):\n",
    "    if isinstance(x, BaseException):\n",
    "        return repr(x)\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return list(x)\n",
    "    return x\n",
    "\n",
    "def save_jsonl(rows: List[Dict[str,Any]], path: Path):\n",
    "    import json\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False, default=_jsonable) + \"\\n\")\n",
    "\n",
    "OUT1 = OUT_DIR / \"exp1_gold_val.jsonl\"\n",
    "OUT2 = OUT_DIR / \"exp2_gold_val.jsonl\"\n",
    "OUT3 = OUT_DIR / \"exp3_gold_val.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0860a0e-ab0f-46f1-9d23-bf2eadc62a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.0) Ejecutamos solo Exp1 (quick test)\n",
    "#Como estamos ajustando prompts/parsers: usa n_limit=5 para pruebas. Cuando vaya bien, subimos a 20/35\n",
    "\n",
    "pred1 = run_experiment(gold_val, exp=1, name=\"EXP1\", n_limit=5)\n",
    "save_jsonl(pred1, OUT1)\n",
    "print(\"Saved:\", OUT1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ec647-bf93-43a9-9e5a-bc1f23ab1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.Función para inspeccionar el caso concreto: muestra por cada span: label, start:end, quote y el substring real text[start:end]\n",
    "def debug_pred(doc_uid: str, preds: list, uid_to_text: dict, n_spans_show: int = 5):\n",
    "    # busca pred por uid\n",
    "    p = next((x for x in preds if x.get(\"doc_uid\") == doc_uid), None)\n",
    "    if p is None:\n",
    "        print(\"No encontrado:\", doc_uid)\n",
    "        return\n",
    "\n",
    "    text = uid_to_text.get(doc_uid, \"\")\n",
    "    print(\"doc_uid:\", doc_uid)\n",
    "    print(\"error:\", p.get(\"_error\", \"\"))\n",
    "    print(\"meta:\", p.get(\"_meta\", {}))\n",
    "    raw = p.get(\"_raw\", \"\")\n",
    "    print(\"\\nRAW (inicio):\\n\", raw[:800])\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "    # intenta recuperar el JSON y los spans del modelo\n",
    "    js = extract_balanced_json(repair_invalid_unicode_escapes(raw))\n",
    "    if not js:\n",
    "        print(\"No se pudo extraer JSON balanceado.\")\n",
    "        return\n",
    "    try:\n",
    "        obj = json.loads(js)\n",
    "    except Exception as e:\n",
    "        print(\"json.loads falló:\", e)\n",
    "        return\n",
    "\n",
    "    spans = obj.get(\"spans\", [])\n",
    "    if not isinstance(spans, list):\n",
    "        print(\"obj['spans'] no es lista\")\n",
    "        return\n",
    "\n",
    "    print(\"Spans en JSON del modelo:\", len(spans))\n",
    "    for i, sp in enumerate(spans[:n_spans_show], start=1):\n",
    "        lab = sp.get(\"label\") or sp.get(\"tag\")\n",
    "        s = sp.get(\"start\"); e = sp.get(\"end\"); q = sp.get(\"quote\")\n",
    "        print(f\"\\n[{i}] label={lab} start={s} end={e}\")\n",
    "        print(\"quote:\", repr(q)[:200])\n",
    "\n",
    "        # compara con substring real si start/end son enteros\n",
    "        try:\n",
    "            si = int(s); ei = int(e)\n",
    "            sub = text[si:ei]\n",
    "            print(\"text[start:end]:\", repr(sub)[:200])\n",
    "            print(\"match:\", sub == q)\n",
    "        except Exception as ex:\n",
    "            print(\"No puedo comparar substring:\", ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d8027-c79a-491b-b654-f1b06416f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.1. Necesitas uid_to_text (map doc_uid → text). Si no lo tienes en exp1, créalo:\n",
    "uid_to_text = {d[\"doc_uid\"]: d[\"text\"] for d in gold_val}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9969c78-712c-41d3-b7f7-ec32d69afffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.2. Elige un doc con error:\n",
    "bad = [p for p in pred1 if p.get(\"_error\") == \"all_spans_discarded\"]\n",
    "bad[0][\"doc_uid\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23b686-f1b1-45f3-9904-1f71e29fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.3. Llama al debug (Esto dirá exactamente por qué se descartan (mismatch))\n",
    "debug_pred(bad[0][\"doc_uid\"], pred1, uid_to_text, n_spans_show=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59e6bd-d841-4b66-971c-70ae2f41affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check para comprobar por qué está dando offsets que no son números (lo vimos en la celda anterior el debug)\n",
    "#Comprobamos si los textos que se estan enfrentando son los mismos\n",
    "uid = \"e65775141ab5c82bd0bd1f89e4873090c43a9569\"\n",
    "t = uid_to_text[uid]\n",
    "print(\"len(text) =\", len(t))\n",
    "print(\"slice 120:200 ->\", repr(t[120:200]))\n",
    "print(\"busco quote OBJETO ->\", t.find(\"Obras de sustitución\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2938c-38ee-4294-88db-44e54865194e",
   "metadata": {},
   "source": [
    "## Errores con offsets del modelo\n",
    "\n",
    "Sabiendo que los resultados del debug fueron, para el caso [1]:\n",
    "\n",
    "    doc_uid: e65775141ab5c82bd0bd1f89e4873090c43a9569\n",
    "    error: all_spans_discarded\n",
    "    meta: {'n_spans_raw': 4, 'n_spans_kept': 4, 'n_spans_final': 0}\n",
    "\n",
    "    RAW (inicio):\n",
    "     {\"spans\": [{\"tag\": \"OBJETO\", \"start\": 143, \"end\": 153, \"quote\": \"Obras de sustituci\\u00f3n del sistema de climatizaci\\u00f3n en el auditorio del edificio de Biolog\\u00eda de la Facultad de Ciencias de la Universidad Aut\\u00f3noma de Madrid\"}, \n",
    "\n",
    "y\n",
    "\n",
    "    [1] label=OBJETO start=143 end=153\n",
    "    quote: 'Obras de sustitución del sistema de climatización en el auditorio del edificio de Biología de la Facultad de Ciencias de la Universidad Autónoma de Madrid'\n",
    "    text[start:end]: 'bre y repr' **(!!)**\n",
    "    match: False\n",
    "\n",
    "Podemos decir que el modelo te está dando offsets (`143`…) como si el texto empezara en “Obras…”, pero en tu `text` real esa frase está en `1573`.\n",
    "Eso pasa cuando el modelo no está calculando offsets sobre el texto completo, sino sobre una versión recortada o distinta del texto (p.ej. solo la sección de “CLÁUSULAS” o un extracto), o simplemente inventa offsets (muy común).\n",
    "\n",
    "Como ya confirmaste que `find()` da `1573`, la solución práctica es:\n",
    "1) Dejar de creer los offsets del modelo\n",
    "Tu pipeline debe tratar start/end del modelo como “sugerencias” y hacer esto:\n",
    "- usar el quote como verdad\n",
    "- recalcular offsets en el texto real con find(quote)\n",
    "- y solo aceptar si el quote aparece (idealmente una sola vez)\n",
    "\n",
    "Esto exactamente lo que acabas de implementar con el `strict_verify` reparador. Con ese cambio, el span OBJETO quedará:\n",
    "- `quote = “…Obras de sustitución…”`\n",
    "- `pos = 1573`\n",
    "- `start=1573, end=1573+len(quote)`\n",
    "- y ya no habrá `all_spans_discarded`.\n",
    "\n",
    "2) ¿Por qué el modelo devuelve `143` entonces?\n",
    "Puede ser cualquiera de estas (todas típicas):\n",
    "    A) El modelo “no sabe” calcular offsets globales\n",
    "        - Muchísimos LLMs fallan con offsets en textos largos. A veces ponen números pequeños “porque sí”.\n",
    "    B) Tu prompt/plantilla induce al modelo a pensar que el texto empieza en otro punto\n",
    "        - Si el modelo se fija en una sección posterior (“CLÁUSULAS CONTRACTUALES…”) puede tomar esa como inicio mental.\n",
    "    C) Texto muy largo + atención limitada\n",
    "\n",
    "Aunque le metas todo el texto, puede “anclar” el cálculo de offsets a lo que tiene más cerca en contexto.\n",
    "\n",
    "Unicode NO: en tu caso el quote está como \\u00f3 pero eso al parsear vuelve a “ó”. No cambia la longitud del string original de tu text, y además el mismatch es de 143 vs 1573 (enorme), no de 1–2 chars.\n",
    "\n",
    "**(*) Opción recomendada (robusta y simple)**: Cambiar el contrato de salida:\n",
    "- que el modelo NO devuelva offsets, solo label + quote, y yo calculo offsets siempre haciendo `find()` y añadiendo `start/end`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7110c3f-8335-4fad-a219-e6bdfedf538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Ver el origen de all_spans_discarded de forma agregada para saber si el mismatch es siempre el mismo patrón (saltos de línea, espacios, comillas, etc.)\n",
    "#Si n_spans_kept es >0, confirma que el problema está solo en strict_verify\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "errs = Counter([p.get(\"_error\",\"\") for p in pred1 if p.get(\"_error\")])\n",
    "print(errs)\n",
    "\n",
    "metas = [p.get(\"_meta\", {}) for p in pred1 if p.get(\"_error\") == \"all_spans_discarded\"]\n",
    "print(\"n_spans_raw (ejemplos):\", [m.get(\"n_spans_raw\") for m in metas[:10]])\n",
    "print(\"n_spans_kept (ejemplos):\", [m.get(\"n_spans_kept\") for m in metas[:10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2307d-d4ba-417a-9709-457f5d51848a",
   "metadata": {},
   "source": [
    "**La causa más probable (99%): normalización de texto**\n",
    "`strict_verify` suele fallar por:\n",
    "- `\\r\\n` vs `\\n`\n",
    "- espacios múltiples vs uno\n",
    "- comillas tipográficas “ ” vs \"\n",
    "- guiones largos – vs -\n",
    "- OCR con caracteres raros\n",
    "- el modelo devolviendo quote sin exactamente las mismas nuevas líneas\n",
    "\n",
    "Tu verificación es “exact string match” y es demasiado estricta para texto con OCR/ruido.\n",
    "\n",
    "Hacemos un arreglo (estratégicamente):\n",
    "\n",
    "**Estrategia 1**: si quote no coincide, buscarlo literalmente en el texto y corregir offsets\n",
    "- Si el modelo te da quote, intenta text.find(quote).\n",
    "- Si aparece una sola vez, reemplaza start/end por esa posición.\n",
    "- Si aparece varias veces, o no aparece, descarta.\n",
    "\n",
    "Esto mantiene precisión (solo aceptamos quotes que están realmente en el texto), pero no dependemos de offsets del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03880381-92d2-4b5d-a107-3bd09c137c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Arreglo recomendado: “strict_verify” tolerante pero seguro\n",
    "def repair_offsets_by_quote(span, text: str):\n",
    "    q = span.quote\n",
    "    if not q:\n",
    "        return None\n",
    "    pos = text.find(q)\n",
    "    if pos == -1:\n",
    "        return None\n",
    "    # si aparece varias veces, mejor descartar (ambiguo)\n",
    "    if text.find(q, pos+1) != -1:\n",
    "        return None\n",
    "    span.start = pos\n",
    "    span.end = pos + len(q)\n",
    "    return span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858a263-5586-4cc6-bcdb-819790b00a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.0.1) Ejecutamos solo Exp2 (quick test) con 5 muestras\n",
    "pred2 = run_experiment(gold_val, exp=2, name=\"EXP2\", n_limit=5)\n",
    "save_jsonl(pred2, OUT2)\n",
    "print(\"Saved:\", OUT2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4797506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Ejecutar Exp1/Exp2/Exp3 sobre gold_val (rápido) y guardar JSONL con 20 muestras. Ajustar muestras: n_limit=20/35/50\n",
    "\n",
    "pred1 = run_experiment(gold_val, exp=1, name=\"EXP1\", n_limit=20)\n",
    "pred2 = run_experiment(gold_val, exp=2, name=\"EXP2\", n_limit=20)\n",
    "pred3 = run_experiment(gold_val, exp=3, name=\"EXP3\", n_limit=20)\n",
    "\n",
    "save_jsonl(pred1, OUT1)\n",
    "save_jsonl(pred2, OUT2)\n",
    "save_jsonl(pred3, OUT3)\n",
    "\n",
    "print(\"Saved:\", OUT1, OUT2, OUT3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf622ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Supervisor: inspección rápida de outputs guardados\n",
    "def read_jsonl(path: Path, n: int = 3):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i >= n: break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "for path in [OUT1, OUT2, OUT3]:\n",
    "    print(\"\\n===\", path.name, \"===\")\n",
    "    rows = read_jsonl(path, n=2)\n",
    "    for r in rows:\n",
    "        print(\"doc_uid:\", r.get(\"doc_uid\"), \"| spans:\", len(r.get(\"spans\",[])), \"| error:\", r.get(\"_error\"))\n",
    "        for sp in r.get(\"spans\", [])[:3]:\n",
    "            print(\" \", sp[\"label\"], sp[\"start\"], sp[\"end\"], \"| quote[:80]=\", sp[\"quote\"][:80].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844e586-8945-4a68-b35f-c43154db5444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c285d8-63bd-487d-b7e4-82acef772e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inesagent_gpu (NFS /home/jovyan)",
   "language": "python",
   "name": "inesagent_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
